{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report,f1_score, precision_score, recall_score, accuracy_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          1\n",
       "3          0\n",
       "4          0\n",
       "          ..\n",
       "1131677    0\n",
       "1131678    1\n",
       "1131679    0\n",
       "1131680    0\n",
       "1131681    0\n",
       "Name: loan_status, Length: 1131682, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/modified_0420.csv\")\n",
    "reversed_df = df.drop(columns=['Unnamed: 0']).copy()\n",
    "# reversed_df['loan_status'] = reversed_df['loan_status'].map({1:0, 0:1})\n",
    "reversed_df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_df.shape\n",
    "reversed_df.to_csv('data/modified_0420_reversed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1131682, 91)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/modified_0420.csv\")\n",
    "x_train = df.drop(columns=['loan_status', 'Unnamed: 0'])\n",
    "y_train = df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = np.isnan(x_train).any(axis=1)\n",
    "x_train = x_train[~nan_indices]\n",
    "y_train = y_train[~nan_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scaler 찾아보기\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15351255 0.23887498 0.29132949 0.33460378 0.37342152 0.40992361\n",
      " 0.44018213 0.46830687 0.49304401 0.51586528 0.53721772 0.55750293\n",
      " 0.57607956 0.59350761 0.60844465 0.62254463 0.63602833 0.64925373\n",
      " 0.66216411 0.67429034 0.68602493 0.69753601 0.70896553 0.7203637\n",
      " 0.73171759 0.74305794 0.75434201 0.76561303 0.77685392 0.78806636\n",
      " 0.79920845 0.81031344 0.8213071  0.83225873 0.843079   0.85338506\n",
      " 0.86359883 0.87338488 0.88257324 0.89131271 0.89893998 0.90642617\n",
      " 0.91346178 0.9200401  0.92612666 0.93178941 0.93700023 0.94206948\n",
      " 0.9466856  0.95121647 0.95564362 0.95970912 0.96346197 0.96705088\n",
      " 0.97012257 0.9729337  0.97537772 0.97757369 0.97956607 0.98142723\n",
      " 0.9830844  0.98459436 0.98608351 0.98749547 0.98884414 0.99014148\n",
      " 0.99140624 0.99253264 0.99357978 0.99458026 0.99551418 0.99635191\n",
      " 0.99705684 0.99757707 0.99805306 0.99842364 0.99877569 0.99905902\n",
      " 0.99931589 0.99949095 0.99965833 0.99977262 0.99987296 0.99993982\n",
      " 0.99999192 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_components_from_pca(scaled_data:pd.DataFrame, threshold:float) -> int:\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "    print(cumulative_explained_variance)\n",
    "    # 누적 설명 분산 시각화\n",
    "    plt.plot(cumulative_explained_variance, marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance by Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # 적절한 주성분 개수 선택\n",
    "    n_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "    return n_components\n",
    "get_n_components_from_pca(X_scaled, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA로 차원 축소\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "x_train = X_pca.astype('float32')\n",
    "# stratify로 학습에 용이하게 비율 유지\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis = 1), tf.float32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis = 1), tf.float32)\n",
    "    true_positives = K.sum((y_true * y_pred))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    predicted_positives = K.sum(y_pred)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis = 1), tf.float32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis = 1), tf.float32)\n",
    "    true_positives = K.sum((y_true * y_pred))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = tf.cast(tf.argmax(y_true, axis = 1), tf.float32)\n",
    "    y_pred = tf.cast(tf.argmax(y_pred, axis = 1), tf.float32)\n",
    "    true_positives = K.sum((y_true * y_pred))\n",
    "    predicted_positives = K.sum(y_pred)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 아래 칸은 최적의 hyperparameter를 찾기 위해서 실행하는 칸이므로, 실행할 필요 없습니다.\n",
    "\n",
    "모델만을 알고싶다면 넘어가세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5039 - accuracy: 0.7855\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4697 - accuracy: 0.7981\n",
      "182/182 [==============================] - 0s 903us/step - loss: 0.4575 - accuracy: 0.7997\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5089 - accuracy: 0.7832\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4704 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 824us/step - loss: 0.4570 - accuracy: 0.7992\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5249 - accuracy: 0.7665\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4713 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 853us/step - loss: 0.4598 - accuracy: 0.7969\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5042 - accuracy: 0.7849\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4692 - accuracy: 0.7981\n",
      "182/182 [==============================] - 0s 890us/step - loss: 0.4586 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5148 - accuracy: 0.7780\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4739 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 850us/step - loss: 0.4584 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4775 - accuracy: 0.7955\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4621 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 909us/step - loss: 0.4556 - accuracy: 0.8003\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4883 - accuracy: 0.7855\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4624 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 931us/step - loss: 0.4557 - accuracy: 0.8008\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4939 - accuracy: 0.7861\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4625 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 990us/step - loss: 0.4597 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4846 - accuracy: 0.7904\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4615 - accuracy: 0.7995\n",
      "182/182 [==============================] - 0s 908us/step - loss: 0.4562 - accuracy: 0.8011\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4785 - accuracy: 0.7945\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4617 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 938us/step - loss: 0.4561 - accuracy: 0.8007\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.7968\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4595 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 971us/step - loss: 0.4547 - accuracy: 0.8020\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4737 - accuracy: 0.7949\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4594 - accuracy: 0.7997\n",
      "182/182 [==============================] - 1s 1ms/step - loss: 0.4550 - accuracy: 0.8010\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4713 - accuracy: 0.7981\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4586 - accuracy: 0.8005\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4698 - accuracy: 0.7979\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4587 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.8009\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4731 - accuracy: 0.7965\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4592 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4557 - accuracy: 0.8008\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5643 - accuracy: 0.7433\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4731 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 948us/step - loss: 0.4583 - accuracy: 0.7992\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4920 - accuracy: 0.7949\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4692 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 981us/step - loss: 0.4609 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4932 - accuracy: 0.7973\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4695 - accuracy: 0.7983\n",
      "182/182 [==============================] - 0s 951us/step - loss: 0.4649 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4989 - accuracy: 0.7924\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4697 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 948us/step - loss: 0.4589 - accuracy: 0.7992\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5070 - accuracy: 0.7865\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4709 - accuracy: 0.7980\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4786 - accuracy: 0.7962\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4621 - accuracy: 0.7984\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.8005\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4918 - accuracy: 0.7893\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4645 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7991\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4844 - accuracy: 0.7917\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4628 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4594 - accuracy: 0.7982\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4785 - accuracy: 0.7970\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4618 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 990us/step - loss: 0.4585 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4768 - accuracy: 0.7973\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4612 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 992us/step - loss: 0.4579 - accuracy: 0.8000\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4772 - accuracy: 0.7951\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4604 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.8013\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4739 - accuracy: 0.7958\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4600 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.8001\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4723 - accuracy: 0.7967\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4595 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4804 - accuracy: 0.7912\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4605 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.7993\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4746 - accuracy: 0.7954\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4603 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4619 - accuracy: 0.8006\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7765\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4714 - accuracy: 0.7977\n",
      "182/182 [==============================] - 0s 899us/step - loss: 0.4576 - accuracy: 0.7995\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4919 - accuracy: 0.7952\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 874us/step - loss: 0.4578 - accuracy: 0.7985\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4897 - accuracy: 0.7948\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4653 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 898us/step - loss: 0.4596 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4910 - accuracy: 0.7939\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4664 - accuracy: 0.7980\n",
      "182/182 [==============================] - 0s 914us/step - loss: 0.4582 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4874 - accuracy: 0.7957\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4673 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 933us/step - loss: 0.4607 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4791 - accuracy: 0.7957\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4619 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 966us/step - loss: 0.4553 - accuracy: 0.8010\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4748 - accuracy: 0.7969\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4608 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 978us/step - loss: 0.4574 - accuracy: 0.8009\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4781 - accuracy: 0.7956\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4609 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 969us/step - loss: 0.4585 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4751 - accuracy: 0.7975\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4604 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 978us/step - loss: 0.4571 - accuracy: 0.8006\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4917 - accuracy: 0.7860\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4630 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.8007\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4713 - accuracy: 0.7970\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4592 - accuracy: 0.7995\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8017\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4716 - accuracy: 0.7966\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4590 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8013\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4741 - accuracy: 0.7961\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4590 - accuracy: 0.8003\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7991\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4744 - accuracy: 0.7952\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4591 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.8008\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4748 - accuracy: 0.7954\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4594 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.8007\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5060 - accuracy: 0.7825\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4726 - accuracy: 0.7977\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4970 - accuracy: 0.7876\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4687 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 979us/step - loss: 0.4589 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5034 - accuracy: 0.7889\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4695 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 941us/step - loss: 0.4613 - accuracy: 0.7963\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5093 - accuracy: 0.7844\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4744 - accuracy: 0.7978\n",
      "182/182 [==============================] - 0s 951us/step - loss: 0.4629 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5069 - accuracy: 0.7837\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4729 - accuracy: 0.7981\n",
      "182/182 [==============================] - 0s 967us/step - loss: 0.4592 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4871 - accuracy: 0.7886\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4634 - accuracy: 0.7983\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.8001\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4785 - accuracy: 0.7971\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4616 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.8002\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4835 - accuracy: 0.7936\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4630 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4634 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4782 - accuracy: 0.7963\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4627 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8000\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4819 - accuracy: 0.7953\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4621 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7997\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 4ms/step - loss: 0.4784 - accuracy: 0.7934\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4606 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4564 - accuracy: 0.8016\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4760 - accuracy: 0.7940\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4608 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.8002\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4722 - accuracy: 0.7979\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4596 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7975\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4734 - accuracy: 0.7960\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4597 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.8004\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4727 - accuracy: 0.7967\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4595 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8008\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4933 - accuracy: 0.7970\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4667 - accuracy: 0.7978\n",
      "182/182 [==============================] - 0s 897us/step - loss: 0.4572 - accuracy: 0.7991\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4971 - accuracy: 0.7944\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4682 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 916us/step - loss: 0.4598 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5175 - accuracy: 0.7735\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4710 - accuracy: 0.7983\n",
      "182/182 [==============================] - 0s 891us/step - loss: 0.4617 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4857 - accuracy: 0.7959\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4666 - accuracy: 0.7981\n",
      "182/182 [==============================] - 0s 901us/step - loss: 0.4582 - accuracy: 0.7984\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5014 - accuracy: 0.7870\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 1s 2ms/step - loss: 0.4675 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 904us/step - loss: 0.4584 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4789 - accuracy: 0.7949\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4617 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8016\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4777 - accuracy: 0.7956\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4616 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 971us/step - loss: 0.4566 - accuracy: 0.8012\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7935\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4616 - accuracy: 0.7995\n",
      "182/182 [==============================] - 0s 957us/step - loss: 0.4584 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4816 - accuracy: 0.7927\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4621 - accuracy: 0.7990\n",
      "182/182 [==============================] - 0s 969us/step - loss: 0.4563 - accuracy: 0.8003\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4839 - accuracy: 0.7914\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4624 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 950us/step - loss: 0.4576 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4712 - accuracy: 0.7972\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4593 - accuracy: 0.7994\n",
      "182/182 [==============================] - 1s 1ms/step - loss: 0.4547 - accuracy: 0.8016\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4748 - accuracy: 0.7951\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4595 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4545 - accuracy: 0.8014\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4788 - accuracy: 0.7933\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4594 - accuracy: 0.8000\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4572 - accuracy: 0.7985\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4713 - accuracy: 0.7975\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4587 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8009\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4737 - accuracy: 0.7956\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4591 - accuracy: 0.7997\n",
      "182/182 [==============================] - 1s 2ms/step - loss: 0.4551 - accuracy: 0.8010\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.5047 - accuracy: 0.7848\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4715 - accuracy: 0.7977\n",
      "182/182 [==============================] - 0s 962us/step - loss: 0.4624 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5208 - accuracy: 0.7791\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4822 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 934us/step - loss: 0.4626 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5052 - accuracy: 0.7904\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4725 - accuracy: 0.7983\n",
      "182/182 [==============================] - 0s 940us/step - loss: 0.4632 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4962 - accuracy: 0.7933\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4717 - accuracy: 0.7980\n",
      "182/182 [==============================] - 0s 940us/step - loss: 0.4588 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.5069 - accuracy: 0.7823\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 2ms/step - loss: 0.4716 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 948us/step - loss: 0.4589 - accuracy: 0.7985\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4808 - accuracy: 0.7943\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4619 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.8014\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4845 - accuracy: 0.7912\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4636 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8005\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4792 - accuracy: 0.7962\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4618 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4803 - accuracy: 0.7961\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4629 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.8000\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4807 - accuracy: 0.7948\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4620 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 4ms/step - loss: 0.4756 - accuracy: 0.7949\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4604 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8015\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4782 - accuracy: 0.7934\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4613 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4555 - accuracy: 0.8015\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4761 - accuracy: 0.7946\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4597 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7988\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4748 - accuracy: 0.7950\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4599 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.8010\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4718 - accuracy: 0.7981\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.8008\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.5515 - accuracy: 0.7422\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4907 - accuracy: 0.7943\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.5676 - accuracy: 0.7573\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4932 - accuracy: 0.7975\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.5632 - accuracy: 0.7539\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4889 - accuracy: 0.7968\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6726\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4977 - accuracy: 0.7939\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.6135 - accuracy: 0.7190\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4953 - accuracy: 0.7952\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5477 - accuracy: 0.7573\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4759 - accuracy: 0.7969\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7997\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5182 - accuracy: 0.7801\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4729 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5116 - accuracy: 0.7867\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4699 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7966\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5050 - accuracy: 0.7882\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4698 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5617 - accuracy: 0.7381\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4802 - accuracy: 0.7948\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5075 - accuracy: 0.7799\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4672 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7991\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4911 - accuracy: 0.7937\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4651 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8001\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.4908 - accuracy: 0.7935\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4647 - accuracy: 0.7991\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5066 - accuracy: 0.7832\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4666 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7998\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5038 - accuracy: 0.7844\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4666 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5541 - accuracy: 0.7643\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4923 - accuracy: 0.7969\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5368 - accuracy: 0.7732\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4879 - accuracy: 0.7975\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5596 - accuracy: 0.7565\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4886 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5961 - accuracy: 0.7369\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4986 - accuracy: 0.7961\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5384 - accuracy: 0.7768\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.4959 - accuracy: 0.7959\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4687 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5222 - accuracy: 0.7736\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4728 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4605 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.4943 - accuracy: 0.7963\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4686 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4646 - accuracy: 0.7966\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 7ms/step - loss: 0.5437 - accuracy: 0.7554\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4755 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5518 - accuracy: 0.7391\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4722 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5025 - accuracy: 0.7871\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4677 - accuracy: 0.7978\n",
      "37/37 [==============================] - 1s 2ms/step - loss: 0.4580 - accuracy: 0.7997\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4923 - accuracy: 0.7938\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4657 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4955 - accuracy: 0.7925\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4663 - accuracy: 0.7987\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7976\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4825 - accuracy: 0.7978\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4631 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4601 - accuracy: 0.8002\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5051 - accuracy: 0.7903\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4673 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5832 - accuracy: 0.7109\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4911 - accuracy: 0.7968\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.7344 - accuracy: 0.5981\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5143 - accuracy: 0.7899\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7973\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7735\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4855 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7964\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.6076 - accuracy: 0.7001\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4845 - accuracy: 0.7975\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.6038 - accuracy: 0.7143\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4992 - accuracy: 0.7937\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5100 - accuracy: 0.7843\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4721 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7992\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5053 - accuracy: 0.7892\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4704 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7982\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5007 - accuracy: 0.7931\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4680 - accuracy: 0.7984\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7966\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5102 - accuracy: 0.7868\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4717 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7993\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7871\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4718 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4604 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4866 - accuracy: 0.7966\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4646 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8008\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5007 - accuracy: 0.7859\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4673 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4580 - accuracy: 0.7991\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5069 - accuracy: 0.7796\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4675 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7973\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4908 - accuracy: 0.7932\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4666 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5011 - accuracy: 0.7849\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4672 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.7991\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5336 - accuracy: 0.7827\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4846 - accuracy: 0.7973\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.6297 - accuracy: 0.6575\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4999 - accuracy: 0.7963\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5860 - accuracy: 0.7203\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4942 - accuracy: 0.7973\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.7964\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.6116 - accuracy: 0.6907\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5115 - accuracy: 0.7891\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.7117 - accuracy: 0.6394\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5175 - accuracy: 0.7934\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5162 - accuracy: 0.7785\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4736 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.4963 - accuracy: 0.7967\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4686 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5424 - accuracy: 0.7532\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4751 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7964\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5579 - accuracy: 0.7559\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4774 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5157 - accuracy: 0.7850\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4731 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 13ms/step - loss: 0.5018 - accuracy: 0.7877\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4681 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4977 - accuracy: 0.7902\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4665 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4952 - accuracy: 0.7911\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4656 - accuracy: 0.7992\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7976\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5040 - accuracy: 0.7874\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4683 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.5113 - accuracy: 0.7808\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4674 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4587 - accuracy: 0.7996\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5689 - accuracy: 0.7494\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4878 - accuracy: 0.7965\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.7991\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.6611 - accuracy: 0.6524\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5066 - accuracy: 0.7933\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5350 - accuracy: 0.7873\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5392 - accuracy: 0.7732\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4803 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5572 - accuracy: 0.7517\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.4898 - accuracy: 0.7959\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5069 - accuracy: 0.7893\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4708 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4950 - accuracy: 0.7967\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4684 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7984\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5244 - accuracy: 0.7759\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4733 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7963\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5272 - accuracy: 0.7702\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4738 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5300 - accuracy: 0.7635\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4764 - accuracy: 0.7973\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5083 - accuracy: 0.7853\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4681 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.8004\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5023 - accuracy: 0.7826\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4665 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4837 - accuracy: 0.7965\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4633 - accuracy: 0.7989\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5165 - accuracy: 0.7741\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4687 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4585 - accuracy: 0.7990\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4833 - accuracy: 0.7972\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4636 - accuracy: 0.7987\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.8001\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5731 - accuracy: 0.7451\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4918 - accuracy: 0.7973\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5832 - accuracy: 0.7571\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4949 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.7048 - accuracy: 0.6319\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5655 - accuracy: 0.7968\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5735 - accuracy: 0.7457\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5019 - accuracy: 0.7968\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.5588 - accuracy: 0.7706\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 0.4908 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5179 - accuracy: 0.7823\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4738 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5450 - accuracy: 0.7564\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4745 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5307 - accuracy: 0.7698\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4728 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5448 - accuracy: 0.7525\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4751 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.5154 - accuracy: 0.7817\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 7ms/step - loss: 0.4705 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 13ms/step - loss: 0.5087 - accuracy: 0.7818\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4683 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4611 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.5177 - accuracy: 0.7714\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4691 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7985\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4971 - accuracy: 0.7930\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4655 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.7974\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.5140 - accuracy: 0.7778\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.4696 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4603 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4974 - accuracy: 0.7915\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4667 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.7222 - accuracy: 0.5315\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5386 - accuracy: 0.7854\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7985\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6667 - accuracy: 0.6164\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5289 - accuracy: 0.7849\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6662 - accuracy: 0.6235\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5237 - accuracy: 0.7885\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6031 - accuracy: 0.6921\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5139 - accuracy: 0.7785\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5951 - accuracy: 0.7455\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5180 - accuracy: 0.7951\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5515 - accuracy: 0.7572\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4867 - accuracy: 0.7956\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7991\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.6155 - accuracy: 0.6800\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4871 - accuracy: 0.7963\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4652 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5294 - accuracy: 0.7878\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4788 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5143 - accuracy: 0.7927\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4766 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4623 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5599 - accuracy: 0.7546\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4858 - accuracy: 0.7972\n",
      "19/19 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5018 - accuracy: 0.7930\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4706 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7994\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5182 - accuracy: 0.7829\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4750 - accuracy: 0.7972\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5377 - accuracy: 0.7632\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4754 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.7965\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5249 - accuracy: 0.7814\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4754 - accuracy: 0.7968\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7988\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5219 - accuracy: 0.7848\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4738 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4614 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5864 - accuracy: 0.7761\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5118 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.6132 - accuracy: 0.7402\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5202 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6976 - accuracy: 0.5464\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5694 - accuracy: 0.7656\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6009 - accuracy: 0.7470\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5160 - accuracy: 0.7959\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6423 - accuracy: 0.6715\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5322 - accuracy: 0.7921\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5704 - accuracy: 0.7325\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4842 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5407 - accuracy: 0.7718\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4834 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7981\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.5428 - accuracy: 0.7675\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4836 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.5084 - accuracy: 0.7965\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4747 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 14ms/step - loss: 0.5185 - accuracy: 0.7938\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4792 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4645 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.5344 - accuracy: 0.7704\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4754 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.7994\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5165 - accuracy: 0.7889\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.4745 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.7982\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5547 - accuracy: 0.7437\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4776 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7964\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5386 - accuracy: 0.7647\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4748 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.7983\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5191 - accuracy: 0.7858\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4736 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6303 - accuracy: 0.6728\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5250 - accuracy: 0.7896\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.7662 - accuracy: 0.4967\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5702 - accuracy: 0.7598\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5056 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6133 - accuracy: 0.6965\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5176 - accuracy: 0.7842\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7960\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.7218 - accuracy: 0.5699\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5441 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5889 - accuracy: 0.7161\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.5178 - accuracy: 0.7889\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5483 - accuracy: 0.7603\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4839 - accuracy: 0.7960\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.7991\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5860 - accuracy: 0.7046\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4860 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5455 - accuracy: 0.7688\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4829 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.7963\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7934\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4757 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5774 - accuracy: 0.7324\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4897 - accuracy: 0.7941\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5402 - accuracy: 0.7653\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4770 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7992\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.4909 - accuracy: 0.7968\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4670 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4580 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5134 - accuracy: 0.7876\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4723 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4628 - accuracy: 0.7966\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5097 - accuracy: 0.7889\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4718 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4606 - accuracy: 0.7984\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5702 - accuracy: 0.7312\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4795 - accuracy: 0.7969\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4628 - accuracy: 0.7982\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5486 - accuracy: 0.7903\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.4967 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4719 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.6663 - accuracy: 0.6435\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5305 - accuracy: 0.7948\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.6374 - accuracy: 0.6775\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5242 - accuracy: 0.7885\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.7964\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5724 - accuracy: 0.7775\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5077 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6382 - accuracy: 0.6779\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5241 - accuracy: 0.7931\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5316 - accuracy: 0.7867\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4792 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5645 - accuracy: 0.7540\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4893 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.5892 - accuracy: 0.7309\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4957 - accuracy: 0.7972\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4693 - accuracy: 0.7965\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.5677 - accuracy: 0.7478\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4891 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5319 - accuracy: 0.7881\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4821 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.5427 - accuracy: 0.7602\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.4751 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4602 - accuracy: 0.7992\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.5470 - accuracy: 0.7607\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4780 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4619 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5912 - accuracy: 0.7061\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.4785 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7964\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5212 - accuracy: 0.7829\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4748 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5370 - accuracy: 0.7674\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4760 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6117 - accuracy: 0.7328\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5237 - accuracy: 0.7949\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6955 - accuracy: 0.5602\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5421 - accuracy: 0.7648\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6609 - accuracy: 0.6217\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.5179 - accuracy: 0.7849\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7946\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.8919 - accuracy: 0.3529\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 8ms/step - loss: 0.6377 - accuracy: 0.6898\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7982\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.5552 - accuracy: 0.7884\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 7ms/step - loss: 0.5065 - accuracy: 0.7972\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5336 - accuracy: 0.7772\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4827 - accuracy: 0.7972\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5384 - accuracy: 0.7810\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4808 - accuracy: 0.7964\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5850 - accuracy: 0.7124\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4889 - accuracy: 0.7952\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.7965\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5503 - accuracy: 0.7643\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4834 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.5951 - accuracy: 0.7076\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.4926 - accuracy: 0.7949\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5314 - accuracy: 0.7686\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4748 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.7992\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5252 - accuracy: 0.7807\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4731 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4592 - accuracy: 0.7992\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5169 - accuracy: 0.7838\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4737 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4627 - accuracy: 0.7965\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5248 - accuracy: 0.7780\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4746 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7990\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5128 - accuracy: 0.7864\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4726 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7982\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5520 - accuracy: 0.7905\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5022 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.6113 - accuracy: 0.7275\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5242 - accuracy: 0.7937\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5774 - accuracy: 0.7788\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5132 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5538 - accuracy: 0.7959\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5101 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4806 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.6299 - accuracy: 0.6558\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.5355 - accuracy: 0.7598\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.5470 - accuracy: 0.7737\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.4845 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5243 - accuracy: 0.7920\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4795 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.7981\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5348 - accuracy: 0.7792\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4832 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4663 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5820 - accuracy: 0.7324\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.4907 - accuracy: 0.7972\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.5202 - accuracy: 0.7925\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.4778 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7980\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 24ms/step - loss: 0.5098 - accuracy: 0.7920\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.4711 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7994\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.5158 - accuracy: 0.7934\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.4715 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4626 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 24ms/step - loss: 0.5078 - accuracy: 0.7950\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4717 - accuracy: 0.7984\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.7964\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5054 - accuracy: 0.7949\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4712 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.5185 - accuracy: 0.7824\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4739 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "906/906 [==============================] - 4s 4ms/step - loss: 0.4679 - accuracy: 0.7986\n",
      "Epoch 2/60\n",
      "906/906 [==============================] - 4s 4ms/step - loss: 0.4583 - accuracy: 0.7998\n",
      "Best params: {'batch_size': 1000, 'epochs': 60, 'loss': 'binary_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
      "Best average accuracy: 0.8007168650627137\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# KerasClassifier을 위한 build_fn 정의 (build_model)\n",
    "# 신경망 model 생성\n",
    "def build_model(num_layers, num_nodes, loss):\n",
    "    # Define and compile the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(num_nodes, input_dim=50, activation='relu'))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(num_nodes, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='Adam', loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "# KFold validation 사용\n",
    "# k: n_splits=5\n",
    "kfold = KFold(random_state=30,\n",
    "           n_splits=5,\n",
    "           shuffle=True\n",
    "          )\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)\n",
    "\n",
    "# 최적 hyperparameter을 찾기위한 gridsearchCV를 위한 parameter\n",
    "parameters = {\n",
    "    'batch_size': [1000,5000,10000],\n",
    "    'epochs': [45,60,100],\n",
    "    'num_layers': [2, 3],\n",
    "    'num_nodes': [10, 25, 40],\n",
    "    'loss':[\"binary_crossentropy\"]\n",
    "    }\n",
    "# GridSearchCV 생성\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           cv = kfold)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "# GridSearchCV fit 시작\n",
    "grid_search.fit(x_train, y_train, callbacks=[early_stopping])\n",
    "# 최적의 param\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "# 최적의 param일 경우 최적의 accuracy\n",
    "print(f\"Best average accuracy: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search를 수행했을 경우의 각각의 결과s\n",
    "# Best params(PCA1_feature86): {'batch_size': 1000, 'epochs': 60, 'loss': 'binary_crossentropy', 'num_layers':s 2, 'num_nodes': 40\n",
    "# Best params(PCA95_feature50): {'batch_size': 1000, 'epochs': 100, 'loss': 'binary_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "# Best params(PCA95_feature50,reversed): {'batch_size': 1000, 'epochs': 60, 'loss': 'binary_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "\n",
    "result = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 위의 모델을 생성한 결과를 바탕으로 hyperparameter를 설정한 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 모델(without EarlyStopping)\n",
    "- avg accuracy: 0.8004517912864685\n",
    "- avg F1-score: 0.8004525303840637\n",
    "- avg recall: 0.8004526019096374\n",
    "- avg precision: 0.8004526019096374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "   print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(905345,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_reshaped = np.reshape(y_train,(-1))\n",
    "y_train_reshaped.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(905345,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 1000, 'epochs': 60, 'loss': 'binary_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 4ms/step - loss: 0.4718 - accuracy: 0.7984\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4584 - accuracy: 0.8003\n",
      "5659/5659 [==============================] - 4s 721us/step - loss: 0.4583 - accuracy: 0.7994\n",
      "===================================\n",
      "Validation accuracy: 0.799407958984375\n",
      "5659/5659 [==============================] - 4s 670us/step\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4576 - accuracy: 0.8004\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4564 - accuracy: 0.8006\n",
      "5659/5659 [==============================] - 4s 770us/step - loss: 0.4538 - accuracy: 0.8014\n",
      "===================================\n",
      "Validation accuracy: 0.8014127016067505\n",
      "5659/5659 [==============================] - 4s 680us/step\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4558 - accuracy: 0.8010\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4550 - accuracy: 0.8012\n",
      "5659/5659 [==============================] - 4s 745us/step - loss: 0.4532 - accuracy: 0.8014\n",
      "===================================\n",
      "Validation accuracy: 0.8014237880706787\n",
      "5659/5659 [==============================] - 4s 667us/step\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4549 - accuracy: 0.8011\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4544 - accuracy: 0.8011\n",
      "5659/5659 [==============================] - 4s 746us/step - loss: 0.4521 - accuracy: 0.8025\n",
      "===================================\n",
      "Validation accuracy: 0.8024786114692688\n",
      "5659/5659 [==============================] - 4s 689us/step\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4540 - accuracy: 0.8012\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4536 - accuracy: 0.8016\n",
      "5659/5659 [==============================] - 4s 755us/step - loss: 0.4532 - accuracy: 0.8019\n",
      "===================================\n",
      "Validation accuracy: 0.8019484281539917\n",
      "5659/5659 [==============================] - 4s 687us/step\n",
      "###################################\n",
      "avg Validation accuracy: 0.8013342976570129\n",
      "avg Validation F1-score: 0.09479844258760484\n",
      "avg Validation recall: 0.051509963306635896\n",
      "avg Validation precision: 0.5996740328167786\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(40, input_dim=50, activation='relu'))\n",
    "for _ in range(2):\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "for train_index, val_index in kf.split(x_train, y_train_reshaped):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train_reshaped[train_index], y_train_reshaped[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, \n",
    "              Y_train_fold, \n",
    "              batch_size=1000, \n",
    "              epochs=60, \n",
    "              verbose=1,\n",
    "              callbacks=[early_stopping])\n",
    "    # 모델 validation\n",
    "    valid_loss, valid_acc= model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "\n",
    "    pred = model.predict(X_val_fold)\n",
    "    preds_1d = pred.flatten()\n",
    "    pred = np.where(preds_1d > 0.5, 1 , 0)\n",
    "    valid_f1s.append(f1_score(Y_val_fold, pred))\n",
    "    valid_precisions.append(precision_score(Y_val_fold, pred))\n",
    "    valid_recalls.append(recall_score(Y_val_fold, pred))\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7074/7074 [==============================] - 5s 677us/step\n",
      "###################################\n",
      "Validation accuracy: 0.8009958601554319\n",
      "Validation F1-score: 0.08614672942704106\n",
      "Validation recall: 0.04641756127424186\n",
      "Validation precision: 0.5978597578147001\n"
     ]
    }
   ],
   "source": [
    "valid_pred = model.predict(x_validation)\n",
    "valid_preds_1d = valid_pred.flatten()\n",
    "valid_pred = np.where(valid_preds_1d>0.5, 1, 0)\n",
    "print(\"###################################\")\n",
    "print(\"Validation accuracy:\", accuracy_score(y_validation, valid_pred))\n",
    "print(\"Validation F1-score:\", f1_score(y_validation, valid_pred))\n",
    "print(\"Validation recall:\", recall_score(y_validation, valid_pred))\n",
    "print(\"Validation precision:\", precision_score(y_validation, valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dnn_models/log_transformed/pca95_feature50/with_earlystopping001_0422(binary).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 model(with EarlyStopping)\n",
    "### earlystopping: min_delta was 0.001\n",
    "- avg accuracy: 0.7999342799186706\n",
    "- avg F1-score: 0.7999348163604736\n",
    "- avg recall: 0.7999348998069763\n",
    "- avg precision: 0.7999348998069763\n",
    "\n",
    "### earlystopping: min_delta was 0.0001\n",
    "- avg accuracy: 0.80015869140625\n",
    "- avg F1-score: 0.8001592040061951\n",
    "- avg recall: 0.8001593112945556\n",
    "- avg precision: 0.8001593112945556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4687 - accuracy: 0.7984 - f1_score: 0.7984 - recall: 0.7984 - precision: 0.7984\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4575 - accuracy: 0.7998 - f1_score: 0.7999 - recall: 0.7999 - precision: 0.7999\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4541 - accuracy: 0.8005 - f1_score: 0.8006 - recall: 0.8006 - precision: 0.8006\n",
      "===================================\n",
      "Validation accuracy: 0.8005456328392029\n",
      "Validation F1-score: 0.8005503416061401\n",
      "Validation recall: 0.8005504608154297\n",
      "Validation precision: 0.8005504608154297\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4555 - accuracy: 0.8004 - f1_score: 0.8004 - recall: 0.8004 - precision: 0.8004\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4542 - accuracy: 0.8008 - f1_score: 0.8008 - recall: 0.8008 - precision: 0.8008\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4532 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "===================================\n",
      "Validation accuracy: 0.8019374012947083\n",
      "Validation F1-score: 0.801925778388977\n",
      "Validation recall: 0.8019258975982666\n",
      "Validation precision: 0.8019258975982666\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4531 - accuracy: 0.8016 - f1_score: 0.8016 - recall: 0.8016 - precision: 0.8016\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4524 - accuracy: 0.8020 - f1_score: 0.8020 - recall: 0.8020 - precision: 0.8020\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4542 - accuracy: 0.8006 - f1_score: 0.8006 - recall: 0.8006 - precision: 0.8006\n",
      "===================================\n",
      "Validation accuracy: 0.8005732893943787\n",
      "Validation F1-score: 0.8005940914154053\n",
      "Validation recall: 0.8005942106246948\n",
      "Validation precision: 0.8005942106246948\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4525 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4519 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4521 - accuracy: 0.8007 - f1_score: 0.8007 - recall: 0.8007 - precision: 0.8007\n",
      "===================================\n",
      "Validation accuracy: 0.8007334470748901\n",
      "Validation F1-score: 0.8007299900054932\n",
      "Validation recall: 0.8007301092147827\n",
      "Validation precision: 0.8007301092147827\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4524 - accuracy: 0.8014 - f1_score: 0.8014 - recall: 0.8014 - precision: 0.8014\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4519 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4499 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "===================================\n",
      "Validation accuracy: 0.8026387691497803\n",
      "Validation F1-score: 0.8026190400123596\n",
      "Validation recall: 0.8026190996170044\n",
      "Validation precision: 0.8026190996170044\n",
      "###################################\n",
      "avg Validation accuracy: 0.8012857079505921\n",
      "avg Validation F1-score: 0.801283848285675\n",
      "avg Validation recall: 0.8012839555740356\n",
      "avg Validation precision: 0.8012839555740356\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(best_params['num_nodes'], input_dim=86, activation='relu'))\n",
    "for _ in range(best_params['num_layers']):\n",
    "    model.add(Dense(best_params['num_nodes'], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss=best_params['loss'], optimizer='Adam', metrics=[\"accuracy\", f1_score, recall, precision])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, Y_train_fold, \n",
    "              batch_size=best_params['batch_size'], \n",
    "              epochs=best_params['epochs'], \n",
    "              verbose=1,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 validation\n",
    "    valid_loss, valid_acc, valid_f1, valid_recall, valid_precision = model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_f1s.append(valid_f1)\n",
    "    valid_recalls.append(valid_recall)\n",
    "    valid_precisions.append(valid_precision)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "    print(\"Validation F1-score:\", valid_f1)\n",
    "    print(\"Validation recall:\", valid_recall)\n",
    "    print(\"Validation precision:\", valid_precision)\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))\n",
    "model.save('dnn_models/non_log_transformed/pca1_feature86/with_earlystopping_001_0421.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4725 - accuracy: 0.7965 - f1_score: 0.7965 - recall: 0.7965 - precision: 0.7965\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4583 - accuracy: 0.7998 - f1_score: 0.7998 - recall: 0.7998 - precision: 0.7998\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4563 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "===================================\n",
      "Validation accuracy: 0.8016557097434998\n",
      "Validation F1-score: 0.8016602993011475\n",
      "Validation recall: 0.801660418510437\n",
      "Validation precision: 0.801660418510437\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4560 - accuracy: 0.8004 - f1_score: 0.8004 - recall: 0.8004 - precision: 0.8004\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4545 - accuracy: 0.8009 - f1_score: 0.8009 - recall: 0.8009 - precision: 0.8009\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4535 - accuracy: 0.8020 - f1_score: 0.8020 - recall: 0.8020 - precision: 0.8020\n",
      "===================================\n",
      "Validation accuracy: 0.8019981384277344\n",
      "Validation F1-score: 0.801986575126648\n",
      "Validation recall: 0.8019866347312927\n",
      "Validation precision: 0.8019866347312927\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4535 - accuracy: 0.8015 - f1_score: 0.8015 - recall: 0.8015 - precision: 0.8015\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4527 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4533 - accuracy: 0.8011 - f1_score: 0.8011 - recall: 0.8011 - precision: 0.8011\n",
      "===================================\n",
      "Validation accuracy: 0.8010979294776917\n",
      "Validation F1-score: 0.801118791103363\n",
      "Validation recall: 0.801118791103363\n",
      "Validation precision: 0.801118791103363\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4527 - accuracy: 0.8016 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4521 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4517 - accuracy: 0.8013 - f1_score: 0.8013 - recall: 0.8013 - precision: 0.8013\n",
      "===================================\n",
      "Validation accuracy: 0.8013133406639099\n",
      "Validation F1-score: 0.8013017773628235\n",
      "Validation recall: 0.801301896572113\n",
      "Validation precision: 0.801301896572113\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4522 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4517 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4502 - accuracy: 0.8021 - f1_score: 0.8021 - recall: 0.8021 - precision: 0.8021\n",
      "===================================\n",
      "Validation accuracy: 0.8020754456520081\n",
      "Validation F1-score: 0.8020557761192322\n",
      "Validation recall: 0.802055835723877\n",
      "Validation precision: 0.802055835723877\n",
      "###################################\n",
      "avg Validation accuracy: 0.8016281127929688\n",
      "avg Validation F1-score: 0.8016246438026429\n",
      "avg Validation recall: 0.8016247153282166\n",
      "avg Validation precision: 0.8016247153282166\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(best_params['num_nodes'], input_dim=86, activation='relu'))\n",
    "for _ in range(best_params['num_layers']):\n",
    "    model.add(Dense(best_params['num_nodes'], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss=best_params['loss'], optimizer='Adam', metrics=[\"accuracy\", f1_score, recall, precision])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.0001)\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, Y_train_fold, \n",
    "              batch_size=best_params['batch_size'], \n",
    "              epochs=best_params['epochs'], \n",
    "              verbose=1,\n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 validation\n",
    "    valid_loss, valid_acc, valid_f1, valid_recall, valid_precision = model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_f1s.append(valid_f1)\n",
    "    valid_recalls.append(valid_recall)\n",
    "    valid_precisions.append(valid_precision)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "    print(\"Validation F1-score:\", valid_f1)\n",
    "    print(\"Validation recall:\", valid_recall)\n",
    "    print(\"Validation precision:\", valid_precision)\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))\n",
    "model.save('dnn_models/non_log_transformed/pca1_feature86/with_earlystopping_0001_0421.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
