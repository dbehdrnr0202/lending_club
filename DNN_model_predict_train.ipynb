{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hi\\AppData\\Local\\Temp\\ipykernel_21468\\3860009895.py:14: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband                 # type: ignore\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                                     # type: ignore\n",
    "import numpy as np                                      # type: ignore\n",
    "import warnings                                         # type: ignore\n",
    "from sklearn.model_selection import train_test_split    # type: ignore\n",
    "from sklearn.decomposition import PCA                   # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler# type: ignore\n",
    "from sklearn.model_selection import GridSearchCV, KFold # type: ignore\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve# type: ignore\n",
    "import tensorflow as tf                                 # type: ignore\n",
    "from tensorflow import keras                            # type: ignore\n",
    "from keras.layers import Dense, Dropout                 # type: ignore\n",
    "from keras.optimizers import Adam, SGD                  # type: ignore\n",
    "from keras.callbacks import EarlyStopping               # type: ignore\n",
    "from kerastuner.tuners import Hyperband                 # type: ignore\n",
    "from tensorboard.plugins.hparams import api as hp       # type: ignore\n",
    "import matplotlib.pyplot as plt                         # type: ignore\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE# type: ignore\n",
    "from imblearn.under_sampling import TomekLinks          # type: ignore\n",
    "from itertools import combinations_with_replacement\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(physical_devices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = None\n",
    "        self.df = None\n",
    "        self.x_train = None\n",
    "        self.y_train = None\n",
    "        self.input_dim = 0\n",
    "        self.model_name = \"\"\n",
    "\n",
    "    def load_file(self, file_path:str)->None:\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        x_train = self.df.drop(columns=['loan_status', 'Unnamed: 0'])\n",
    "        # x_train = self.df.drop(columns=['loan_status'])\n",
    "        \n",
    "        y_train = self.df['loan_status']\n",
    "        nan_indices = np.isnan(x_train).any(axis=1)\n",
    "        self.x_train = x_train[~nan_indices]\n",
    "        self.y_train = y_train[~nan_indices]\n",
    "    \n",
    "    def __get_n_components_from_pca(self, scaled_data:pd.DataFrame, threshold:float) -> int:\n",
    "        pca = PCA()\n",
    "        pca.fit(scaled_data)\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "        n_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "        return n_components\n",
    "    \n",
    "    def scaling_data(\n",
    "            self, \n",
    "            scaler:MinMaxScaler|StandardScaler=StandardScaler, \n",
    "            threshold:float=0.95, \n",
    "            over_sampler:SMOTE|RandomOverSampler=None, \n",
    "            under_sampler:TomekLinks=None)->None:\n",
    "        x_scaled = scaler().fit_transform(self.x_train)\n",
    "        self.input_dim = self.__get_n_components_from_pca(x_scaled, threshold=threshold)\n",
    "        self.model_name+=(f'scaler({scaler.__name__})_pca({(int(threshold*100))})_input({str(self.input_dim)})')\n",
    "        self.x_train = PCA(n_components=self.input_dim).fit_transform(x_scaled).astype('float32')\n",
    "        if over_sampler!=None:\n",
    "            os = over_sampler(random_state=30)\n",
    "            self.x_train, self.y_train = os.fit_resample(self.x_train,self.y_train)\n",
    "            self.model_name+=f'_sampler({os.__class__.__name__})'\n",
    "            print(f\"====Data set resampled(oversampled)_{os.__class__.__name__}\")\n",
    "        elif under_sampler!=None:\n",
    "            us = under_sampler(random_state=30, sampling_strategy='majority')\n",
    "            self.x_train, self.y_train = us.fit_resample(self.x_train, self.y_train)\n",
    "            self.model_name+=f'_sampler({us.__class__.__name__})'\n",
    "            print(f\"====Data set resampled(undersampled)_{us.__class__.__name__}\")\n",
    "        print(f\"==label ratio\")\n",
    "        print(f\"True Label:\\t{np.sum(self.y_train==1)/len(self.y_train)}\")\n",
    "        print(f\"False Label:\\t{np.sum(self.y_train==0)/len(self.y_train)}\")\n",
    "        self.x_train, self.x_validation, self.y_train, self.y_validation = train_test_split(self.x_train, self.y_train, test_size=0.2, stratify=self.y_train, random_state=30)\n",
    "    \n",
    "    def __build_model(self, num_layers:int, num_nodes:int, loss:str, num_nodes_per_each_layers:list[int]=None)->keras.Sequential:\n",
    "        # Define and compile the model\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(num_nodes, input_dim=self.input_dim, activation='relu'))\n",
    "        for layer_index in range(num_layers):\n",
    "            if num_nodes_per_each_layers:\n",
    "                model.add(Dense(num_nodes_per_each_layers[layer_index], activation='relu'))\n",
    "            else:\n",
    "                model.add(Dense(num_nodes, activation='relu'))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='Adam', loss=loss, metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    def searching_best_param_grid(\n",
    "            self, \n",
    "            grid_params:dict,\n",
    "            params:dict={'min_delta':0.001, \"n_splits\":5},\n",
    "            scoring='accuracy'\n",
    "            ):\n",
    "        kfold = KFold(random_state=30,\n",
    "                n_splits=params['n_splits'],\n",
    "                shuffle=True\n",
    "                )\n",
    "        model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=self.__build_model)\n",
    "        # GridSearchCV 생성\n",
    "        grid_search = GridSearchCV(estimator = model,\n",
    "                                param_grid = grid_params,\n",
    "                                cv = kfold,\n",
    "                                scoring=scoring)\n",
    "        early_stopping = EarlyStopping(monitor='loss',min_delta=params['min_delta'])\n",
    "        # GridSearchCV fit 시작\n",
    "        grid_search.fit(self.x_train, self.y_train, callbacks=[early_stopping])\n",
    "        # 최적의 param\n",
    "        print(f\"Best params: {grid_search.best_params_}\")\n",
    "        # 최적의 param일 경우 최적의 accuracy\n",
    "        print(f\"Best average accuracy: {grid_search.best_score_}\")\n",
    "        return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "    def build_model_with_best_params(\n",
    "            self, \n",
    "            best_params:dict, \n",
    "            params:dict={'min_delta':0.001, \"n_splits\":5}, \n",
    "            threshold:float=0.5)->dict:\n",
    "        valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "        result_dict = {\n",
    "                       'average validation accuracy':0,\n",
    "                       'average validation recall':0,\n",
    "                       'average validation precision':0,\n",
    "                       'average validation f1':0,\n",
    "                       'validation accuracy':0,\n",
    "                       'validation recall':0,\n",
    "                       'validation precision':0,\n",
    "                       'validation f1':0,\n",
    "                       }\n",
    "        y_train_reshaped = np.reshape(self.y_train,(-1))\n",
    "        model = self.__build_model(best_params['num_layers'], best_params['num_nodes'], best_params['loss'])\n",
    "        kf = KFold(n_splits=params['n_splits'], shuffle=True, random_state=30)\n",
    "        early_stopping = EarlyStopping(monitor='loss',min_delta=params['min_delta'])\n",
    "        for train_index, val_index in kf.split(self.x_train, y_train_reshaped):\n",
    "            X_train_fold, X_val_fold = self.x_train[train_index], self.x_train[val_index]\n",
    "            Y_train_fold, Y_val_fold = y_train_reshaped[train_index], y_train_reshaped[val_index]\n",
    "            # 모델 학습\n",
    "            model.fit(X_train_fold, \n",
    "                    Y_train_fold, \n",
    "                    batch_size=best_params['batch_size'], \n",
    "                    epochs=best_params['epochs'], \n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping])\n",
    "            # 모델 validation\n",
    "            valid_loss, valid_acc= model.evaluate(X_val_fold, Y_val_fold)\n",
    "            pred = model.predict(X_val_fold).flatten()\n",
    "            pred = np.where(pred >= threshold, 1 , 0)\n",
    "            valid_accs.append(valid_acc)\n",
    "            valid_f1s.append(f1_score(Y_val_fold, pred))\n",
    "            valid_precisions.append(precision_score(Y_val_fold, pred))\n",
    "            valid_recalls.append(recall_score(Y_val_fold, pred))\n",
    "        result_dict['average validation accuracy'] = np.mean(valid_accs)\n",
    "        result_dict['average validation recall'] = np.mean(valid_recalls)\n",
    "        result_dict['average validation precision'] = np.mean(valid_precisions)\n",
    "        result_dict['average validation f1'] = np.mean(valid_f1s)\n",
    "        print(\"###################################\")\n",
    "        print(\"avg Validation accuracy:\", result_dict['average validation accuracy'])\n",
    "        print(\"avg Validation recall:\", result_dict['average validation recall'])\n",
    "        print(\"avg Validation precision:\", result_dict['average validation precision'])\n",
    "        print(\"avg Validation F1-score:\", result_dict['average validation f1'])\n",
    "\n",
    "        valid_pred = model.predict(self.x_validation).flatten()\n",
    "        valid_pred = np.where(valid_pred>=threshold, 1, 0)\n",
    "\n",
    "        result_dict['validation accuracy'] = accuracy_score(Y_val_fold, pred)\n",
    "        result_dict['validation f1'] = (f1_score(Y_val_fold, pred))\n",
    "        result_dict['validation precision'] = (precision_score(Y_val_fold, pred))\n",
    "        result_dict['validation recall'] = (recall_score(Y_val_fold, pred))\n",
    "        print(\"###################################\")\n",
    "        print(\"Validation accuracy:\", result_dict['validation accuracy'])\n",
    "        print(\"Validation recall:\", result_dict['validation recall'])\n",
    "        print(\"Validation precision:\", result_dict['validation precision'])\n",
    "        print(\"Validation F1-score:\", result_dict['validation f1'])\n",
    "        self.model = model\n",
    "        return result_dict\n",
    "    \n",
    "    def save_model(self, file_path:str=\"dnn_models/0425/dnn_\")->None:\n",
    "        self.model.save(file_path+self.model_name+'.h5')\n",
    " \n",
    "    def load_model(self, model_file_path:str=\"dnn_models/0425/dnn_\"):\n",
    "        self.model = tf.keras.models.load_model(model_file_path)\n",
    "\n",
    "    def get_input_dim(self)->int:\n",
    "        return self.input_dim\n",
    "\n",
    "    def get_model(self)->keras.Sequential:\n",
    "        return self.model\n",
    "\n",
    "    def get_model_name(self)->str:\n",
    "        return self.model_name\n",
    "\n",
    "    def get_roc_auc_score(self, save:bool=True)->float:\n",
    "        valid_pred = self.model.predict(self.x_validation).flatten()\n",
    "        fprs, tprs, thresholds = roc_curve(self.y_validation, valid_pred)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(fprs, tprs, label='ROC')\n",
    "        plt.xlabel(\"FPR(Fall-Out)\")\n",
    "        plt.ylabel(\"TPR(Recall):재현률\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if save:\n",
    "            plt.savefig(f'plots/{self.model_name}.png')\n",
    "        return roc_auc_score(self.y_validation, valid_pred)\n",
    "    \n",
    "    def draw_precision_recall_curve_plot(self, min_delta:float=0.01, save:bool=True)->float:\n",
    "        valid_pred = self.model.predict(self.x_validation).flatten()\n",
    "        precisions, recalls, thresholds = precision_recall_curve(self.y_validation, valid_pred)\n",
    "        # y축인 precisions와 recalls의 갯수 = threshold 갯수 + 1 이어서 x,y 갯수를 동일하게 맞춰줌\n",
    "        thres_boundary = thresholds.shape[0]\n",
    "        plt.plot(thresholds, precisions[:thres_boundary], \"r\", label=\"precision\")\n",
    "        plt.plot(thresholds, recalls[:thres_boundary], label=\"recall\")\n",
    "\n",
    "        # x축 스케일 0.1 단위로 조정\n",
    "        start, end = plt.xlim()\n",
    "        plt.xticks( np.round( np.arange(start, end, 0.1), 2))\n",
    "        # 라벨링\n",
    "        plt.xlabel(\"Threshold value\"), plt.ylabel(\"Precision and Recall value\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if save:\n",
    "            plt.savefig(f'plots/{self.model_name}_precision&recall.png')\n",
    "        min_delta_index = None\n",
    "        for i in range(len(precisions)):\n",
    "            delta = abs(precisions[i] - recalls[i])\n",
    "            if delta <= min_delta:\n",
    "                min_delta_index = i\n",
    "                break\n",
    "        return recalls[min_delta_index]\n",
    "    \n",
    "    def predict_test(self, test_x, test_y, threshold):\n",
    "        y_pred = self.model.predict(test_x).flatten()\n",
    "        y_pred = np.where(y_pred>=threshold, 1, 0)\n",
    "        print(\"###################################\")\n",
    "        print(\"Validation accuracy:\", accuracy_score(test_y, y_pred))\n",
    "        print(\"Validation recall:\", recall_score(test_y, y_pred))\n",
    "        print(\"Validation precision:\", precision_score(test_y, y_pred))\n",
    "        print(\"Validation F1-score:\", f1_score(test_y, y_pred))\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_test_prob(self, test_x):\n",
    "        y_pred = self.model.predict(test_x).flatten()\n",
    "        return y_pred\n",
    "    \n",
    "    def get_y_train(self)->np.ndarray:\n",
    "        return self.y_train\n",
    "    \n",
    "    def get_y_validation(self)->np.ndarray:\n",
    "        return self.y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler_pca_with_origin_data(scaler, n_comp, origin_x):\n",
    "    sc = scaler()\n",
    "    sc.fit(origin_x)\n",
    "    scaled_x = sc.transform(origin_x)\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(scaled_x)\n",
    "    return sc, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_RECOVERY_RATE = 0.44\n",
    "RISK_FREE = 1.04\n",
    "df = pd.read_csv(\"data/modified_0420.csv\")\n",
    "\n",
    "def get_expected_roe(y_pred, y_true, df):\n",
    "    earning = np.sum(df.loc[np.where((y_pred==0)&(y_true==0))]['total_loan_amnt'])\n",
    "    recevored = np.sum(df.loc[np.where((y_pred==0)&(y_true==1))]['total_loan_amnt'])*WEIGHTED_RECOVERY_RATE\n",
    "    risk_free = np.sum(df.loc[np.where(y_pred==1)]['total_loan_amnt'])*RISK_FREE\n",
    "    return earning+recevored+risk_free\n",
    "\n",
    "def get_expected_loss(y_pred, y_true, df):\n",
    "    return np.sum(df.loc[np.where((y_pred==0)&(y_true==1))]['total_loan_amnt'])*(1-WEIGHTED_RECOVERY_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oridin_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m origin_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/lending_club_2020_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43moridin_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_count()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'oridin_df' is not defined"
     ]
    }
   ],
   "source": [
    "origin_df = pd.read_csv(\"data/lending_club_2020_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_status\n",
       "Fully Paid                                             599261\n",
       "Current                                                412328\n",
       "Charged Off                                            145182\n",
       "Late (31-120 days)                                       6314\n",
       "In Grace Period                                          3979\n",
       "Late (16-30 days)                                        1099\n",
       "Issued                                                    804\n",
       "Does not meet the credit policy. Status:Fully Paid        765\n",
       "Does not meet the credit policy. Status:Charged Off       301\n",
       "Default                                                   165\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1805994, 89)\n",
      "(1805994,)\n",
      "23584/23584 [==============================] - 18s 758us/step\n",
      "###################################\n",
      "Validation accuracy: 0.5621627709757778\n",
      "Validation recall: 0.46560610630959093\n",
      "Validation precision: 0.2219311927035635\n",
      "Validation F1-score: 0.30058738239122845\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'dnn_scaler(StandardScaler)_pca(100)_input(86)_sampler(SMOTE).h5'\n",
    "dnn_model = DNNModel()\n",
    "dnn_model.load_model(\"dnn_models/0425/\"+model_file_name)\n",
    "df =pd.read_csv('data/modified_0420.csv')\n",
    "X = df.drop(columns=['Unnamed: 0', 'loan_status'])\n",
    "Y = df['loan_status']\n",
    "resampled_x, resampled_y = SMOTE(random_state=30).fit_resample(X, Y)\n",
    "print(resampled_x.shape)\n",
    "print(resampled_y.shape)\n",
    "df = pd.read_csv(\"data/modified_test.csv\")\n",
    "x_test = df.drop(columns=['loan_status', 'Unnamed: 0']).to_numpy()\n",
    "y_test = df['loan_status'].to_numpy()\n",
    "sc, pca = get_scaler_pca_with_origin_data(StandardScaler, n_comp=86, origin_df=resampled_x)\n",
    "x_test = sc.transform(x_test)\n",
    "x_test = pca.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 19s 798us/step\n",
      "###################################\n",
      "Validation accuracy: 0.568056659776329\n",
      "Validation recall: 0.44992721216015946\n",
      "Validation precision: 0.2208286263264104\n",
      "Validation F1-score: 0.2962532787858507\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_model.predict_test(x_test, y_test, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 20s 839us/step\n",
      "###################################\n",
      "Validation accuracy: 0.5599870143637038\n",
      "Validation recall: 0.4712324096053719\n",
      "Validation precision: 0.22227996102630723\n",
      "Validation F1-score: 0.3020725399490955\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_model.predict_test(x_test, y_test, threshold=0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 18s 760us/step\n",
      "###################################\n",
      "Validation accuracy: 0.6311774526951821\n",
      "Validation recall: 0.2904038085745387\n",
      "Validation precision: 0.20654145893282716\n",
      "Validation F1-score: 0.24139650872817955\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_model.predict_test(x_test, y_test, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 19s 790us/step\n",
      "###################################\n",
      "Validation accuracy: 0.6148685535591244\n",
      "Validation recall: 0.32751249196710774\n",
      "Validation precision: 0.209815033418332\n",
      "Validation F1-score: 0.2557734015122612\n"
     ]
    }
   ],
   "source": [
    "y_pred = dnn_model.predict_test(x_test, y_test, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df.drop(columns=['loan_status', 'Unnamed: 0']).to_numpy()\n",
    "y_test = df['loan_status'].to_numpy()\n",
    "x_test = sc.transform(X)\n",
    "x_test = pca.transform(x_test)\n",
    "df['total_loan_amnt'] = df['loan_amnt']*(df['int_rate']/100+1)\n",
    "dnn_model.load_model(model_file_path='dnn_models/0425/'+model_file_name)\n",
    "y_prob = dnn_model.predict_test_prob(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23584/23584 [==============================] - 19s 797us/step\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/modified_test.csv\")\n",
    "\n",
    "x_test = df.drop(columns=['loan_status', 'Unnamed: 0']).to_numpy()\n",
    "y_test = df['loan_status'].to_numpy()\n",
    "x_test = sc.transform(x_test)\n",
    "x_test = pca.transform(x_test)\n",
    "df['total_loan_amnt'] = df['loan_amnt']*(df['int_rate']/100+1)\n",
    "dnn_model.load_model(model_file_path='dnn_models/0425/'+model_file_name)\n",
    "y_prob = dnn_model.predict_test_prob(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.45\n",
    "y_pred = np.where(y_prob>=THRESHOLD, 1, 0)\n",
    "expected_income = get_expected_income(y_pred, y_test, df)\n",
    "amnt = np.sum(df.loc[np.where(y_pred==1)]['loan_amnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자기자본이익률(ROE) = 6171512223/4859168725 = 1.27\n"
     ]
    }
   ],
   "source": [
    "print(f\"자기자본이익률(ROE) = {expected_income:.0f}/{amnt:.0f} = {expected_income/amnt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.45\n",
    "y_pred = np.where(y_prob>=THRESHOLD, 1, 0)\n",
    "expected_income = get_expected_income(y_pred, Y, df)\n",
    "amnt = np.sum(df.loc[np.where(y_pred==1)]['loan_amnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자기자본이익률(ROE) = 9766200928/7032811450 = 1.39\n"
     ]
    }
   ],
   "source": [
    "print(f\"자기자본이익률(ROE) = {expected_income:.0f}/{amnt:.0f} = {expected_income/amnt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
