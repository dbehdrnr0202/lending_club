{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/modified_0420.csv\")\n",
    "x_train = df.drop(columns=['loan_status','Unnamed: 0'])\n",
    "y_train = df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = np.isnan(x_train).any(axis=1)\n",
    "x_train = x_train[~nan_indices]\n",
    "y_train = y_train[~nan_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1131682, 89)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax scaler 찾아보기\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15351255 0.23887498 0.29132949 0.33460378 0.37342152 0.40992361\n",
      " 0.44018213 0.46830687 0.49304401 0.51586528 0.53721772 0.55750293\n",
      " 0.57607956 0.59350761 0.60844465 0.62254463 0.63602833 0.64925373\n",
      " 0.66216411 0.67429034 0.68602493 0.69753601 0.70896553 0.7203637\n",
      " 0.73171759 0.74305794 0.75434201 0.76561303 0.77685392 0.78806636\n",
      " 0.79920845 0.81031344 0.8213071  0.83225873 0.843079   0.85338506\n",
      " 0.86359883 0.87338488 0.88257324 0.89131271 0.89893998 0.90642617\n",
      " 0.91346178 0.9200401  0.92612666 0.93178941 0.93700023 0.94206948\n",
      " 0.9466856  0.95121647 0.95564362 0.95970912 0.96346197 0.96705088\n",
      " 0.97012257 0.9729337  0.97537772 0.97757369 0.97956607 0.98142723\n",
      " 0.9830844  0.98459436 0.98608351 0.98749547 0.98884414 0.99014148\n",
      " 0.99140624 0.99253264 0.99357978 0.99458026 0.99551418 0.99635191\n",
      " 0.99705684 0.99757707 0.99805306 0.99842364 0.99877569 0.99905902\n",
      " 0.99931589 0.99949095 0.99965833 0.99977262 0.99987296 0.99993982\n",
      " 0.99999192 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABpxUlEQVR4nO3de1xT9f8H8Ne4jTuIyEVEQLwiKiqBeMlSFC95qcxLKmppv7xkilZaKaEpmkV28ZL1VSvzfklNQ8lL5ZW85v2KYsZFUwFBLm6f3x+05dzADTY2ttfz8aDc55yd8975jO3N53wuEiGEABEREZGZsDJ2AERERET6xOSGiIiIzAqTGyIiIjIrTG6IiIjIrDC5ISIiIrPC5IaIiIjMCpMbIiIiMitMboiIiMisMLkhIiIis8LkxowMHz4cgYGBej3m8uXLIZFIcO3aNb0e15RV5joGBgZi+PDheo1HW4ao/8oyxZgqIjAwEM8995yxwzAqiUSCcePGGTsMrTx8+BBvv/02/P39YWVlhb59+xo7JKpiTG4ec+XKFfzf//0f6tWrB3t7e7i6uqJdu3b47LPP8ODBA2OHZzCzZ8/Gjz/+aOwwlBRJVVk/hw4dMnaI1U52djZsbGwwZMiQMvfJy8uDg4MDXnjhhSqMjADg2rVryvf3hg0b1LZ/8MEHkEgkuH37thGiq16WLl2KefPmoV+/fvj2228xceLEJz5n06ZN6N69Ozw9PWFnZ4fatWujf//+2L17dxVEbN4KCgrwwQcfYO/evVV2TpsqO1M1sG3bNrz00kuQSqWIjY1FaGgoiouLsW/fPrz11ls4c+YMlixZYuwwDWL27Nno16+f2l84Q4cOxcCBAyGVSo0S14wZMxAUFKRWXr9+fSNE82QXLlyAlZVp/s3g5eWFLl26YPPmzSgoKICjo6PaPhs3bkRhYWG5CZAuvv76a8jlcr0cy5LMmDEDL7zwAiQSibFDqZZ2794NPz8/fPrpp0/cVwiBV155BcuXL0fLli0RFxcHHx8fZGRkYNOmTejcuTP279+Ptm3bVkHk5qmgoAAJCQkAgGeeeaZKzsnk5l9paWkYOHAgAgICsHv3bvj6+iq3jR07FpcvX8a2bduMGKFxWFtbw9ra2mjn7969O8LDw412fl0ZKwnU1uDBg5GcnIwtW7Zg4MCBattXrlwJNzc39OzZs1Lnyc/Ph5OTE2xtbSt1HEsUFhaGEydOYNOmTRbXglZYWAg7O7tK/4GQnZ0Nd3d3rfb95JNPsHz5ckyYMAFJSUkqCeV7772H77//HjY2/KqsbkzzT0wj+Oijj3D//n3873//U0lsFOrXr48333wTwH/Nx8uXL1fbTyKR4IMPPlA+VjQlX7x4EUOGDIGbmxtq1aqFadOmQQiBGzduoE+fPnB1dYWPjw8++eQTleOV1edl7969kEgkT2zm+/jjj9G2bVvUrFkTDg4OaN26NdavX68Wc35+Pr799ltls7ii38jj53/uuedQr149jeeKiopSS0RWrFiB1q1bw8HBAR4eHhg4cCBu3LhRbsy6iI+Ph5WVFXbt2qVS/tprr8HOzg4nT54E8N/1WrNmDd599134+PjAyckJvXv31ioeba4joN7nRnH99u/fj7i4ONSqVQtOTk54/vnncevWLbXn//zzz+jQoQOcnJzg4uKCnj174syZM2r7/fjjjwgNDYW9vT1CQ0OxadOmJ74GAHj++efh5OSElStXqm3Lzs7Grl270K9fP0ilUvz+++946aWXULduXUilUvj7+2PixIlqt2eHDx8OZ2dnXLlyBT169ICLiwsGDx6s3PZ4nxttr6Wij4fitUqlUjRt2hTJyclq+968eROvvvoqateuDalUiqCgIIwePRrFxcXKfe7du4cJEybA398fUqkU9evXx9y5c3VqWdq5cyfCwsJgb2+PkJAQbNy4Ubnt6tWrkEgkGlsLDhw4AIlEglWrVj3xHAMHDkTDhg0xY8YMCCHK3besPl7PPPOMyl/Iivf/2rVrkZCQAD8/P7i4uKBfv37IyclBUVERJkyYAC8vLzg7O2PEiBEoKirSeM4ffvgBjRo1gr29PVq3bo3ffvtNbZ+bN2/ilVdegbe3t7Leli5dqrKPIqbVq1fj/fffh5+fHxwdHZGbm1vm683Pz8ekSZOUddioUSN8/PHHyuuk+Gzes2cPzpw5o/w8K+tz8sGDB0hMTETjxo3x8ccfa2wpGzp0KCIiIpSPr169ipdeegkeHh5wdHREmzZt1P7w1cf1Vrz/tbnex48fR/fu3eHq6gpnZ2d07txZ7da9IT6LFL/7N2/eRN++feHs7IxatWph8uTJkMlkyjqpVasWACAhIUFZJ4rvyczMTIwYMQJ16tSBVCqFr68v+vTpU/l+noKEEEL4+fmJevXqabVvWlqaACCWLVumtg2AiI+PVz6Oj48XAERYWJgYNGiQWLhwoejZs6cAIJKSkkSjRo3E6NGjxcKFC0W7du0EAPHrr78qn79s2TIBQKSlpamcZ8+ePQKA2LNnj7Js2LBhIiAgQGW/OnXqiDFjxogvv/xSJCUliYiICAFA/PTTT8p9vv/+eyGVSkWHDh3E999/L77//ntx4MABjef/7rvvBACRmpqqcp5r164JAGLevHnKsg8//FBIJBIxYMAAsXDhQpGQkCA8PT1FYGCguHv3brnXWHHeX375Rdy6dUvl5/bt28r9iouLRcuWLUVAQIDIzc0VQgiRnJwsAIiZM2eqXa9mzZqJ5s2bi6SkJDFlyhRhb28vGjZsKAoKCip9HYUQIiAgQAwbNkztdbRs2VJ06tRJfPHFF2LSpEnC2tpa9O/fX+W53333nZBIJKJbt27iiy++EHPnzhWBgYHC3d1dpf537NghrKysRGhoqEhKShLvvfeecHNzE02bNlWLW5OXX35Z2NnZiX/++Uel/PPPPxcAxO7du4UQQrzxxhuiR48eYvbs2eKrr74Sr776qrC2thb9+vVTed6wYcOEVCoVwcHBYtiwYWLx4sXiu+++q/S1BCBatGghfH19xcyZM8X8+fNFvXr1hKOjo8p74ObNm6J27drC0dFRTJgwQSxevFhMmzZNNGnSRPk+y8/PF82bNxc1a9YU7777rli8eLGIjY0VEolEvPnmm0+8ZgEBAaJhw4bC3d1dTJkyRSQlJYlmzZoJKysrsXPnTuV+7dq1E61bt1Z7/pgxY4SLi4vIz88v8xyKz5V58+Ypf882bNig3K74LLl165ZKXI++3xQ6duwoOnbsqHyseP+HhYWJqKgo8fnnn4vx48cLiUQiBg4cKF5++WXRvXt3sWDBAjF06FABQCQkJKgcE4AIDQ0Vnp6eYsaMGWLu3LkiICBAODg4iFOnTin3y8zMFHXq1BH+/v5ixowZYtGiRaJ3794CgPj000/VYgoJCRFhYWEiKSlJJCYmlnmN5HK56NSpk5BIJGLkyJHiyy+/FL169RIAxIQJE4QQQty/f198//33onHjxqJOnTrKz7PMzEyNx9y5c6cAIGbMmFFmvTwqMzNTeHt7CxcXF/Hee++JpKQk0aJFC2FlZSU2btxolOt9+vRp4eTkpPw9mTNnjggKChJSqVQcOnRIuZ8hPouGDRsm7O3tRdOmTcUrr7wiFi1aJF588UUBQCxcuFBZJ4sWLRIAxPPPP6+sk5MnTwohhGjbtq1wc3MT77//vvjmm2/E7NmzxbPPPqvyPVgRTG6EEDk5OQKA6NOnj1b7VyS5ee2115RlDx8+FHXq1BESiUTMmTNHWX737l3h4OCg8cuxosnNo1/aQpQmA6GhoaJTp04q5U5OTho/JB8/f05OjpBKpWLSpEkq+3300UdCIpGI69evCyFKkx1ra2sxa9Yslf1OnTolbGxs1MrLOq+mH6lUqnZMOzs7MXLkSHH37l3h5+cnwsPDRUlJiXIfxfXy8/NTJkFCCLF27VoBQHz22WfKsspcx7KSm+joaCGXy5XlEydOFNbW1uLevXtCCCHy8vKEu7u7GDVqlMrxMjMzhZubm0p5WFiY8PX1VT5XiP8+pLVJbrZt2yYAiK+++kqlvE2bNsLPz0/IZDKNr1kIIRITE1XqWYjS6wVATJkyRW3/ylxLAMLOzk5cvnxZWXby5EkBQHzxxRfKstjYWGFlZSX++OMPtfMrrvnMmTOFk5OTuHjxosr2KVOmCGtra5Genq723EcFBASoJRs5OTnC19dXtGzZUln21VdfCQDi3LlzKq/P09NT4+/Xox5Nbh4+fCgaNGggWrRooXwN+khuQkNDRXFxsbJ80KBBQiKRiO7du6s8PyoqSq3eFL9/R44cUZZdv35d2Nvbi+eff15Z9uqrrwpfX1+VBFQIIQYOHCjc3NyU9a+IqV69ehrfa4/78ccfBQDx4YcfqpT369dPSCQSlfdJx44dRdOmTZ94zM8++0wAEJs2bXrivkIIMWHCBAFA/P7778qyvLw8ERQUJAIDA5W/O1V5vfv27Svs7OzElStXlGV///23cHFxEU8//bSyzBCfRYrf/ceTw5YtW6ok+bdu3VL7bhSi9Dvv8T+K9YW3pQBlM6iLi4vBzjFy5Ejlv62trREeHg4hBF599VVlubu7Oxo1aoSrV6/q7bwODg7Kf9+9exc5OTno0KEDjh07VqHjubq6onv37li7dq1Kk/maNWvQpk0b1K1bF0Bpx1S5XI7+/fvj9u3byh8fHx80aNAAe/bs0ep8CxYsQEpKisrPzz//rLJPaGgoEhIS8M033yAmJga3b9/Gt99+q/E+eWxsrEo99+vXD76+vti+fXu5cVT2Or722msqTd4dOnSATCbD9evXAQApKSm4d+8eBg0apHK9rK2tERkZqbxeGRkZOHHiBIYNGwY3Nzfl8bp06YKQkBCtYunatStq1aqlcmsqLS0Nhw4dwqBBg5T9HR59zfn5+bh9+zbatm0LIQSOHz+udtzRo0drdX5drmV0dDSCg4OVj5s3bw5XV1fl74hcLsePP/6IXr16aeybpbjm69atQ4cOHVCjRg2V6xsdHQ2ZTKaxqf9xtWvXxvPPP6987OrqitjYWBw/fhyZmZkAgP79+8Pe3h4//PCDcr8dO3bg9u3bOnXStra2xvvvv4+TJ0/qdRRjbGysSj+oyMhIZYfaR0VGRuLGjRt4+PChSnlUVBRat26tfFy3bl306dMHO3bsgEwmgxACGzZsQK9evSCEULnWMTExyMnJUavnYcOGqbwnyrJ9+3ZYW1tj/PjxKuWTJk2CEELtc0Ebun72b9++HREREWjfvr2yzNnZGa+99hquXbuGs2fPquxv6Ostk8mwc+dO9O3bV6W7gK+vL15++WXs27dP7Tafvj6LHvX666+rPO7QoYNW32MODg6ws7PD3r17cffu3Sfurwv2kkLphxRQOgzWUBRf+gpubm6wt7eHp6enWvk///yjt/P+9NNP+PDDD3HixAmVe7qVGYUxYMAA/Pjjjzh48CDatm2LK1eu4OjRo5g/f75yn0uXLkEIgQYNGmg8hrYdTSMiIrTqUPzWW29h9erVSE1NxezZs8v8on88HolEgvr16z/x/m5lr+Pj9V+jRg0AUP5CX7p0CQDQqVMnjc9XvEcVH0CarmujRo20SrZsbGwwYMAALFy4EDdv3oSfn58y0VH0lQGA9PR0TJ8+HVu2bFH74MnJyVE7Zp06dZ54bkC3a/n4dQNKr50inlu3biE3NxehoaHlnvPSpUv4888/lff+H5ednf3EuOvXr68WY8OGDQGU9ivw8fGBu7s7evXqhZUrV2LmzJkASvuo+Pn5lVm3ZRk8eDBmzpyJGTNm6G2eFk2fQwDg7++vVi6Xy5GTk4OaNWsqyzW97xo2bIiCggLcunULVlZWuHfvHpYsWVLmyNLHr7Wm0ZCaXL9+HbVr11ZLRJo0aaLcritdP/uvX7+OyMhItfJHY3j0vWjo6w2UjkRq1KiRxpjkcjlu3LiBpk2blhlTRT+LFOzt7dV+rx79HS2PVCrF3LlzMWnSJHh7e6NNmzZ47rnnEBsbCx8fnyc+vzxMblBaWbVr18bp06e12r+sLzRFBypNNI04KmsU0qMtIhU5l8Lvv/+O3r174+mnn8bChQvh6+sLW1tbLFu2TGOHUm316tULjo6OWLt2Ldq2bYu1a9fCysoKL730knIfuVwOiUSCn3/+WePrdHZ2rvD5Nbl69aryl/LUqVN6PbY+ruOT6lrRqfX777/X+Eut79EaQ4YMwZdffolVq1Zh8uTJWLVqFUJCQhAWFgag9P3VpUsX3LlzB++88w4aN24MJycn3Lx5E8OHD1frhCuVSrUa4aLrtdTmd0QbcrkcXbp0wdtvv61xuyJJ0YfY2FisW7cOBw4cQLNmzbBlyxaMGTNG5xFAitab4cOHY/PmzRr3Ke/zQZfPHH1eZ6D0/TVs2DCN+zRv3lzlsTatNobSuHFjAKWfGYaY6M/Q17si9P1ZVNnRtBMmTECvXr3w448/YseOHZg2bRoSExOxe/dutGzZssLHZXLzr+eeew5LlizBwYMHERUVVe6+ikz33r17KuUV+cvhSSpzrg0bNsDe3h47duxQGaK8bNkytX11aclxcnLCc889h3Xr1iEpKQlr1qxBhw4dULt2beU+wcHBEEIgKChIr18cmsjlcgwfPhyurq6YMGGCcs4eTcNoFQmQghACly9fVvvAfZQu17GiFLdevLy8EB0dXeZ+AQEBANRfB1A6x462IiMjERwcjJUrV6JLly44c+YMZs2apdx+6tQpXLx4Ed9++y1iY2OV5SkpKVqfQxN9X8tatWrB1dX1iX+YBAcH4/79++Ve2ye5fPkyhBAqvysXL14EAJURYd26dUOtWrXwww8/IDIyEgUFBRg6dGiFzjlkyBB8+OGHSEhIQO/evdW216hRQ+2zASj9fChrVGNlaHrfXbx4EY6Ojsq/3l1cXCCTySp1rTUJCAjAL7/8gry8PJXWm/Pnzyu366p9+/aoUaMGVq1ahXffffeJX9QBAQEaf88qE0N5tLnejo6OZcZkZWWl1kr0JNp+FuniSd8vwcHBmDRpEiZNmoRLly4hLCwMn3zyCVasWFHhc7LPzb/efvttODk5YeTIkcjKylLbfuXKFXz22WcASlt6PD091e7TL1y4UO9xKd5oj55LJpNpNZmgtbU1JBKJSivPtWvXNN7Dd3Jy0vghWZYBAwbg77//xjfffIOTJ09iwIABKttfeOEFWFtbIyEhQe2vESGEXm+9JSUl4cCBA1iyZAlmzpyJtm3bYvTo0Rpncv3uu+9UmqDXr1+PjIwMdO/evczj63IdKyomJgaurq6YPXs2SkpK1LYrmqB9fX0RFhaGb7/9VuXWUEpKitr9/icZPHgwjh8/jvj4eEgkErz88svKbYoP+UfrTgih/B2oKH1fS8XU+lu3bsWRI0fUtivi79+/Pw4ePIgdO3ao7XPv3j21vg6a/P333ypD7nNzc/Hdd98hLCxM5S9cGxsbDBo0CGvXrsXy5cvRrFmzcpPn8ihab06cOIEtW7aobQ8ODsahQ4dUhrz/9NNPep1u4VEHDx5UufV548YNbN68GV27dlXOifXiiy9iw4YNGhNOTUOOtdWjRw/IZDJ8+eWXKuWffvopJBJJub/DZXF0dMQ777yDc+fO4Z133tHYcrJixQqkpqYqY0hNTcXBgweV2/Pz87FkyRIEBgZq3e9NW9pc765du2Lz5s0qt9azsrKwcuVKtG/fXu020pNo+1mkC8WEoY9/xxQUFKCwsFClLDg4GC4uLmVORaAtttz8S/FX7IABA9CkSROVGYoPHDiAdevWqcwnMXLkSMyZMwcjR45EeHg4fvvtN+VfcfrUtGlTtGnTBlOnTsWdO3fg4eGB1atXa/Vh3LNnTyQlJaFbt254+eWXkZ2djQULFqB+/fr4888/VfZt3bo1fvnlFyQlJaF27doICgrSeG9ZQTGfyeTJk5UfaI8KDg7Ghx9+iKlTp+LatWvo27cvXFxckJaWhk2bNuG1117D5MmTn/gafv75Z+VfRY9q27Yt6tWrh3PnzmHatGkYPnw4evXqBaB0PoewsDCMGTMGa9euVXmeh4cH2rdvjxEjRiArKwvz589H/fr1MWrUKL1cx4pydXXFokWLMHToULRq1QoDBw5ErVq1kJ6ejm3btqFdu3bKD/XExET07NkT7du3xyuvvII7d+7giy++QNOmTXH//n2tzzlkyBDMmDEDmzdvRrt27VRaHxo3bozg4GBMnjwZN2/ehKurKzZs2FDpTn+GuJazZ8/Gzp070bFjR7z22mto0qQJMjIysG7dOuzbtw/u7u546623sGXLFjz33HMYPnw4Wrdujfz8fJw6dQrr16/HtWvX1Pq/Pa5hw4Z49dVX8ccff8Db2xtLly5FVlaWxlan2NhYfP7559izZw/mzp1bodeloOh7c+LECbVtI0eOxPr169GtWzf0798fV65cwYoVK1Q6YetTaGgoYmJiMH78eEilUuUfdIrZZwFgzpw52LNnDyIjIzFq1CiEhITgzp07OHbsGH755RfcuXOnQufu1asXnn32Wbz33nu4du0aWrRogZ07d2Lz5s2YMGFChV+zYvb5Tz75BHv27EG/fv3g4+ODzMxM/Pjjj0hNTcWBAwcAAFOmTMGqVavQvXt3jB8/Hh4eHvj222+RlpaGDRs26H12cm2u94cffoiUlBS0b98eY8aMgY2NDb766isUFRXho48+0vmcunwWacvBwQEhISFYs2YNGjZsCA8PD4SGhuLhw4fo3Lkz+vfvj5CQENjY2GDTpk3IysrSOMmoTvQ+/qqau3jxohg1apQIDAwUdnZ2wsXFRbRr10588cUXorCwULlfQUGBePXVV4Wbm5twcXER/fv3F9nZ2WUOBX90+KYQpUPonJyc1M6vaQjjlStXRHR0tJBKpcLb21u8++67IiUlRauh4P/73/9EgwYNhFQqFY0bNxbLli1TxvSo8+fPi6efflo4ODgIAMrhpWUNRRdCiMGDByuHFpZlw4YNon379sLJyUk4OTmJxo0bi7Fjx4oLFy6U+ZxHz1vWz7Jly8TDhw/FU089JerUqaMyLFqI/4Z4rlmzRgjx39DMVatWialTpwovLy/h4OAgevbsqTKsubLXsayh4I8PU9Y0lF9RHhMTI9zc3IS9vb0IDg4Ww4cPVxkOqriuTZo0EVKpVISEhIiNGzdqjPtJnnrqKZU5KR519uxZER0dLZydnYWnp6cYNWqUcij2o9MglPVeVmyr6LUEIMaOHat2TE3Dn69fvy5iY2NFrVq1hFQqFfXq1RNjx44VRUVFyn3y8vLE1KlTRf369YWdnZ3w9PQUbdu2FR9//LHKcF1NAgICRM+ePcWOHTtE8+bNlbGvW7euzOc0bdpUWFlZib/++qvcYys8OhT8cY/+Pjz+WfLJJ58IPz8/IZVKRbt27cSRI0fKHAr+eLxlvT81fW4p6mPFihXK+mvZsqXae1gIIbKyssTYsWOFv7+/sLW1FT4+PqJz585iyZIlT4ypPHl5eWLixImidu3awtbWVjRo0EDMmzdPZWizENoPBX/U+vXrRdeuXYWHh4ewsbERvr6+YsCAAWLv3r0q+125ckX069dPuLu7C3t7exEREaE2R1NVX+9jx46JmJgY4ezsLBwdHcWzzz6rnKvsSeeuzGdRWb/7mn6fDxw4IFq3bi3s7OyU35O3b98WY8eOFY0bNxZOTk7Czc1NREZGirVr16odU1cSIaqgBxORke3duxfPPvss1q1bh379+hk7HLIALVu2hIeHh9rs2UTakEgkGDt2rM6tJFSKfW6IiPTsyJEjOHHihEpnbCKqOuxzQ0SkJ6dPn8bRo0fxySefwNfXV62jPRFVDbbcEBHpyfr16zFixAiUlJRg1apVsLe3N3ZIRBaJfW6IiIjIrLDlhoiIiMwKkxsiIiIyKxbXoVgul+Pvv/+Gi4tLpRaPJCIioqojhEBeXh5q1679xAkTLS65+fvvv3Vea4OIiIhMw40bN1CnTp1y97G45Eax4NqNGzd0XnPjSUpKSrBz50507doVtra2ej02VRzrxXSxbkwT68V0WXLd5Obmwt/fX2Xh1LJYXHKjuBXl6upqkOTG0dERrq6uFvemM2WsF9PFujFNrBfTxbp58irjADsUExERkZlhckNERERmhckNERERmRUmN0RERGRWmNwQERGRWWFyQ0RERGaFyQ0RERGZFSY3REREZFaY3BAREZFZsbgZiomIiAxNJhdITbuD7LxCeLnYIyLIAwC0KrO2kpT5/MNpd3D0tgQ10+4gqr6X1sfUd1l5MWrat6oZNbn57bffMG/ePBw9ehQZGRnYtGkT+vbtW+5z9u7di7i4OJw5cwb+/v54//33MXz48CqJl4iIqr/KJB7alN3NL8bMbWeRkVOoPKe7Y+lSCfcKSsot83WzR+8WvthyMqOc51vju0tHtD6mvsu0i/G/feN7haBbqC+qklGTm/z8fLRo0QKvvPIKXnjhhSfun5aWhp49e+L111/HDz/8gF27dmHkyJHw9fVFTExMFURMRETGpkuLweP7Vibx0LZME03bNZVl5BTiq9/SKvz8qijTJcbMnEKMXnEMi4a0qtIEx6jJTffu3dG9e3et91+8eDGCgoLwySefAACaNGmCffv24dNPP2VyQ0RUzWmTtGibnJTVuqCJvr/86T8CgARAwtaz6BLiU2W3qKpVn5uDBw8iOjpapSwmJgYTJkwo8zlFRUUoKipSPs7NzQVQurJqSYl+35SK4+n7uFQ5rBfTxboxTfquF5lc4Mj1u8jOK4KXixThATUAQKXsTn4xZv98AZm5/31euzvYAJDg3gPdW0XKal2gqidQWh8HL2cj8t+EtSJ0eT9Wq+QmMzMT3t7eKmXe3t7Izc3FgwcP4ODgoPacxMREJCQkqJXv3LkTjo6OBokzJSXFIMelymG9mC7WjWl6Ur3IBXAlV4LcEsDVFgh2FQBUy/IfApuuWeFe8X9/sTvalO5X8PDRv+LFv///r+y/pObx/bT961+XfcnQdv5+GP+cE0/esQwFBQVa71utkpuKmDp1KuLi4pSPc3Nz4e/vj65du8LV1VWv5yopKUFKSgq6dOkCW1tbvR6bKo71YrpYN6bp8XrR1PLyy7lsJG4/X6GWFtWkRqEyZWVhYmNKunaIrFTLjeLOizaqVXLj4+ODrKwslbKsrCy4urpqbLUBAKlUCqlUqlZua2trsA9TQx6bKo71YrpYN8ZRVh+XY4rhxn/lIbdQrrGPi8b+Jw8eVlnsVH1IAPi42SOqvlel+tzo8hlRrZKbqKgobN++XaUsJSUFUVFRRoqIiKh60H3UUOlwY03YiZa0pUhl4nuFVOl8N0ZNbu7fv4/Lly8rH6elpeHEiRPw8PBA3bp1MXXqVNy8eRPfffcdAOD111/Hl19+ibfffhuvvPIKdu/ejbVr12Lbtm3GeglERCZHm0RGEyYthmOIOWSqwzw3PpY4z82RI0fw7LPPKh8r+sYMGzYMy5cvR0ZGBtLT05Xbg4KCsG3bNkycOBGfffYZ6tSpg2+++YbDwInIYlU0kaH/GOLLf1rPJqjhJK3w7L9vd2uicd+Dl7Ox8/fD6Noh0ugzFJcVoynMUCwRQlS863I1lJubCzc3N+Tk5BikQ/H27dvRo0cP9h8wIawX08W60Q0TGc10aV2obOKhbZmhvtAt+XdGl+/vatXnhojIUjCRqXyrSFmtC2UlHlHBNfVaRsbD5IaIyIg0jVhKOZuJhK3mm8joI2nRRFOCYW0lYeJhgZjcEBFVEW1HLJl6x14JSqfHezzWqkxaiMrD5IaIqAokn87QqjXG1BMb4L8RMF1CfCrV/4RJCxkKkxsiIgN4tJXm2u0CzP/lIkx99IYuLS+KhIX9T8gUMbkhItIzbVtpjKm820WPDzc2xlBeospgckNEVEmm3krzpNaXx0UGeeCfcwKRRpqjhKiymNwQEenA1Ido65rIEJkjJjdERFoytdtNTGSINGNyQ0RUBmPcbiprmDUTGSLtMbkhItLAWK005Q2zZiJDpB0mN0REME4rTUWGWRPRkzG5ISKLVxWtNIrbTROjGyDQ04mtMUQGxOSGiCyOMVppFLebuoX6GvhMRMTkhogsCltpiMwfkxsiMlua5qQZu/IYW2mIzByTGyIyS5paaKwk0HtiwyHaRKaHyQ0RmQVt+tHIK5nZ8HYTUfXA5IaIqr2qmpOGt5uIqgcmN0RUrSWfzsDoFfrvR8NWGqLqi8kNEVU7iltQmTkPMHPbOYN0EGYrDVH1xeSGiKoVQ9yCYisNkXlhckNEJs0QE+5ZSVQ7F7OVhsi8MLkhIpOl71YaRTvMl4Nacug2kRljckNEJskQHYXZQkNkGZjcEJHJ0GdHYfajIbJcTG6IyCTo+xYUW2mILBeTGyIyuh1nsvDG6pOVvgXl4WSLac81hY8rW2mILBmTGyIyCplc4HDaHfxxS4LtJ89W+hYUAMx+vhlbaoiIyQ0RVT3VW1DWAEoqdTzegiKiRzG5IaIqVdlRUOwoTERPwuSGiAxOn6Og2EpDRE/C5IaIDEofo6DYUZiIdMHkhogMRh+3oAB2FCYi3TC5ISKDkMkFErZWbhQUb0ERUUUwuSEivVL0r9l/+VaFbkXxFhQRVRaTGyLSm8r0r+EtKCLSFyY3RKQXle1fw1tQRKQvTG6IqEIUt5+y8wrh6STFB1t071/j4WSLnr6FiHk6ElH1vXgLioj0gskNEemsssO7FSnMjF4hkF0/ikj2rSEiPbIydgBEVL0obj9VZt4aHzd7LBrSCjFNvfUYGRFRKbbcEJHWKju8e9yz9dGuvqdyFFRJSeXWlCIi0oTJDRE9UWWHd0tQ2lozsUtD3n4iIoNjckNE5dJX/5r4XiFMbIioSjC5IaIyVXZ4N8Ah3kRU9ZjcEJFGFelfIwHg7SrFJ/3DcPt+EbxcOMswEVU9JjdEpKKi/WsU6csHvZuiXX1PwwRHRKQFJjdEpFSZ/jW8/UREpoLJDREBqHj/mseHdxMRGRuTGyKqcP8aDu8mIlPE5IbIglW2fw2HdxORKWJyQ2Sh2L+GiMwVkxsiC8T+NURkzpjcEFkY9q8hInNXoVXBv//+e7Rr1w61a9fG9evXAQDz58/H5s2b9RocEelfatod9q8hIrOmc3KzaNEixMXFoUePHrh37x5kMhkAwN3dHfPnz9d3fESkJzK5wMEr/+Dn0xk6Pc/HzR6LhrRi/xoiqjZ0vi31xRdf4Ouvv0bfvn0xZ84cZXl4eDgmT56s1+CISD8q0nmY/WuIqLrSOblJS0tDy5Yt1cqlUiny8/P1EhQR6Y+unYfZv4aIqjudb0sFBQXhxIkTauXJyclo0qSJPmIiIj3RtfMw+9cQkTnQueUmLi4OY8eORWFhIYQQSE1NxapVq5CYmIhvvvnGEDESkY4qOjkf568hInOgc3IzcuRIODg44P3330dBQQFefvll1K5dG5999hkGDhxoiBiJSAcV6V8TGxWA7qG+7F9DRGahQvPcDB48GIMHD0ZBQQHu378PLy8vfcdFRBVQ0cn5uof6Iiq4pkFiIiKqahXqUPzw4UM0aNAAjo6OcHR0BABcunQJtra2CAwM1HeMRKSFykzOFxHkYaiwiIiqnM4diocPH44DBw6olR8+fBjDhw/XR0xEpAPF/DWfplzg5HxERKhAy83x48fRrl07tfI2bdpg3LhxegmKiLTDxS+JiNTpnNxIJBLk5eWplefk5ChnKyYiw+Pil0REmul8W+rpp59GYmKiSiIjk8mQmJiI9u3b6zU4ItKsov1rfP+dnC8quCYTGyIyWzq33MydOxdPP/00GjVqhA4dOgAAfv/9d+Tm5mL37t16D5CI1HHxSyKisuncchMSEoI///wT/fv3R3Z2NvLy8hAbG4vz588jNDRU5wAWLFiAwMBA2NvbIzIyEqmpqeXuP3/+fDRq1AgODg7w9/fHxIkTUVioe38DouqIi18SET1Zhea5qV27NmbPnl3pk69ZswZxcXFYvHgxIiMjMX/+fMTExODChQsa585ZuXIlpkyZgqVLl6Jt27a4ePEihg8fDolEgqSkpErHQ2TKuPglEZF2KpTc3Lt3D6mpqcjOzoZcLlfZFhsbq/VxkpKSMGrUKIwYMQIAsHjxYmzbtg1Lly7FlClT1PY/cOAA2rVrh5dffhkAEBgYiEGDBuHw4cMVeRlE1QYXvyQi0p7Oyc3WrVsxePBg3L9/H66urpBI/vvglEgkWic3xcXFOHr0KKZOnaoss7KyQnR0NA4ePKjxOW3btsWKFSuQmpqKiIgIXL16Fdu3b8fQoUPLPE9RURGKioqUj3NzcwEAJSUlKCkp0SpWbSmOp+/jUuVU93qRyQU+2HJG58Uv3+veCHLZQ8hNeBBjda8bc8V6MV2WXDe6vGadk5tJkybhlVdewezZs5WzE1fE7du3IZPJ4O3trVLu7e2N8+fPa3zOyy+/jNu3b6N9+/YQQuDhw4d4/fXX8e6775Z5nsTERCQkJKiV79y5s1LxlyclJcUgx6XKqU71IhfAlVwJckuA3GIgM9da6+e62Qm8ECiH7PpRbL9uwCD1qDrVjSVhvZguS6ybgoICrffVObm5efMmxo8fb7DEoDx79+7F7NmzsXDhQkRGRuLy5ct48803MXPmTEybNk3jc6ZOnYq4uDjl49zcXPj7+6Nr165wdXXVa3wlJSVISUlBly5dYGtrq9djU8VVt3rZcSYLidvPIzO36Mk7P2JIpD+6NfVGeECNanMrqrrVjaVgvZguS64bxZ0Xbeic3MTExODIkSOoV6+erk9V4enpCWtra2RlZamUZ2VlwcfHR+Nzpk2bhqFDh2LkyJEAgGbNmiE/Px+vvfYa3nvvPVhZqQ/+kkqlkEqlauW2trYGe2MY8thUcdWhXpJPZ+CN1Sd1npgPAHo296u2i19Wh7qxRKwX02WJdaPL69U5uenZsyfeeustnD17Fs2aNVM7We/evbU6jp2dHVq3bo1du3ahb9++AAC5XI5du3aVuYxDQUGBWgJjbV3aXC9ERb4OiExHRSbmA7j4JRHR43RObkaNGgUAmDFjhto2iUSi0xIMcXFxGDZsGMLDwxEREYH58+cjPz9fOXoqNjYWfn5+SExMBAD06tULSUlJaNmypfK21LRp09CrVy9lkkNU3cjkAqlpd7D/8i2d14ji5HxEROp0Tm4eH/pdGQMGDMCtW7cwffp0ZGZmIiwsDMnJycpOxunp6SotNe+//z4kEgnef/993Lx5E7Vq1UKvXr0wa9YsvcVEVJUqs/AlwMUviYg0qdA8N/o0bty4Mm9D7d27V+WxjY0N4uPjER8fXwWRERlWRRe+nNazCTxdpPBysefkfEREGlQoucnPz8evv/6K9PR0FBcXq2wbP368XgIjMmcVXfjSx80ew9sFMaEhIiqHzsnN8ePH0aNHDxQUFCA/Px8eHh64ffs2HB0d4eXlxeSGSAtc+JKIyHB0Xjhz4sSJ6NWrF+7evQsHBwccOnQI169fR+vWrfHxxx8bIkYis8GFL4mIDE/nlpsTJ07gq6++gpWVFaytrVFUVIR69erho48+wrBhw/DCCy8YIk6iao8LXxIRVQ2dkxtbW1vlCCYvLy+kp6ejSZMmcHNzw40bN/QeIJE54MKXRERVR+fkpmXLlvjjjz/QoEEDdOzYEdOnT8ft27fx/fffIzQ01BAxElVrunYeZv8aIqLK0bnPzezZs+HrW3rff9asWahRowZGjx6NW7duYcmSJXoPkKi6UvSv+TTlgk63oti/hoiocnRuuQkPD1f+28vLC8nJyXoNiMgcVKR/TWxUALqH+rJ/DRFRJRl9Ej8ic1PRyfm6h/pW24UviYhMiVbJTatWrbBr1y7UqFEDLVu2hERS9l+Vx44d01twRNVNZSbn48KXRET6oVVy06dPH0ilUgBQruBNROo4OR8RkfFpldwo1nKSyWR49tln0bx5c7i7uxsyLqJqRbGyd0Um5+PCl0RE+qVTnxtra2t07doV586dY3JD9C9OzkdEZFp07lAcGhqKq1evIigoyBDxEFUrnJyPiMj06DzPzYcffojJkyfjp59+QkZGBnJzc1V+iCwFJ+cjIjJNOrfc9OjRAwDQu3dvlVFTQghIJBLIZDL9RUdkghT9a/ZfvqXz5HzsX0NEZHg6Jzd79uwxRBxE1QIn5yMiMn06JzcdO3Y0RBxEJo+T8xERVQ8VnqG4oKAA6enpKC4uVilv3rx5pYMiMjWcnI+IqPrQObm5desWRowYgZ9//lnjdva5IXPEyfmIiKoPnUdLTZgwAffu3cPhw4fh4OCA5ORkfPvtt2jQoAG2bNliiBiJjEaxsndFJufjyt5ERMahc8vN7t27sXnzZoSHh8PKygoBAQHo0qULXF1dkZiYiJ49exoiTqIqx8n5iIiqJ52Tm/z8fHh5eQEAatSogVu3bqFhw4Zo1qwZF80ks8HJ+YiIqi+db0s1atQIFy5cAAC0aNECX331FW7evInFixfD15dN8FT9cXI+IqLqTeeWmzfffBMZGaX9D+Lj49GtWzf88MMPsLOzw/Lly/UdH1GV4eR8RETmQevkpl+/fhg5ciQGDx6snJm4devWuH79Os6fP4+6devC09PTYIESGRIn5yMiMh9a35a6e/cuevbsibp162L69Om4evUqAMDR0RGtWrViYkPVlqJ/jS6JDfDf5HxMbIiITIvWyc2uXbtw9epVvPrqq1ixYgUaNGiATp06YeXKlSgqKjJkjEQGU9HJ+Xw5OR8RkcnSqUNxQEAAPvjgA1y9ehUpKSmoXbs2Ro0aBV9fX4wdOxZHjx41VJxEBsHJ+YiIzE+Fl1/o1KkTOnXqhLy8PKxcuRLvvvsuvvrqKzx8+FCf8RHpnaLjcHZeIS5l3dfpuew8TERk+iqc3ABAWloali9fjuXLlyMnJwfR0dH6iovIICrScRjg5HxERNWJzslNYWEh1q9fj6VLl+K3336Dv78/Xn31VYwYMQL+/v6GiJFIL3acycIbq09WaPFLTs5HRFR9aJ3cpKamYunSpVizZg0KCwvx/PPPIzk5GZ07d1YODScyVXIBJG4/r3NiA7B/DRFRdaN1ctOmTRu0aNECM2fOxODBg1GjRg1DxkWkV1dyJcjM1W1UH/vXEBFVT1onN0eOHEGrVq0MGQuR3snkAofT7uDkHe1aXsY9G4wG3i7wcrFn/xoiompK6+SGiQ1VN6qdh7Wb9aBd/VqICq5p2MCIiMigKjVaishUVXRVb07MR0RU/em8KjiRqeOq3kRElo0tN2Q2uKo3EREBTG7ITHBVbyIiUtAquWnZsqXWc9kcO3asUgER6UrX/jUKilW9iYjIvGiV3PTt21f578LCQixcuBAhISGIiooCABw6dAhnzpzBmDFjDBIkUVkquqo3Ow8TEZkvrZKb+Ph45b9HjhyJ8ePHY+bMmWr73LhxQ7/RET0BV/UmIqLH6Txaat26dYiNjVUrHzJkCDZs2KCXoIieRCYXOHjlH/x8OkOn5/m42WPRkFbsPExEZMZ07lDs4OCA/fv3o0GDBirl+/fvh729vd4CIypLRToPd/WTIzYmAlH1vdhiQ0Rk5nRObiZMmIDRo0fj2LFjiIiIAAAcPnwYS5cuxbRp0/QeINGjKjY5nxTd/fMRyVFRREQWQefkZsqUKahXrx4+++wzrFixAgDQpEkTLFu2DP3799d7gEQKFZ2c773ujSG7ftRQYRERkYmp0Dw3/fv3ZyJDVU7XzsOKyfk6N/LE9usGDIyIiExKhZKbe/fuYf369bh69SomT54MDw8PHDt2DN7e3vDz89N3jGThFDMPa9t5+PHJ+UpKSgwcIRERmRKdk5s///wT0dHRcHNzw7Vr1zBy5Eh4eHhg48aNSE9Px3fffWeIOMlCVaTzMCfnIyKybDoPBY+Li8Pw4cNx6dIlldFRPXr0wG+//abX4MiyKToPa5vYSAD4cnI+IiKLp3Ny88cff+D//u//1Mr9/PyQmZmpl6CIuLI3ERFVlM63paRSKXJzc9XKL168iFq1auklKKKKdh7m5HxERKRzctO7d2/MmDEDa9euBQBIJBKkp6fjnXfewYsvvqj3AMmyVLbzMBERkc7JzSeffIJ+/frBy8sLDx48QMeOHZGZmYmoqCjMmjXLEDGShWDnYSIi0gedkxs3NzekpKRg3759+PPPP3H//n20atUK0dHRhoiPLETFZh5m52EiIlJXoXluAKB9+/Zo3769PmMhC8XOw0REpE8VSm527dqFXbt2ITs7G3K5XGXb0qVL9RIYWQ52HiYiIn3SOblJSEjAjBkzEB4eDl9fX0gk/MuZKic7T7vEhp2HiYhIGzonN4sXL8by5csxdOhQQ8RDFkQxMupSVp5W+7PzMBERaUPn5Ka4uBht27Y1RCxkQXQZGcXOw0REpAudZygeOXIkVq5caYhYyELosqwCOw8TEZGudG65KSwsxJIlS/DLL7+gefPmsLW1VdmelJSkt+DI/Og6Moqdh4mISFcVWhU8LCwMAHD69GmVbexcTE+i7ciocc/WR7v6nuw8TEREOtM5udmzZ48h4iAzp+uyCg28ndl5mIiIKqTCk/gRaasiyyp4udgbMCIiIjJnWiU3L7zwApYvXw5XV1e88MIL5e67ceNGvQRG5oHLKhARUVXTKrlxc3NT9qdxc3MzaEBkPrisAhERGYNWyc2yZcs0/lsfFixYgHnz5iEzMxMtWrTAF198gYiIiDL3v3fvHt577z1s3LgRd+7cQUBAAObPn48ePXroNS6qPC6rQERExmDUPjdr1qxBXFwcFi9ejMjISMyfPx8xMTG4cOECvLy81PYvLi5Gly5d4OXlhfXr18PPzw/Xr1+Hu7t71QdPT8RlFYiIyBgqlNysX78ea9euRXp6OoqLi1W2HTt2TOvjJCUlYdSoURgxYgSA0qUdtm3bhqVLl2LKlClq+y9duhR37tzBgQMHlPPrBAYGVuQlkIEoRkVl5xXidl6RVs/hsgpERKRPOs9Q/Pnnn2PEiBHw9vbG8ePHERERgZo1a+Lq1avo3r271scpLi7G0aNHER0d/V8wVlaIjo7GwYMHNT5ny5YtiIqKwtixY+Ht7Y3Q0FDMnj0bMplM15dBBpB8OgPt5+7GoK8P4c3VJzBz27ly95cA8GXnYSIi0jOdW24WLlyIJUuWYNCgQVi+fDnefvtt1KtXD9OnT8edO3e0Ps7t27chk8ng7e2tUu7t7Y3z589rfM7Vq1exe/duDB48GNu3b8fly5cxZswYlJSUID4+XuNzioqKUFT0XwtCbm4uAKCkpAQlJSVax6sNxfH0fdzqYMeZLLyx+qTOnYff694IctlDyA2Yn1pyvZg61o1pYr2YLkuuG11es87JTXp6unLhTAcHB+Tlla7oPHToULRp0wZffvmlrofUmlwuh5eXF5YsWQJra2u0bt0aN2/exLx588pMbhITE5GQkKBWvnPnTjg6OhokzpSUFIMc11TJBZBwzPrfxEZTnxkBifK/pdzsBF4IlEN2/Si2X6+aOC2tXqoT1o1pYr2YLkusm4KCAq331Tm58fHxUY5Sqlu3Lg4dOoQWLVogLS0NQmj7dzvg6ekJa2trZGVlqZRnZWXBx8dH43N8fX1ha2sLa2trZVmTJk2QmZmJ4uJi2NnZqT1n6tSpiIuLUz7Ozc2Fv78/unbtCldXV63j1UZJSQlSUlLQpUsXtTW3zNnhtDu4d+hIOXtIIAC8270hPJ2l8HKRIjygRpV1HrbUeqkOWDemifViuiy5bhR3XrShc3LTqVMnbNmyBS1btsSIESMwceJErF+/HkeOHHniBH+PsrOzQ+vWrbFr1y707dsXQGnLzK5duzBu3DiNz2nXrh1WrlwJuVwOK6vS7kIXL16Er6+vxsQGAKRSKaRSqVq5ra2twd4Yhjy2Kfqn4KFW+3m7OaJPmJ+BoymbpdVLdcK6MU2sF9NliXWjy+vVOblZsmQJ5HI5AGDs2LGoWbMmDhw4gN69e+P//u//dDpWXFwchg0bhvDwcERERGD+/PnIz89Xjp6KjY2Fn58fEhMTAQCjR4/Gl19+iTfffBNvvPEGLl26hNmzZ2P8+PG6vgzSI22XSuCSCkREVBV0Tm6srKyUrSYAMHDgQAwcOLBCJx8wYABu3bqF6dOnIzMzE2FhYUhOTlZ2Mk5PT1c5l7+/P3bs2IGJEyeiefPm8PPzw5tvvol33nmnQuenylEM+87MeQAPJzvcyS/WuB+XVCAioqqkVXLz559/an3A5s2b6xTAuHHjyrwNtXfvXrWyqKgoHDp0SKdzkP5puxgml1QgIqKqplVyExYWBolE8sQOwxKJhHPOWABdFsPkkgpERFTVtEpu0tLSDB0HVRPaLIbp4WSLac81hY+rPZdUICKiKqdVchMQEGDoOKia0GYxzDv5JfBxteeSCkREZBQVWlvqwoUL+OKLL3DuXOn0+k2aNMEbb7yBRo0a6TU4Mj3aLoap7X5ERET6pvPaUhs2bEBoaCiOHj2KFi1aoEWLFjh27BhCQ0OxYcMGQ8RIJoTDvomIyNTp3HLz9ttvY+rUqZgxY4ZKeXx8PN5++228+OKLeguOTE9EkAd83ezLvDXFYd9ERGRsOrfcZGRkIDY2Vq18yJAhyMjI0EtQZHpkcoGDV/7BT3/+XWZfGg77JiIiU6Bzy80zzzyD33//HfXr11cp37dvHzp06KC3wMh0lDWnjZ2NFYofypWPOeybiIhMgc7JTe/evfHOO+/g6NGjaNOmDQDg0KFDWLduHRISErBlyxaVfal6K29Om+KHckyMboBATyd4uXDYNxERmQadk5sxY8YAABYuXIiFCxdq3AZwQj9z8KQ5bSQAVv9xA/ve6cSkhoiITIbOfW7kcrlWP0xsqr8nzWkjAGTkFCI17U7VBUVERPQEOic35SkoKNDn4cjIOKcNERFVRzonN507d8bNmzfVyg8fPoywsDB9xEQmgnPaEBFRdaRzcmNvb4/mzZtjzZo1AEpvU33wwQfo0KEDevToofcAyXgigjzg7Sotc7sEgC/ntCEiIhOjc4fibdu2YcGCBXjllVewefNmXLt2DdevX8dPP/2Erl27GiJGqmIyuUBq2h1k5xWitpsDsnKL1PbhnDZERGSqKrS21NixY/HXX39h7ty5sLGxwd69e9G2bVt9x0ZGUNacNs5Sa9wv+q+TOOe0ISIiU6VzcnP37l2MHDkSu3btwldffYVff/0VXbt2xUcffaQyFJyqn/LmtLlfJOOcNkREVC3onNyEhoYiKCgIx48fR1BQEEaNGoU1a9ZgzJgx2LZtG7Zt22aIOMnAOKcNERGZC507FL/++uv47bffEBQUpCwbMGAATp48ieLiYr0GR1WHc9oQEZG50LnlZtq0aRrL69Spg5SUlEoHRMbBOW2IiMhcaN1y89FHH+HBgwfKx/v370dR0X+jaPLy8tjnphrjnDZERGQutE5upk6diry8POXj7t27q0zmV1BQgK+++kq/0VGViQjygA/ntCEiIjOg9W0pIUS5j6l6enROm4CajsjknDZERFTNVWieGzIPZc1p42RnjfxizmlDRETVE5MbC1XenDb5xZzThoiIqi+dkptvvvkGzs7OAICHDx9i+fLl8PT0BACV/jhk2jinDRERmTOtk5u6devi66+/Vj728fHB999/r7YPmT5d5rSJCq5ZdYERERHpgdbJzbVr1wwYBlUlzmlDRETmTOcZiqn645w2RERkzpjcWKCIIA/4upWduHBOGyIiqs6Y3FggaysJhrcN1LiNc9oQEVF1x6HgFkQ5YV9uITYdL51d2t7WCoUlcuU+nNOGiIiqOyY3FqKsCfve69kE9Wu5IDuvkHPaEBGRWajQbakrV67g/fffx6BBg5CdnQ0A+Pnnn3HmzBm9Bkf6oZiwT9Pw7+k/nkHOg2L0CfNDVHBNJjZERFTt6Zzc/Prrr2jWrBkOHz6MjRs34v79+wCAkydPIj4+Xu8BUuU8acI+AEjYehYyOdcKIyIi86BzcjNlyhR8+OGHSElJgZ2dnbK8U6dOOHTokF6Do8rTZcI+IiIic6BzcnPq1Ck8//zzauVeXl64ffu2XoIi/eGEfUREZGl0Tm7c3d2RkZGhVn78+HH4+fnpJSjSH07YR0RElkbn5GbgwIF45513kJmZCYlEArlcjv3792Py5MmIjY01RIxUCZywj4iILI3Oyc3s2bPRuHFj+Pv74/79+wgJCcHTTz+Ntm3b4v333zdEjFQJ1lYSjHmmvsZtnLCPiIjMkc7z3NjZ2eHrr7/GtGnTcPr0ady/fx8tW7ZEgwYNDBEf6cHx9LsAADsbKxQ/5IR9RERk3nRObvbt24f27dujbt26qFu3riFiIj1QzEb851/3sPHf2YjXvhaFByUyTthHRERmTefkplOnTvDz88OgQYMwZMgQhISEGCIuqgRNsxFLbayQmfuArTRERGT2dO5z8/fff2PSpEn49ddfERoairCwMMybNw9//fWXIeIjHZU1G3HRQzlGrziG5NPqI92IiIjMic7JjaenJ8aNG4f9+/fjypUreOmll/Dtt98iMDAQnTp1MkSMpCXORkxERFTBtaUUgoKCMGXKFMyZMwfNmjXDr7/+qq+4qAI4GzEREVElkpv9+/djzJgx8PX1xcsvv4zQ0FBs27ZNn7GRjjgbMRERUQU6FE+dOhWrV6/G33//jS5duuCzzz5Dnz594OjoaIj4SAecjZiIiKgCyc1vv/2Gt956C/3794enp6chYqIKUsxGXNatKQlK57bhbMRERGTOdE5u9u/fb4g4SA+srSQY37kBpm48pbaNsxETEZGl0Cq52bJlC7p37w5bW1ts2bKl3H179+6tl8CoYq7dzgcA2FlLUCz7b1QUZyMmIiJLoVVy07dvX2RmZsLLywt9+/Ytcz+JRAKZTKav2EhHuYUlWHk4HQCw4OVWcLa35WzERERkcbRKbuRyucZ/k2lQLLWwKjUdeUUPEVzLCZ2beMOKyQwREVkgnYeCf/fddygqKlIrLy4uxnfffaeXoEh7yacz0H7ubgz6+hC2nPwbAHD7fjF2ns00cmRERETGoXNyM2LECOTk5KiV5+XlYcSIEXoJirRT1lILuQ9KuNQCERFZLJ2TGyEEJBL12x1//fUX3Nzc9BIUPVl5Sy0oyrjUAhERWSKth4K3bNkSEokEEokEnTt3ho3Nf0+VyWRIS0tDt27dDBIkqdNlqYWo4JpVFxgREZGRaZ3cKEZJnThxAjExMXB2dlZus7OzQ2BgIF588UW9B0iacakFIiIizbRObuLj4wEAgYGBGDBgAOztOYW/MXGpBSIiIs107nMzbNgwJjYmQLHUQlkkAHy51AIREVkgnZMbmUyGjz/+GBEREfDx8YGHh4fKD1UNaysJ3opppHEbl1ogIiJLpnNyk5CQgKSkJAwYMAA5OTmIi4vDCy+8ACsrK3zwwQcGCJHKcq+gBABg81gC4+Nmj0VDWnGpBSIiskg6L5z5ww8/4Ouvv0bPnj3xwQcfYNCgQQgODkbz5s1x6NAhjB8/3hBx0mPkcoHvDl4DAEzrFYKGXi5caoGIiAgVSG4yMzPRrFkzAICzs7NyQr/nnnsO06ZN0290VKZfL93CtX8K4GJvg36t6sBJqnNVEhERmSWdb0vVqVMHGRmlM98GBwdj586dAIA//vgDUqlUv9GRCplc4OCVf7D5xE3M/+UiAKB/uD8TGyIiokfo/K34/PPPY9euXYiMjMQbb7yBIUOG4H//+x/S09MxceJEQ8RIKF1qIWHrWbWJ+wJrOhopIiIiItOkc3IzZ84c5b8HDBiAunXr4uDBg2jQoAF69eql1+ColGINKU0LKUzffAa1XKTsPExERPSvSt/PiIqKQlRUlD5iIQ3KW0NKIWHrWXQJ8WEnYiIiImiZ3GzZskXrA/bu3bvCwZA6riFFRESkG62SG8W6Uk8ikUggk8l0DmLBggWYN28eMjMz0aJFC3zxxReIiIh44vNWr16NQYMGoU+fPvjxxx91Pm91wDWkiIiIdKPVaCm5XK7VT0USmzVr1iAuLg7x8fE4duwYWrRogZiYGGRnZ5f7vGvXrmHy5Mno0KGDzuesTriGFBERkW50Hgqub0lJSRg1ahRGjBiBkJAQLF68GI6Ojli6dGmZz5HJZBg8eDASEhJQr169Koy26inWkCqrNw3XkCIiIlKlc4fiGTNmlLt9+vTpWh+ruLgYR48exdSpU5VlVlZWiI6OxsGDB8uNwcvLC6+++ip+//33cs9RVFSEoqIi5ePc3FwAQElJCUpKSrSOVRuK4+n7uO91b4Q3Vp9UK5c8sl0uewi57g1nFsFQ9UKVx7oxTawX02XJdaPLa9Y5udm0aZPaydLS0mBjY4Pg4GCdkpvbt29DJpPB29tbpdzb2xvnz5/X+Jx9+/bhf//7H06cOKHVORITE5GQkKBWvnPnTjg6GmaOmJSUFL0f87m6EmxNt1Ypc7MTeCFQDtn1o9h+Xe+nNDuGqBfSD9aNaWK9mC5LrJuCggKt99U5uTl+/LhaWW5uLoYPH47nn39e18PpJC8vD0OHDsXXX38NT09PrZ4zdepUxMXFKR/n5ubC398fXbt2haurq17jKykpQUpKCrp06QJbW1u9Hvv49vNAejraBNVA//A68HKRIjygBod/a8GQ9UKVw7oxTawX02XJdaO486INvczb7+rqioSEBPTq1QtDhw7V+nmenp6wtrZGVlaWSnlWVhZ8fHzU9r9y5QquXbumMlmgXC4HANjY2ODChQsIDg5WeY5UKtW4LIStra3B3hj6PrZMLrDtdOk1GtkhGNEh3k94BmliyDqnymHdmCbWi+myxLrR5fXqrUNxTk6OchFNbdnZ2aF169bYtWuXskwul2PXrl0aJwZs3LgxTp06hRMnTih/evfujWeffRYnTpyAv79/pV+HKTp45R/cyiuCu6Mtnm5Yy9jhEBERmTSdW24+//xzlcdCCGRkZOD7779H9+7ddQ4gLi4Ow4YNQ3h4OCIiIjB//nzk5+djxIgRAIDY2Fj4+fkhMTER9vb2CA0NVXm+u7s7AKiVm5MfT9wEAPRo5gs7G6MPcCMiIjJpOic3n376qcpjKysr1KpVC8OGDVMZ9aStAQMG4NatW5g+fToyMzMRFhaG5ORkZSfj9PR0WFlZ7hd6YYkMyaczAQB9w/yMHA0REZHp0zm5SUtL03sQ48aNw7hx4zRu27t3b7nPXb58ud7jMQUyuUBq2h3sPJuJ+0UP4eta2oGYiIiIyqeXDsWkX8mnM5Cw9azKmlK5hQ+x82wmV/8mIiJ6Ap2Tm8LCQnzxxRfYs2cPsrOzlaOVFI4dO6a34CxR8ukMjF5xTG0V8PxiGUavOIZFQ1oxwSEiIiqHzsnNq6++ip07d6Jfv36IiIiARMJ5VvRFJhdI2HpWLbF5VMLWs+gS4sP5bYiIiMqgc3Lz008/Yfv27WjXrp0h4rFoqWl3VG5FPU4AyMgpRGraHUQF16y6wIiIiKoRnYch+fn5wcXFxRCxWLzsvLITm4rsR0REZIl0Tm4++eQTvPPOO7h+nYsZ6ZuXi71e9yMiIrJEOt+WCg8PR2FhIerVqwdHR0e16ZDv3Lmjt+AsTUSQB3zd7JGZU6ix340EgI+bPSKCPKo6NCIiompD5+Rm0KBBuHnzJmbPng1vb292KNYjaysJ4nuF4PUV6iPOFFc5vlcIOxMTERGVQ+fk5sCBAzh48CBatGhhiHgsXrdQX7wc4Y+VqTdUyn3c7BHfK4TDwImIiJ5A5+SmcePGePDggSFioX9dv1MAABj0lD/aBNeEl0vprSi22BARET2ZzsnNnDlzMGnSJMyaNQvNmjVT63Pj6uqqt+As0T/3i3Dwyj8AgNHP1Efdmo5GjoiIiKh60Tm56datGwCgc+fOKuVCCEgkEshkMv1EZqF2nMmCXAChfq5MbIiIiCpA5+Rmz549hoiD/rX9VAYAoEcz9q0hIiKqCJ2Tm44dOxoiDgJwJ78YB6+W3pLqyeSGiIioQnRObn777bdytz/99NMVDsZSyeQCqWl38OOJm5DJBZrWdkFATSdjh0VERFQt6ZzcPPPMM2plj851wz43ukk+nYGErWdV1pRK/+cBkk9ncNg3ERFRBei8/MLdu3dVfrKzs5GcnIynnnoKO3fuNESMZiv5dAZGrzimtlhmXtFDjF5xDMmnM4wUGRERUfWlc8uNm5ubWlmXLl1gZ2eHuLg4HD16VC+BmTuZXCBh61mNyywoJGw9iy4hPpzfhoiISAc6t9yUxdvbGxcuXNDX4cxeatodtRabRwkAGTmFSE3jWl1ERES60Lnl5s8//1R5LIRARkYG5syZg7CwMH3FZfay88pObCqyHxEREZXSObkJCwuDRCKBEKo3VNq0aYOlS5fqLTBz5+Vir9f9iIiIqJTOyU1aWprKYysrK9SqVQv29vwS1kVEkAd83eyRmVOosd+NBKWLZUYEeVR1aERERNWazslNQECAIeKwONZWEsT3CsHoFcfUtim6D8f3CmFnYiIiIh1p3aF49+7dCAkJQW5urtq2nJwcNG3aFL///rtegzN33UJ9MTmmkVq5j5s9Fg1pxXluiIiIKkDrlpv58+dj1KhRGlf9dnNzw//93/8hKSkJHTp00GuA5i6/6CEAICq4JgY+5Q8vl9JbUWyxISIiqhitW25OnjypXBFck65du3KOmwrYfT4bADAg3B99wvwQFVyTiQ0REVElaJ3cZGVlwdbWtsztNjY2uHXrll6CshR/3S3A+cw8WEmAZxrVMnY4REREZkHr5MbPzw+nT58uc/uff/4JX1/2EdGFotUmPMAD7o52Ro6GiIjIPGid3PTo0QPTpk1DYaH6pHIPHjxAfHw8nnvuOb0GZ+52nStNbjo18TJyJEREROZD6w7F77//PjZu3IiGDRti3LhxaNSodJTP+fPnsWDBAshkMrz33nsGC9Tc5Bc9xMEr/wAAopncEBER6Y3WyY23tzcOHDiA0aNHY+rUqcoZiiUSCWJiYrBgwQJ4e3sbLFBzs+/ybRTL5Kjr4YjgWs7GDoeIiMhs6DSJX0BAALZv3467d+/i8uXLEEKgQYMGqFGjhqHiMzsyuUBq2h0s21c60/MzjWpBIuHoKCIiIn3ReYZiAKhRowaeeuopfcdi9pJPZyBh61mV1cB/+jMDbYNrcsI+IiIiPdG6QzFVTvLpDIxecUwlsQGAu/nFGL3iGJJPZxgpMiIiIvPC5KYKyOQCCVvPalwgU1GWsPUsZHJNexAREZEumNxUgdS0O2otNo8SADJyCpGadqfqgiIiIjJTTG6qQHZe2YlNRfYjIiKisjG5qQJeLvZ63Y+IiIjKxuSmCkQEecDXzR5lDfiWAPB1K10NnIiIiCqHyU0VsLaSIL5XiMZtioQnvlcIVwMnIiLSAyY3VaRbqC8+fqmFWrmPmz0WDWnFeW6IiIj0pEKT+FHFuDrYAii9BTWle2N4uZTeimKLDRERkf4wualCh6+WLpT5TCMv9AnzM3I0RERE5om3parQ4X/nsWlTjx2HiYiIDIXJTRXJLSzBmb9zAACRQTWNHA0REZH5YnJTRY5euwu5AAJqOsLHjfPZEBERGQqTmypyKK20v00k57IhIiIyKCY3VeTw1dL+NrwlRUREZFhMbqpAftFDnLr5b38bdiYmIiIyKCY3VeDo9buQyQX83B1Qp4ajscMhIiIya0xuqsBhRX8bttoQEREZHJObKqDob9OG/W2IiIgMjjMUG5BMLrDv0i0cT78LAAgPrGHkiIiIiMwfW24MJPl0BtrP3Y1hy/6ATJSWvfzNYSSfzjBuYERERGaOyY0B7DiThdErjiEjp1ClPCunEKNXHGOCQ0REZEBMbvRMLoAPt5+H0LBNUZaw9Sxkck17EBERUWUxudGzK7kSZOYWlbldAMjIKUTqv4toEhERkX4xudGz3BLt9svOK3zyTkRERKQzJjd65mqr3X5eLlw8k4iIyBCY3OhZsKuAj6sUkjK2SwD4utkjggtoEhERGQSTGz2zkgDv92iscZsi4YnvFQJrq7LSHyIiIqoMJjcGENPUG4uGtIKttWoC4+Nmj0VDWqFbqK+RIiMiIjJ/nKHYQGKa+sDexgolMhmmdGuMFv7uiAjyYIsNERGRgTG5MZB/8ouRVySDRAIMbxcIe1trY4dERERkEXhbykCu3soHAPi5OzCxISIiqkJMbgzk6q37AIB6tZyNHAkREZFlYXJjIGm3S1tu6nk6GTkSIiIiy8LkxkCu/Htbql4tJjdERERVySSSmwULFiAwMBD29vaIjIxEampqmft+/fXX6NChA2rUqIEaNWogOjq63P2N5ertf29LefK2FBERUVUyenKzZs0axMXFIT4+HseOHUOLFi0QExOD7Oxsjfvv3bsXgwYNwp49e3Dw4EH4+/uja9euuHnzZhVHXraHMjnS/ykAAASx5YaIiKhKGT25SUpKwqhRozBixAiEhIRg8eLFcHR0xNKlSzXu/8MPP2DMmDEICwtD48aN8c0330Aul2PXrl1VHHnZ/rr3AA/lAva2VvB15RpSREREVcmo89wUFxfj6NGjmDp1qrLMysoK0dHROHjwoFbHKCgoQElJCTw8NK/VVFRUhKKiIuXj3NxcAEBJSQlKSrRcwltLiuNdyiw9R2BNJ8hkDyGT6fU0pCNFvei7vqnyWDemifViuiy5bnR5zUZNbm7fvg2ZTAZvb2+Vcm9vb5w/f16rY7zzzjuoXbs2oqOjNW5PTExEQkKCWvnOnTvh6Oioe9Ba2HHwBABr2JfkYPv27QY5B+kuJSXF2CFQGVg3pon1YrossW4KCgq03rdaz1A8Z84crF69Gnv37oW9vebbP1OnTkVcXJzycW5urrKfjqurq17jKSkpQUpKCuxq+gPX/0bb0ProEV1fr+cg3SnqpUuXLrC1tTV2OPQI1o1pYr2YLkuuG8WdF20YNbnx9PSEtbU1srKyVMqzsrLg4+NT7nM//vhjzJkzB7/88guaN29e5n5SqRRSqVSt3NbW1mBvjOt3HgAAGvi4WNybz5QZss6pclg3pon1YrossW50eb1G7VBsZ2eH1q1bq3QGVnQOjoqKKvN5H330EWbOnInk5GSEh4dXRag6+W8CPw4DJyIiqmpGvy0VFxeHYcOGITw8HBEREZg/fz7y8/MxYsQIAEBsbCz8/PyQmJgIAJg7dy6mT5+OlStXIjAwEJmZmQAAZ2dnODsbP5kofAjcul8MgMPAiYiIjMHoyc2AAQNw69YtTJ8+HZmZmQgLC0NycrKyk3F6ejqsrP5rYFq0aBGKi4vRr18/lePEx8fjgw8+qMrQNcouLP2/p7MUrvaW1WRIRERkCoye3ADAuHHjMG7cOI3b9u7dq/L42rVrhg+oErIfSABw2QUiIiJjMfokfuZGmdxwwUwiIiKjYHKjZ4rbUmy5ISIiMg4mN3r2X8uN8Ts3ExERWSImN3oklwvc+rflhiOliIiIjIPJjR5l5RWhWC6BjZUEdT0Ms7QDERERlY/JjR4pJu/zr+EAW2teWiIiImPgN7AeKZKbQE+22hARERkLkxs9kckFDly5AwBwsLWGTC6MHBEREZFlYnKjB8mnM9B+7m7sPJcNANh+Ogvt5+5G8ukMI0dGRERkeZjcVFLy6QyMXnEMGTmFKuWZOYUYveIYExwiIqIqxuSmEmRygYStZ6HpBpSiLGHrWd6iIiIiqkJMbiohNe2OWovNowSAjJxCpKbdqbqgiIiILByTm0rIzis7sanIfkRERFR5TG4qwcvFXq/7ERERUeUxuamEiCAP+LrZQ1LGdgkAXzd7RAR5VGVYREREFo3JTSVYW0kQ3ysEANQSHMXj+F4hsLYqK/0hIiIifWNyU0ndQn2xaEgr+Lip3nrycbPHoiGt0C3U10iRERERWSYbYwdgDrqF+qJLiA8OXs7Gzt8Po2uHSETV92KLDRERkREwudETaysJIoM88M85gcggDyY2RERERsLbUkRERGRWmNwQERGRWWFyQ0RERGaFyQ0RERGZFSY3REREZFaY3BAREZFZYXJDREREZoXJDREREZkVJjdERERkVixuhmIhBAAgNzdX78cuKSlBQUEBcnNzYWtrq/fjU8WwXkwX68Y0sV5MlyXXjeJ7W/E9Xh6LS27y8vIAAP7+/kaOhIiIiHSVl5cHNze3cveRCG1SIDMil8vx999/w8XFBRKJftd/ys3Nhb+/P27cuAFXV1e9HpsqjvViulg3pon1YrosuW6EEMjLy0Pt2rVhZVV+rxqLa7mxsrJCnTp1DHoOV1dXi3vTVQesF9PFujFNrBfTZal186QWGwV2KCYiIiKzwuSGiIiIzAqTGz2SSqWIj4+HVCo1dij0CNaL6WLdmCbWi+li3WjH4joUExERkXljyw0RERGZFSY3REREZFaY3BAREZFZYXJDREREZoXJjZ4sWLAAgYGBsLe3R2RkJFJTU40dksVJTEzEU089BRcXF3h5eaFv3764cOGCyj6FhYUYO3YsatasCWdnZ7z44ovIysoyUsSWac6cOZBIJJgwYYKyjPViHDdv3sSQIUNQs2ZNODg4oFmzZjhy5IhyuxAC06dPh6+vLxwcHBAdHY1Lly4ZMWLLIJPJMG3aNAQFBcHBwQHBwcGYOXOmyppKrJsnEFRpq1evFnZ2dmLp0qXizJkzYtSoUcLd3V1kZWUZOzSLEhMTI5YtWyZOnz4tTpw4IXr06CHq1q0r7t+/r9zn9ddfF/7+/mLXrl3iyJEjok2bNqJt27ZGjNqypKamisDAQNG8eXPx5ptvKstZL1Xvzp07IiAgQAwfPlwcPnxYXL16VezYsUNcvnxZuc+cOXOEm5ub+PHHH8XJkydF7969RVBQkHjw4IERIzd/s2bNEjVr1hQ//fSTSEtLE+vWrRPOzs7is88+U+7Duikfkxs9iIiIEGPHjlU+lslkonbt2iIxMdGIUVF2drYAIH799VchhBD37t0Ttra2Yt26dcp9zp07JwCIgwcPGitMi5GXlycaNGggUlJSRMeOHZXJDevFON555x3Rvn37MrfL5XLh4+Mj5s2bpyy7d++ekEqlYtWqVVURosXq2bOneOWVV1TKXnjhBTF48GAhBOtGG7wtVUnFxcU4evQooqOjlWVWVlaIjo7GwYMHjRgZ5eTkAAA8PDwAAEePHkVJSYlKXTVu3Bh169ZlXVWBsWPHomfPnirXH2C9GMuWLVsQHh6Ol156CV5eXmjZsiW+/vpr5fa0tDRkZmaq1IubmxsiIyNZLwbWtm1b7Nq1CxcvXgQAnDx5Evv27UP37t0BsG60YXELZ+rb7du3IZPJ4O3trVLu7e2N8+fPGykqksvlmDBhAtq1a4fQ0FAAQGZmJuzs7ODu7q6yr7e3NzIzM40QpeVYvXo1jh07hj/++ENtG+vFOK5evYpFixYhLi4O7777Lv744w+MHz8ednZ2GDZsmPLaa/psY70Y1pQpU5Cbm4vGjRvD2toaMpkMs2bNwuDBgwGAdaMFJjdklsaOHYvTp09j3759xg7F4t24cQNvvvkmUlJSYG9vb+xw6F9yuRzh4eGYPXs2AKBly5Y4ffo0Fi9ejGHDhhk5Osu2du1a/PDDD1i5ciWaNm2KEydOYMKECahduzbrRku8LVVJnp6esLa2VhvZkZWVBR8fHyNFZdnGjRuHn376CXv27EGdOnWU5T4+PiguLsa9e/dU9mddGdbRo0eRnZ2NVq1awcbGBjY2Nvj111/x+eefw8bGBt7e3qwXI/D19UVISIhKWZMmTZCeng4AymvPz7aq99Zbb2HKlCkYOHAgmjVrhqFDh2LixIlITEwEwLrRBpObSrKzs0Pr1q2xa9cuZZlcLseuXbsQFRVlxMgsjxAC48aNw6ZNm7B7924EBQWpbG/dujVsbW1V6urChQtIT09nXRlQ586dcerUKZw4cUL5Ex4ejsGDByv/zXqpeu3atVObKuHixYsICAgAAAQFBcHHx0elXnJzc3H48GHWi4EVFBTAykr169na2hpyuRwA60Yrxu7RbA5Wr14tpFKpWL58uTh79qx47bXXhLu7u8jMzDR2aBZl9OjRws3NTezdu1dkZGQofwoKCpT7vP7666Ju3bpi9+7d4siRIyIqKkpERUUZMWrL9OhoKSFYL8aQmpoqbGxsxKxZs8SlS5fEDz/8IBwdHcWKFSuU+8yZM0e4u7uLzZs3iz///FP06dOHw42rwLBhw4Sfn59yKPjGjRuFp6enePvtt5X7sG7Kx+RGT7744gtRt25dYWdnJyIiIsShQ4eMHZLFAaDxZ9myZcp9Hjx4IMaMGSNq1KghHB0dxfPPPy8yMjKMF7SFejy5Yb0Yx9atW0VoaKiQSqWicePGYsmSJSrb5XK5mDZtmvD29hZSqVR07txZXLhwwUjRWo7c3Fzx5ptvirp16wp7e3tRr1498d5774mioiLlPqyb8kmEeGTKQyIiIqJqjn1uiIiIyKwwuSEiIiKzwuSGiIiIzAqTGyIiIjIrTG6IiIjIrDC5ISIiIrPC5IaIiIjMCpMbIgIAXLt2DRKJBCdOnDB2KErnz59HmzZtYG9vj7CwMGOHQ0TVBJMbIhMxfPhwSCQSzJkzR6X8xx9/hEQiMVJUxhUfHw8nJydcuHBBZR2dx2VmZuKNN95AvXr1IJVK4e/vj169epX7HEs0fPhw9O3b19hhEBkckxsiE2Jvb4+5c+fi7t27xg5Fb4qLiyv83CtXrqB9+/YICAhAzZo1Ne5z7do1tG7dGrt378a8efNw6tQpJCcn49lnn8XYsWMrfG4iqr6Y3BCZkOjoaPj4+CAxMbHMfT744AO1WzTz589HYGCg8rHiL/TZs2fD29sb7u7umDFjBh4+fIi33noLHh4eqFOnDpYtW6Z2/PPnz6Nt27awt7dHaGgofv31V5Xtp0+fRvfu3eHs7Axvb28MHToUt2/fVm5/5plnMG7cOEyYMAGenp6IiYnR+DrkcjlmzJiBOnXqQCqVIiwsDMnJycrtEokER48exYwZMyCRSPDBBx9oPM6YMWMgkUiQmpqKF198EQ0bNkTTpk0RFxeHQ4cOKfdLT09Hnz594OzsDFdXV/Tv3x9ZWVlq13Xp0qWoW7cunJ2dMWbMGMhkMnz00Ufw8fGBl5cXZs2apXJ+iUSCRYsWoXv37nBwcEC9evWwfv16lX1OnTqFTp06wcHBATVr1sRrr72G+/fvq9XXxx9/DF9fX9SsWRNjx45FSUmJcp+ioiJMnjwZfn5+cHJyQmRkJPbu3avcvnz5cri7u2PHjh1o0qQJnJ2d0a1bN2RkZChf37fffovNmzdDIpFAIpFg7969KC4uxrhx4+Dr6wt7e3sEBASU+/4jqhaMvbgVEZUaNmyY6NOnj9i4caOwt7cXN27cEEIIsWnTJvHor2p8fLxo0aKFynM//fRTERAQoHIsFxcXMXbsWHH+/Hnxv//9TwAQMTExYtasWeLixYti5syZwtbWVnmetLQ0AUDUqVNHrF+/Xpw9e1aMHDlSuLi4iNu3bwshhLh7966oVauWmDp1qjh37pw4duyY6NKli3j22WeV5+7YsaNwdnYWb731ljh//rw4f/68xteblJQkXF1dxapVq8T58+fF22+/LWxtbcXFixeFEEJkZGSIpk2bikmTJomMjAyRl5endox//vlHSCQSMXv27HKvrUwmE2FhYaJ9+/biyJEj4tChQ6J169aiY8eOKtfV2dlZ9OvXT5w5c0Zs2bJF2NnZiZiYGPHGG2+I8+fPi6VLlwoAKgvjAhA1a9YUX3/9tbhw4YJ4//33hbW1tTh79qwQQoj79+8LX19f8cILL4hTp06JXbt2iaCgIDFs2DCV+nJ1dRWvv/66OHfunNi6datwdHRUWchy5MiRom3btuK3334Tly9fFvPmzRNSqVR5vZYtWyZsbW1FdHS0+OOPP8TRo0dFkyZNxMsvvyyEECIvL0/0799fdOvWTWRkZIiMjAxRVFQk5s2bJ/z9/cVvv/0mrl27Jn7//XexcuXKcq8nkaljckNkIhTJjRBCtGnTRrzyyitCiIonNwEBAUImkynLGjVqJDp06KB8/PDhQ+Hk5CRWrVolhPgvuZkzZ45yn5KSElGnTh0xd+5cIYQQM2fOFF27dlU5940bNwQA5YrEHTt2FC1btnzi661du7aYNWuWStlTTz0lxowZo3zcokULER8fX+YxDh8+LACIjRs3lnuunTt3Cmtra5Genq4sO3PmjAAgUlNThRCl19XR0VHk5uYq94mJiRGBgYFq1zExMVH5GIB4/fXXVc4XGRkpRo8eLYQQYsmSJaJGjRri/v37yu3btm0TVlZWIjMzUwjxX309fPhQuc9LL70kBgwYIIQQ4vr168La2lrcvHlT5TydO3cWU6dOFUKUJjcAxOXLl5XbFyxYILy9vZWPH32PKbzxxhuiU6dOQi6Xl3n9iKob3pYiMkFz587Ft99+i3PnzlX4GE2bNoWV1X+/4t7e3mjWrJnysbW1NWrWrIns7GyV50VFRSn/bWNjg/DwcGUcJ0+exJ49e+Ds7Kz8ady4MYDS/jEKrVu3Lje23Nxc/P3332jXrp1Kebt27XR6zUIIrfY7d+4c/P394e/vrywLCQmBu7u7yvkCAwPh4uKifOzt7Y2QkBC161jeNVM8Vhz33LlzaNGiBZycnJTb27VrB7lcjgsXLijLmjZtCmtra+VjX19f5XlOnToFmUyGhg0bqlz7X3/9VeW6Ozo6Ijg4WOMxyjJ8+HCcOHECjRo1wvjx47Fz585y9yeqDmyMHQARqXv66acRExODqVOnYvjw4SrbrKys1L7UH+2boWBra6vyWCKRaCyTy+Vax3X//n306tULc+fOVdvm6+ur/PejX+SG1KBBA0gkEpw/f14vxzPENavMuRXnuX//PqytrXH06FGVBAgAnJ2dyz3GkxLAVq1aIS0tDT///DN++eUX9O/fH9HR0Wr9hoiqE7bcEJmoOXPmYOvWrTh48KBKea1atZCZmanypaXPuWke7YT78OFDHD16FE2aNAFQ+kV45swZBAYGon79+io/uiQ0rq6uqF27Nvbv369Svn//foSEhGh9HA8PD8TExGDBggXIz89X237v3j0AQJMmTXDjxg3cuHFDue3s2bO4d++eTucry6PXTPFYcc2aNGmCkydPqsS3f/9+WFlZoVGjRlodv2XLlpDJZMjOzla77j4+PlrHaWdnB5lMplbu6uqKAQMG4Ouvv8aaNWuwYcMG3LlzR+vjEpkaJjdEJqpZs2YYPHgwPv/8c5XyZ555Brdu3cJHH32EK1euYMGCBfj555/1dt4FCxZg06ZNOH/+PMaOHYu7d+/ilVdeAQCMHTsWd+7cwaBBg/DHH3/gypUr2LFjB0aMGKHxS7M8b731FubOnYs1a9bgwoULmDJlCk6cOIE333xT53hlMhkiIiKwYcMGXLp0CefOncPnn3+uvF0UHR2tvJ7Hjh1DamoqYmNj0bFjR4SHh+t0Pk3WrVuHpUuX4uLFi4iPj0dqairGjRsHABg8eDDs7e0xbNgwnD59Gnv27MEbb7yBoUOHwtvbW6vjN2zYEIMHD0ZsbCw2btyItLQ0pKamIjExEdu2bdM6zsDAQPz555+4cOECbt++jZKSEiQlJWHVqlU4f/48Ll68iHXr1sHHxwfu7u4VuRREJoHJDZEJmzFjhtotkCZNmmDhwoVYsGABWrRogdTUVEyePFlv55wzZw7mzJmDFi1aYN++fdiyZQs8PT0BQNnaIpPJ0LVrVzRr1gwTJkyAu7u7Sr8UbYwfPx5xcXGYNGkSmjVrhuTkZGzZsgUNGjTQ6Tj16tXDsWPH8Oyzz2LSpEkIDQ1Fly5dsGvXLixatAhA6e2ZzZs3o0aNGnj66acRHR2NevXqYc2aNTqdqywJCQlYvXo1mjdvju+++w6rVq1Stgg5Ojpix44duHPnDp566in069cPnTt3xpdffqnTOZYtW4bY2FhMmjQJjRo1Qt++ffHHH3+gbt26Wh9j1KhRaNSoEcLDw1GrVi3s378fLi4u+OijjxAeHo6nnnoK165dw/bt23WuTyJTIhHa9sgjIiI1EokEmzZt4sy/RCaEqTkRERGZFSY3REREZFY4FJyIqBJ4Z5/I9LDlhoiIiMwKkxsiIiIyK0xuiIiIyKwwuSEiIiKzwuSGiIiIzAqTGyIiIjIrTG6IiIjIrDC5ISIiIrPC5IaIiIjMyv8DOIh2/XqKH3IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_components_from_pca(scaled_data:pd.DataFrame, threshold:float) -> int:\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "    print(cumulative_explained_variance)\n",
    "    # 누적 설명 분산 시각화\n",
    "    plt.plot(cumulative_explained_variance, marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Cumulative Explained Variance by Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # 적절한 주성분 개수 선택\n",
    "    n_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n",
    "    return n_components\n",
    "get_n_components_from_pca(X_scaled, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA로 차원 축소\n",
    "pca = PCA(n_components=86)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "x_train = X_pca.astype('float32')\n",
    "# stratify로 학습에 용이하게 비율 유지\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train, 2).astype(int)\n",
    "y_validation=to_categorical(y_validation, 2).astype(int)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder()\n",
    "# y_train = y_train.values\n",
    "# y_train = encoder.fit_transform(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 아래 칸은 최적의 hyperparameter를 찾기 위해서 실행하는 칸이므로, 실행할 필요 없습니다.\n",
    "\n",
    "모델만을 알고싶다면 넘어가세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best average accuracy: 0.8014812111854553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4818 - accuracy: 0.7952\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4630 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4905 - accuracy: 0.7897\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4645 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4982 - accuracy: 0.7855\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4626 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7999\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4884 - accuracy: 0.7915\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4622 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4937 - accuracy: 0.7929\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4625 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7999\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4748 - accuracy: 0.7960\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4589 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8007\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4799 - accuracy: 0.7952\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4593 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4768 - accuracy: 0.7941\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4586 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8007\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4776 - accuracy: 0.7932\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4590 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8006\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4860 - accuracy: 0.7866\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4597 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8007\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4687 - accuracy: 0.7972\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4569 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4541 - accuracy: 0.8010\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4713 - accuracy: 0.7970\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4567 - accuracy: 0.8006\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.8004\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4750 - accuracy: 0.7942\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4574 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.8012\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4885 - accuracy: 0.7885\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4580 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8011\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4677 - accuracy: 0.7973\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4563 - accuracy: 0.8002\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8005\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4922 - accuracy: 0.7882\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4637 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4855 - accuracy: 0.7979\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4654 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4628 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4914 - accuracy: 0.7899\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4634 - accuracy: 0.7984\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4597 - accuracy: 0.7993\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.5007 - accuracy: 0.7851\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4653 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.7988\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.5074 - accuracy: 0.7819\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4657 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7984\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4788 - accuracy: 0.7954\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4595 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.8018\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4793 - accuracy: 0.7958\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4610 - accuracy: 0.7984\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4813 - accuracy: 0.7923\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4605 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8006\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4790 - accuracy: 0.7964\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4600 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7992\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4850 - accuracy: 0.7924\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4610 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4710 - accuracy: 0.7970\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4570 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.8014\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4757 - accuracy: 0.7959\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4579 - accuracy: 0.8001\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4770 - accuracy: 0.7939\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4580 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8004\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 6s 6ms/step - loss: 0.4718 - accuracy: 0.7963\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4574 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.7991\n",
      "Epoch 1/45\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4688 - accuracy: 0.7983\n",
      "Epoch 2/45\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4572 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.8010\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4837 - accuracy: 0.7969\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4635 - accuracy: 0.7978\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4854 - accuracy: 0.7947\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4656 - accuracy: 0.7982\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4854 - accuracy: 0.7940\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4645 - accuracy: 0.7977\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4966 - accuracy: 0.7842\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4646 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4814 - accuracy: 0.7961\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4611 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7995\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4736 - accuracy: 0.7972\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4586 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.8015\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4769 - accuracy: 0.7962\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4588 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7992\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4809 - accuracy: 0.7929\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4598 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4556 - accuracy: 0.8005\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4760 - accuracy: 0.7958\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4593 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8012\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4707 - accuracy: 0.7986\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4582 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.8006\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4744 - accuracy: 0.7951\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4568 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.8017\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4751 - accuracy: 0.7942\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4576 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8004\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4778 - accuracy: 0.7941\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4574 - accuracy: 0.8000\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4545 - accuracy: 0.8015\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4675 - accuracy: 0.7985\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4566 - accuracy: 0.8001\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8014\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4721 - accuracy: 0.7968\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 5ms/step - loss: 0.4569 - accuracy: 0.8003\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8010\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4910 - accuracy: 0.7919\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4659 - accuracy: 0.7978\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4671 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4948 - accuracy: 0.7902\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4660 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4613 - accuracy: 0.7972\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4834 - accuracy: 0.7974\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4649 - accuracy: 0.7977\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4930 - accuracy: 0.7915\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4639 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7982\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4931 - accuracy: 0.7923\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4640 - accuracy: 0.7988\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4806 - accuracy: 0.7938\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4603 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.7990\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4763 - accuracy: 0.7980\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4598 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4845 - accuracy: 0.7904\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4609 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7994\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4775 - accuracy: 0.7977\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4581 - accuracy: 0.8004\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4787 - accuracy: 0.7951\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4605 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4732 - accuracy: 0.7948\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4577 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.7994\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4723 - accuracy: 0.7968\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4575 - accuracy: 0.8003\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.7992\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4762 - accuracy: 0.7947\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4581 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8011\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4700 - accuracy: 0.7977\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4574 - accuracy: 0.7999\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.8009\n",
      "Epoch 1/60\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4744 - accuracy: 0.7937\n",
      "Epoch 2/60\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4577 - accuracy: 0.8000\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.8011\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4915 - accuracy: 0.7866\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4617 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7988\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4882 - accuracy: 0.7925\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4616 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4905 - accuracy: 0.7880\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4620 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4586 - accuracy: 0.7992\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4875 - accuracy: 0.7941\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4623 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4822 - accuracy: 0.7964\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4615 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.7990\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4729 - accuracy: 0.7975\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4583 - accuracy: 0.7992\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.8011\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4797 - accuracy: 0.7935\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4592 - accuracy: 0.7994\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.7995\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4737 - accuracy: 0.7969\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4584 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.8011\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4858 - accuracy: 0.7918\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4599 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.7998\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4842 - accuracy: 0.7901\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4601 - accuracy: 0.7993\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7997\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4725 - accuracy: 0.7951\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4571 - accuracy: 0.8001\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8017\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4705 - accuracy: 0.7968\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4567 - accuracy: 0.8002\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8002\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4680 - accuracy: 0.7981\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4562 - accuracy: 0.8001\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.8023\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4686 - accuracy: 0.7980\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4569 - accuracy: 0.8000\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8018\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4738 - accuracy: 0.7961\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 5ms/step - loss: 0.4569 - accuracy: 0.8002\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8014\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4859 - accuracy: 0.7959\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4634 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7991\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4880 - accuracy: 0.7946\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4645 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4618 - accuracy: 0.7970\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4882 - accuracy: 0.7940\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4636 - accuracy: 0.7985\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7988\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4884 - accuracy: 0.7917\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4663 - accuracy: 0.7980\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4668 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 3ms/step - loss: 0.4829 - accuracy: 0.7975\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 2s 3ms/step - loss: 0.4655 - accuracy: 0.7979\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 4ms/step - loss: 0.4766 - accuracy: 0.7963\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7987\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.8000\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 4s 4ms/step - loss: 0.4768 - accuracy: 0.7968\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7984\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4850 - accuracy: 0.7867\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4599 - accuracy: 0.7991\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.8013\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4750 - accuracy: 0.7973\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4598 - accuracy: 0.7989\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.7999\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4836 - accuracy: 0.7947\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 3s 4ms/step - loss: 0.4611 - accuracy: 0.7986\n",
      "182/182 [==============================] - 0s 1ms/step - loss: 0.4574 - accuracy: 0.7984\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4697 - accuracy: 0.7977\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4569 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8018\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4702 - accuracy: 0.7977\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4575 - accuracy: 0.8000\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7998\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4682 - accuracy: 0.7981\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4570 - accuracy: 0.7997\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.8012\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4738 - accuracy: 0.7959\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4575 - accuracy: 0.7998\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8005\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4745 - accuracy: 0.7955\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 4s 6ms/step - loss: 0.4579 - accuracy: 0.7996\n",
      "182/182 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.8002\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5428 - accuracy: 0.7816\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4826 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5382 - accuracy: 0.7849\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4734 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4627 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5551 - accuracy: 0.7581\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4828 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.5313 - accuracy: 0.7827\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4781 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4637 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5269 - accuracy: 0.7767\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4768 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5535 - accuracy: 0.7350\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4686 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5128 - accuracy: 0.7776\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4708 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7969\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5102 - accuracy: 0.7873\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4687 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4596 - accuracy: 0.7992\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5395 - accuracy: 0.7589\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4730 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7978\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4894 - accuracy: 0.7957\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4653 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4845 - accuracy: 0.7965\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4607 - accuracy: 0.7991\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.8008\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4892 - accuracy: 0.7942\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4619 - accuracy: 0.7991\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.7994\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4943 - accuracy: 0.7920\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4634 - accuracy: 0.7984\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4575 - accuracy: 0.8003\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.5020 - accuracy: 0.7882\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4648 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.5038 - accuracy: 0.7880\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4656 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5194 - accuracy: 0.7868\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4780 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5418 - accuracy: 0.7608\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4784 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.6866 - accuracy: 0.6757\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4969 - accuracy: 0.7962\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5394 - accuracy: 0.7879\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4832 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4667 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5266 - accuracy: 0.7948\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4795 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4658 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4942 - accuracy: 0.7974\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4664 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7993\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5219 - accuracy: 0.7738\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4706 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4610 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5173 - accuracy: 0.7809\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4715 - accuracy: 0.7977\n",
      "37/37 [==============================] - 1s 3ms/step - loss: 0.4649 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5100 - accuracy: 0.7849\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4695 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5054 - accuracy: 0.7947\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4686 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7980\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 3s 17ms/step - loss: 0.5032 - accuracy: 0.7789\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4651 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7999\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 3s 17ms/step - loss: 0.4807 - accuracy: 0.7979\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 17ms/step - loss: 0.4618 - accuracy: 0.7987\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 3s 17ms/step - loss: 0.4820 - accuracy: 0.7971\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 17ms/step - loss: 0.4614 - accuracy: 0.7986\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.8004\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 3s 16ms/step - loss: 0.5236 - accuracy: 0.7751\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 16ms/step - loss: 0.4679 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "145/145 [==============================] - 3s 16ms/step - loss: 0.4902 - accuracy: 0.7949\n",
      "Epoch 2/45\n",
      "145/145 [==============================] - 2s 16ms/step - loss: 0.4634 - accuracy: 0.7984\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7995\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5348 - accuracy: 0.7765\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4761 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7990\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5696 - accuracy: 0.7382\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4824 - accuracy: 0.7975\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4623 - accuracy: 0.7969\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5166 - accuracy: 0.7915\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4723 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4612 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.6056 - accuracy: 0.7146\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4867 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4693 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5715 - accuracy: 0.7479\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4873 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4920 - accuracy: 0.7941\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4652 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4990 - accuracy: 0.7910\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4672 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.7973\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5153 - accuracy: 0.7809\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4700 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5048 - accuracy: 0.7885\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4674 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.7995\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5199 - accuracy: 0.7756\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4702 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 13ms/step - loss: 0.5090 - accuracy: 0.7820\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4659 - accuracy: 0.7977\n",
      "37/37 [==============================] - 1s 3ms/step - loss: 0.4582 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 15ms/step - loss: 0.4907 - accuracy: 0.7942\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4631 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 15ms/step - loss: 0.4942 - accuracy: 0.7919\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4628 - accuracy: 0.7988\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.8004\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 15ms/step - loss: 0.4982 - accuracy: 0.7874\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4647 - accuracy: 0.7985\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4890 - accuracy: 0.7944\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4612 - accuracy: 0.7994\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.8002\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5481 - accuracy: 0.7821\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4798 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5355 - accuracy: 0.7734\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4800 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5323 - accuracy: 0.7844\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4810 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4673 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5424 - accuracy: 0.7714\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4842 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5293 - accuracy: 0.7854\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4763 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5009 - accuracy: 0.7952\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4685 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4602 - accuracy: 0.7989\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5152 - accuracy: 0.7794\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4694 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4621 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5098 - accuracy: 0.7871\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4699 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4623 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4980 - accuracy: 0.7973\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4680 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7981\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.5289 - accuracy: 0.7735\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4733 - accuracy: 0.7978\n",
      "37/37 [==============================] - 1s 3ms/step - loss: 0.4605 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4942 - accuracy: 0.7906\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4647 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4589 - accuracy: 0.7993\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4857 - accuracy: 0.7966\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 3s 19ms/step - loss: 0.4628 - accuracy: 0.7988\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.7983\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.5132 - accuracy: 0.7823\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4663 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4579 - accuracy: 0.7996\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4961 - accuracy: 0.7912\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4641 - accuracy: 0.7986\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.8002\n",
      "Epoch 1/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4991 - accuracy: 0.7884\n",
      "Epoch 2/60\n",
      "145/145 [==============================] - 3s 18ms/step - loss: 0.4643 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4577 - accuracy: 0.7995\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5654 - accuracy: 0.7539\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4831 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.6326 - accuracy: 0.7083\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4923 - accuracy: 0.7966\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4723 - accuracy: 0.7969\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5661 - accuracy: 0.7501\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4839 - accuracy: 0.7976\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.7988\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5278 - accuracy: 0.7863\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4763 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4638 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 8ms/step - loss: 0.5151 - accuracy: 0.7893\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 8ms/step - loss: 0.4740 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5319 - accuracy: 0.7793\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4723 - accuracy: 0.7972\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4615 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5093 - accuracy: 0.7944\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4676 - accuracy: 0.7987\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5064 - accuracy: 0.7935\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4673 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4584 - accuracy: 0.7995\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5042 - accuracy: 0.7909\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.4669 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5129 - accuracy: 0.7851\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4693 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4593 - accuracy: 0.7983\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.5034 - accuracy: 0.7840\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4649 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.7997\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4822 - accuracy: 0.7971\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4602 - accuracy: 0.7999\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4560 - accuracy: 0.7992\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.5190 - accuracy: 0.7746\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4675 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.7996\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4849 - accuracy: 0.7951\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 14ms/step - loss: 0.4622 - accuracy: 0.7988\n",
      "37/37 [==============================] - 1s 4ms/step - loss: 0.4558 - accuracy: 0.7997\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 15ms/step - loss: 0.4949 - accuracy: 0.7903\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 15ms/step - loss: 0.4638 - accuracy: 0.7986\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5402 - accuracy: 0.7817\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4811 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 10ms/step - loss: 0.5166 - accuracy: 0.7940\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4745 - accuracy: 0.7982\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4618 - accuracy: 0.7969\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5312 - accuracy: 0.7893\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 10ms/step - loss: 0.4833 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5546 - accuracy: 0.7769\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4840 - accuracy: 0.7980\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4653 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 9ms/step - loss: 0.5132 - accuracy: 0.7955\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 1s 9ms/step - loss: 0.4750 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5248 - accuracy: 0.7718\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4717 - accuracy: 0.7978\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4599 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5261 - accuracy: 0.7792\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4717 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7968\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5040 - accuracy: 0.7925\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.4694 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5320 - accuracy: 0.7701\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4735 - accuracy: 0.7977\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 2s 12ms/step - loss: 0.5330 - accuracy: 0.7722\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 11ms/step - loss: 0.4727 - accuracy: 0.7979\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 16ms/step - loss: 0.4952 - accuracy: 0.7922\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 16ms/step - loss: 0.4637 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.8011\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 17ms/step - loss: 0.5130 - accuracy: 0.7734\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 2s 17ms/step - loss: 0.4653 - accuracy: 0.7989\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7988\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 4s 20ms/step - loss: 0.5011 - accuracy: 0.7881\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 3s 20ms/step - loss: 0.4653 - accuracy: 0.7981\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7999\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 20ms/step - loss: 0.4848 - accuracy: 0.7970\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 3s 20ms/step - loss: 0.4614 - accuracy: 0.7984\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.7998\n",
      "Epoch 1/100\n",
      "145/145 [==============================] - 3s 20ms/step - loss: 0.4898 - accuracy: 0.7949\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 3s 20ms/step - loss: 0.4635 - accuracy: 0.7983\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5639 - accuracy: 0.7526\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4950 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4719 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.6060 - accuracy: 0.7080\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4983 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4710 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 15ms/step - loss: 0.5463 - accuracy: 0.7722\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.4889 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4704 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.6012 - accuracy: 0.7025\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4966 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4714 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.7690 - accuracy: 0.5471\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5289 - accuracy: 0.7916\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4872 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5528 - accuracy: 0.7565\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4815 - accuracy: 0.7961\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4638 - accuracy: 0.7989\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5083 - accuracy: 0.7965\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4716 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5502 - accuracy: 0.7737\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4814 - accuracy: 0.7973\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4630 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5215 - accuracy: 0.7875\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4751 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4632 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5575 - accuracy: 0.7611\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4901 - accuracy: 0.7968\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5030 - accuracy: 0.7916\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4684 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4592 - accuracy: 0.7995\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5315 - accuracy: 0.7718\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4720 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4598 - accuracy: 0.7975\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5173 - accuracy: 0.7883\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4696 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4601 - accuracy: 0.7996\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5200 - accuracy: 0.7781\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 25ms/step - loss: 0.4708 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 25ms/step - loss: 0.5028 - accuracy: 0.7928\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4659 - accuracy: 0.7983\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4593 - accuracy: 0.7990\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 15ms/step - loss: 0.5286 - accuracy: 0.7974\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4876 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4717 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 15ms/step - loss: 0.5690 - accuracy: 0.7614\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4945 - accuracy: 0.7981\n",
      "19/19 [==============================] - 1s 5ms/step - loss: 0.4690 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.6259 - accuracy: 0.6972\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.5091 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4887 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.5574 - accuracy: 0.7915\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4952 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4726 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.6054 - accuracy: 0.7221\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.5029 - accuracy: 0.7969\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4687 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5327 - accuracy: 0.7816\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4791 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5257 - accuracy: 0.7905\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4784 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.7968\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5454 - accuracy: 0.7704\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4823 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4648 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5755 - accuracy: 0.7403\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4892 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4658 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 2s 20ms/step - loss: 0.6502 - accuracy: 0.6800\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5023 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4702 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 3s 31ms/step - loss: 0.5351 - accuracy: 0.7646\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 0.4744 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4627 - accuracy: 0.7986\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 3s 31ms/step - loss: 0.5227 - accuracy: 0.7791\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 0.4720 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4599 - accuracy: 0.7969\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 3s 31ms/step - loss: 0.5099 - accuracy: 0.7911\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 30ms/step - loss: 0.4690 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4591 - accuracy: 0.7987\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 3s 30ms/step - loss: 0.5373 - accuracy: 0.7632\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 30ms/step - loss: 0.4719 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4596 - accuracy: 0.7977\n",
      "Epoch 1/45\n",
      "73/73 [==============================] - 3s 30ms/step - loss: 0.5392 - accuracy: 0.7693\n",
      "Epoch 2/45\n",
      "73/73 [==============================] - 2s 30ms/step - loss: 0.4754 - accuracy: 0.7973\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4631 - accuracy: 0.7979\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/45\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.6002 - accuracy: 0.7420\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5024 - accuracy: 0.7971\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4787 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5522 - accuracy: 0.7751\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4930 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.6025 - accuracy: 0.7162\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5039 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4793 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.6279 - accuracy: 0.6949\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4998 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4748 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.6559 - accuracy: 0.6615\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5074 - accuracy: 0.7942\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4754 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5538 - accuracy: 0.7574\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4827 - accuracy: 0.7968\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5520 - accuracy: 0.7629\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4834 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4652 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5268 - accuracy: 0.7788\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4779 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4627 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.6270 - accuracy: 0.6806\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4905 - accuracy: 0.7973\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4672 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5375 - accuracy: 0.7722\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4767 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4602 - accuracy: 0.7982\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.5850 - accuracy: 0.7295\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.4780 - accuracy: 0.7963\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4612 - accuracy: 0.7994\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.5421 - accuracy: 0.7565\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.4733 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4600 - accuracy: 0.7970\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.5061 - accuracy: 0.7906\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 27ms/step - loss: 0.4669 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4584 - accuracy: 0.7997\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5736 - accuracy: 0.7471\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4780 - accuracy: 0.7967\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4616 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.5176 - accuracy: 0.7862\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 26ms/step - loss: 0.4707 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4598 - accuracy: 0.7988\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.5859 - accuracy: 0.7328\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4998 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4781 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.5645 - accuracy: 0.7685\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4932 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4728 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 15ms/step - loss: 0.5965 - accuracy: 0.7284\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.5108 - accuracy: 0.7970\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4803 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 15ms/step - loss: 0.6019 - accuracy: 0.7266\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.5053 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.8290 - accuracy: 0.4995\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5361 - accuracy: 0.7941\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4936 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 22ms/step - loss: 0.5848 - accuracy: 0.7278\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4900 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4662 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5606 - accuracy: 0.7490\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4880 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4661 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5170 - accuracy: 0.7917\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4768 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4642 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5280 - accuracy: 0.7888\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4774 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4620 - accuracy: 0.7977\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5245 - accuracy: 0.7919\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4803 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 32ms/step - loss: 0.5309 - accuracy: 0.7734\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 33ms/step - loss: 0.4748 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4624 - accuracy: 0.7986\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 32ms/step - loss: 0.5173 - accuracy: 0.7810\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 0.4707 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4609 - accuracy: 0.7968\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 32ms/step - loss: 0.5338 - accuracy: 0.7698\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 0.4742 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4615 - accuracy: 0.7987\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 31ms/step - loss: 0.5133 - accuracy: 0.7900\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 31ms/step - loss: 0.4717 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4591 - accuracy: 0.7979\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 3s 31ms/step - loss: 0.5333 - accuracy: 0.7764\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 2s 32ms/step - loss: 0.4753 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4613 - accuracy: 0.7980\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/60\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5780 - accuracy: 0.7465\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.4986 - accuracy: 0.7973\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5855 - accuracy: 0.7228\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.4937 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4702 - accuracy: 0.7968\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5487 - accuracy: 0.7792\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4906 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4704 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.5552 - accuracy: 0.7913\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.4955 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 1s 14ms/step - loss: 0.6281 - accuracy: 0.6856\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 15ms/step - loss: 0.5125 - accuracy: 0.7968\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5195 - accuracy: 0.7880\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 19ms/step - loss: 0.4762 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4614 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5754 - accuracy: 0.7408\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4884 - accuracy: 0.7957\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4651 - accuracy: 0.7969\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5118 - accuracy: 0.7969\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4724 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 19ms/step - loss: 0.5487 - accuracy: 0.7623\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4787 - accuracy: 0.7967\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4611 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 18ms/step - loss: 0.5246 - accuracy: 0.7820\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 18ms/step - loss: 0.4807 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4632 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 28ms/step - loss: 0.5138 - accuracy: 0.7878\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.4696 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4591 - accuracy: 0.7999\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.5258 - accuracy: 0.7821\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.4727 - accuracy: 0.7974\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4594 - accuracy: 0.7982\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.5199 - accuracy: 0.7757\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.4703 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4593 - accuracy: 0.7993\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.5120 - accuracy: 0.7873\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.4686 - accuracy: 0.7980\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4594 - accuracy: 0.7984\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.5058 - accuracy: 0.7906\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 28ms/step - loss: 0.4680 - accuracy: 0.7981\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4587 - accuracy: 0.7989\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.6007 - accuracy: 0.7315\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.5002 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4742 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5783 - accuracy: 0.7450\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.5020 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.7968\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5486 - accuracy: 0.7804\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 17ms/step - loss: 0.4937 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4730 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 17ms/step - loss: 0.5644 - accuracy: 0.7673\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.5012 - accuracy: 0.7975\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 16ms/step - loss: 0.5817 - accuracy: 0.7617\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 1s 16ms/step - loss: 0.4999 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4719 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5196 - accuracy: 0.7871\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.4767 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5471 - accuracy: 0.7598\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4824 - accuracy: 0.7982\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4668 - accuracy: 0.7968\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5217 - accuracy: 0.7840\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4791 - accuracy: 0.7976\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4673 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.5383 - accuracy: 0.7870\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 21ms/step - loss: 0.4796 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4643 - accuracy: 0.7977\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 2s 22ms/step - loss: 0.5515 - accuracy: 0.7645\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 23ms/step - loss: 0.4876 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4668 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 35ms/step - loss: 0.5081 - accuracy: 0.7937\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 3s 35ms/step - loss: 0.4693 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.7986\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 35ms/step - loss: 0.5618 - accuracy: 0.7701\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 3s 35ms/step - loss: 0.4812 - accuracy: 0.7978\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4643 - accuracy: 0.7968\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 34ms/step - loss: 0.5089 - accuracy: 0.7936\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 34ms/step - loss: 0.4692 - accuracy: 0.7977\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4616 - accuracy: 0.7987\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 34ms/step - loss: 0.5244 - accuracy: 0.7822\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 34ms/step - loss: 0.4737 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4622 - accuracy: 0.7978\n",
      "Epoch 1/100\n",
      "73/73 [==============================] - 3s 34ms/step - loss: 0.5346 - accuracy: 0.7627\n",
      "Epoch 2/100\n",
      "73/73 [==============================] - 2s 34ms/step - loss: 0.4726 - accuracy: 0.7979\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4608 - accuracy: 0.7979\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      "906/906 [==============================] - 6s 7ms/step - loss: 0.4698 - accuracy: 0.7968\n",
      "Epoch 2/100\n",
      "906/906 [==============================] - 6s 6ms/step - loss: 0.4561 - accuracy: 0.8006\n",
      "Best params: {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
      "Best average accuracy: 0.8014812111854553\n"
     ]
    }
   ],
   "source": [
    "# KerasClassifier을 위한 build_fn 정의 (build_model)\n",
    "# 신경망 model 생성\n",
    "def build_model(num_layers, num_nodes, loss):\n",
    "    # Define and compile the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(num_nodes, input_dim=86, activation='relu'))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(num_nodes, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "# KFold validation 사용\n",
    "# k: n_splits=5\n",
    "kfold = KFold(random_state=30,\n",
    "           n_splits=5,\n",
    "           shuffle=True\n",
    "          )\n",
    "model = KerasClassifier(build_fn = build_model)\n",
    "# model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)\n",
    "# 최적 hyperparameter을 찾기위한 gridsearchCV를 위한 parameter\n",
    "parameters = {\n",
    "    'batch_size': [1000,5000,10000],\n",
    "    'epochs': [45,60,100],\n",
    "    'num_layers': [2, 3],\n",
    "    'num_nodes': [10, 20, 40],\n",
    "    'loss':[\"categorical_crossentropy\",\"sparse_categorical_crossentropy\"]\n",
    "    }\n",
    "# GridSearchCV 생성\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           cv = kfold)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "# GridSearchCV fit 시작\n",
    "grid_search.fit(x_train, y_train, callbacks=[early_stopping])\n",
    "# 최적의 param\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "# 최적의 param일 경우 최적의 accuracy\n",
    "print(f\"Best average accuracy: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.75789332, 4.30777836, 4.33999658, 5.5541275 , 5.11061521,\n",
       "        6.16098056, 1.06994209, 0.63555026, 0.81090918, 0.7712852 ,\n",
       "        0.68296213, 0.72728543, 4.50320058, 4.00286765, 4.94313641,\n",
       "        5.41075573, 4.64401445, 5.67322831, 0.577356  , 0.52926874,\n",
       "        0.55411096, 0.6682375 , 0.57839413, 0.66581478, 4.41535201,\n",
       "        4.13779402, 5.41786242, 5.87651119, 4.68887033, 6.29433832,\n",
       "        0.56359096, 0.6403368 , 0.5700037 , 0.63589191, 0.68769994,\n",
       "        0.61517172, 5.39195166, 4.64282236, 5.5790482 , 6.40150347,\n",
       "        5.49485493, 6.54536386, 0.63533525, 0.56839733, 0.63380957,\n",
       "        0.61277924, 0.61797104, 0.69280815, 2.3939703 , 2.55472102,\n",
       "        3.02915616, 2.83349409, 2.92165227, 3.80169635, 0.57504468,\n",
       "        0.64414258, 0.57909603, 0.68418126, 0.62514095, 0.62413435,\n",
       "        2.6084971 , 2.80952044, 3.35774288, 2.88276944, 3.37500687,\n",
       "        3.88307576, 0.63523531, 0.5669229 , 0.64589343, 0.62757993,\n",
       "        0.63945289, 0.70597734, 2.55939112, 2.8867475 , 3.64117084,\n",
       "        3.65592275, 4.01937208, 5.61894636, 0.56264319, 0.63029599,\n",
       "        0.56967788, 0.69054985, 0.63706007, 0.62201176, 3.31586895,\n",
       "        3.58624902, 4.52967029, 3.58617311, 4.18562012, 5.06780596,\n",
       "        0.65096827, 0.58028545, 0.66880159, 0.62582722, 0.62714653,\n",
       "        0.69144883, 2.60070233, 2.84532461, 3.53162904, 3.12774596,\n",
       "        3.21776538, 4.48410645, 0.58273516, 0.63797898, 0.58013473,\n",
       "        0.70094142, 0.63391628, 0.64144621, 2.69252563, 2.9007658 ,\n",
       "        3.74797025, 3.06999903, 3.48922968, 4.26013541, 0.63610458,\n",
       "        0.57622952, 0.64924726, 0.63610687, 0.6348362 , 0.70157228,\n",
       "        2.66763859, 2.95601783, 3.62160702, 3.11152101, 3.26429882,\n",
       "        4.26945801, 0.5815352 , 0.65600915, 0.57842512, 0.6321939 ,\n",
       "        0.71310449, 0.63216314, 2.75645218, 2.90050521, 3.66597586,\n",
       "        3.06877189, 3.17519131, 3.843221  , 0.52552829, 0.60628991,\n",
       "        0.55357251, 0.64502273, 0.57817197, 0.58486323]),\n",
       " 'std_fit_time': array([0.1925421 , 0.70643852, 0.23036074, 0.27687868, 0.70740904,\n",
       "        0.48064259, 0.65007861, 0.05123591, 0.22396597, 0.08290501,\n",
       "        0.0113133 , 0.14846591, 0.10078073, 0.14086615, 0.11775685,\n",
       "        0.18139353, 0.17838538, 0.17603391, 0.10472702, 0.00798936,\n",
       "        0.01453697, 0.11231465, 0.00461625, 0.11846247, 0.0752628 ,\n",
       "        0.36171876, 0.16174982, 0.13106738, 0.12312872, 0.44463949,\n",
       "        0.00338059, 0.15186067, 0.0181327 , 0.01929887, 0.12846141,\n",
       "        0.01354364, 0.17062119, 0.122625  , 0.33168178, 0.12750227,\n",
       "        0.20194749, 0.19265126, 0.13417508, 0.0141197 , 0.12446636,\n",
       "        0.01407986, 0.01281844, 0.13071449, 0.07483336, 0.04912983,\n",
       "        0.11132352, 0.17919264, 0.10370253, 0.17543747, 0.01633223,\n",
       "        0.11406055, 0.01604342, 0.12922842, 0.01232107, 0.01287192,\n",
       "        0.13487472, 0.16486546, 0.07169599, 0.05392444, 0.14851606,\n",
       "        0.12780805, 0.12663606, 0.00923551, 0.12084385, 0.01360703,\n",
       "        0.02168398, 0.12653027, 0.04402305, 0.12309826, 0.36773721,\n",
       "        0.06957663, 0.08604568, 0.23164911, 0.01028971, 0.12516964,\n",
       "        0.01060794, 0.13399346, 0.01266073, 0.00460516, 0.13858651,\n",
       "        0.05027122, 0.06102613, 0.09785777, 0.16114583, 0.10292329,\n",
       "        0.11975696, 0.0185183 , 0.13331354, 0.01699588, 0.01131884,\n",
       "        0.11278957, 0.03609214, 0.06073799, 0.06543839, 0.1484835 ,\n",
       "        0.05247898, 0.20613217, 0.01387872, 0.12880793, 0.01466612,\n",
       "        0.14169636, 0.01725185, 0.02145641, 0.04095719, 0.04548888,\n",
       "        0.15944221, 0.06457493, 0.17703272, 0.0501623 , 0.11749625,\n",
       "        0.01283318, 0.13352551, 0.01860062, 0.01542479, 0.11937576,\n",
       "        0.03690483, 0.15926895, 0.02763149, 0.20237486, 0.08696316,\n",
       "        0.16118322, 0.01625721, 0.15295804, 0.00936044, 0.01147482,\n",
       "        0.10967453, 0.00955385, 0.21203266, 0.04665094, 0.16664894,\n",
       "        0.02931282, 0.14781121, 0.10848926, 0.02116663, 0.13110688,\n",
       "        0.01351043, 0.11703499, 0.01307747, 0.0148723 ]),\n",
       " 'mean_score_time': array([0.33157196, 0.40184903, 0.43260121, 0.4361794 , 0.4825696 ,\n",
       "        0.52517872, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35999207, 0.35971227, 0.36958251,\n",
       "        0.36802959, 0.37944322, 0.3879683 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35667109,\n",
       "        0.39519477, 0.44402423, 0.41292124, 0.42210951, 0.45828376,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.42004042, 0.44977655, 0.5079185 , 0.44864593,\n",
       "        0.45054054, 0.47415247, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.26356068, 0.31496511,\n",
       "        0.2651782 , 0.26714931, 0.27433705, 0.35577364, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.26445231, 0.28484983, 0.34496984, 0.28458638, 0.27349777,\n",
       "        0.3001502 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.26717362, 0.2744226 , 0.28119888,\n",
       "        0.35522561, 0.2940464 , 0.32676167, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28908663,\n",
       "        0.29245405, 0.37640967, 0.28216839, 0.29188142, 0.30699096,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25147376, 0.31427193, 0.27241254, 0.25226898,\n",
       "        0.25559235, 0.27346206, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30740843, 0.24881649,\n",
       "        0.26774778, 0.26253643, 0.27662864, 0.2930141 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25292511, 0.25678778, 0.26993213, 0.25706677, 0.27659907,\n",
       "        0.34506326, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2545846 , 0.25760412, 0.26643229,\n",
       "        0.2719243 , 0.31596055, 0.23909731, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'std_score_time': array([0.02050137, 0.03947583, 0.06922094, 0.0526521 , 0.07209741,\n",
       "        0.10284045, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01595539, 0.01715587, 0.01588864,\n",
       "        0.02158808, 0.02449454, 0.0126265 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01030555,\n",
       "        0.02996239, 0.01335426, 0.00606249, 0.00932753, 0.00808743,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00924377, 0.01198771, 0.11631464, 0.02095503,\n",
       "        0.01997075, 0.01053709, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02202946, 0.11254028,\n",
       "        0.00674691, 0.00586256, 0.01366298, 0.11227568, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0052003 , 0.05353888, 0.12729659, 0.01699933, 0.00456351,\n",
       "        0.01930055, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01340371, 0.02545836, 0.00492736,\n",
       "        0.11703332, 0.01143757, 0.01011604, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02394067,\n",
       "        0.01318879, 0.13863057, 0.00312639, 0.00857724, 0.00558561,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01129101, 0.11526616, 0.00892484, 0.00428621,\n",
       "        0.00787793, 0.01140625, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1237083 , 0.01004114,\n",
       "        0.01075336, 0.01439222, 0.01051107, 0.01676805, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01327633, 0.0122721 , 0.02026204, 0.00534214, 0.00796445,\n",
       "        0.11658345, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.0035026 , 0.00987511, 0.00796712,\n",
       "        0.01400484, 0.09160936, 0.00677545, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " 'param_batch_size': masked_array(data=[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "                    1000, 1000, 1000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
       "                    5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
       "                    5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
       "                    5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
       "                    5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
       "                    5000, 5000, 5000, 5000, 5000, 5000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n",
       "                    10000, 10000, 10000, 10000, 10000, 10000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_epochs': masked_array(data=[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 45, 45,\n",
       "                    45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 60, 60, 60, 60,\n",
       "                    60, 60, 60, 60, 60, 60, 60, 60, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 20, 20, 20, 20, 20, 45, 45, 45, 45, 45, 45,\n",
       "                    45, 45, 45, 45, 45, 45, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "                    60, 60, 60, 60, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100, 100, 100, 100, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "                    20, 20, 20, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "                    45, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "                    100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_loss': masked_array(data=['categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'categorical_crossentropy', 'categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy',\n",
       "                    'sparse_categorical_crossentropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_num_layers': masked_array(data=[2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3,\n",
       "                    2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_num_nodes': masked_array(data=[15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20,\n",
       "                    30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15,\n",
       "                    20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30,\n",
       "                    15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20,\n",
       "                    30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15,\n",
       "                    20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30,\n",
       "                    15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20,\n",
       "                    30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15,\n",
       "                    20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30,\n",
       "                    15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20, 30, 15, 20,\n",
       "                    30, 15, 20, 30],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 1000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 5000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 20,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 45,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 60,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 2,\n",
       "   'num_nodes': 30},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 15},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 20},\n",
       "  {'batch_size': 10000,\n",
       "   'epochs': 100,\n",
       "   'loss': 'sparse_categorical_crossentropy',\n",
       "   'num_layers': 3,\n",
       "   'num_nodes': 30}],\n",
       " 'split0_test_score': array([0.80039102, 0.80079418, 0.80131882, 0.799905  , 0.80023086,\n",
       "        0.79954052,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.80105376, 0.8010869 , 0.80102611,\n",
       "        0.79988289, 0.80050147, 0.80031371,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.80066162,\n",
       "        0.80105376, 0.80116421, 0.79994369, 0.79953498, 0.79992712,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.80204785, 0.80198711, 0.80140722, 0.79933065,\n",
       "        0.80037445, 0.79994917,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79939139, 0.79957914,\n",
       "        0.80027503, 0.79908764, 0.79908764, 0.79908764,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79991603, 0.80023634, 0.79921466, 0.79908764, 0.79909319,\n",
       "        0.79998231,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79908764, 0.79915941, 0.79916495,\n",
       "        0.79908764, 0.79912078, 0.79956812,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79908764,\n",
       "        0.79971725, 0.79939693, 0.79945767, 0.79906553, 0.79908764,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79912633, 0.79908764, 0.80019772, 0.79908764,\n",
       "        0.79909319, 0.79908764,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79908764, 0.79907662,\n",
       "        0.7992478 , 0.79908764, 0.79908764, 0.79915392,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79908764, 0.79908764, 0.79928648, 0.79908764, 0.79908764,\n",
       "        0.79908764,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79908764, 0.79907107, 0.79989946,\n",
       "        0.79908764, 0.79908764, 0.79908764,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'split1_test_score': array([0.79762411, 0.79762411, 0.79824269, 0.79707736, 0.7975744 ,\n",
       "        0.7968896 ,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79819298, 0.79745293, 0.79920363,\n",
       "        0.79769039, 0.7972154 , 0.79874521,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79790574,\n",
       "        0.79833102, 0.79912078, 0.79697794, 0.79670733, 0.79807144,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79744738, 0.7977401 , 0.79841387, 0.79727066,\n",
       "        0.79719883, 0.79855192,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79633176, 0.7964257 ,\n",
       "        0.7970829 , 0.79633176, 0.79670733, 0.79633176,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79633176, 0.7963649 , 0.79704976, 0.7963649 , 0.79633176,\n",
       "        0.79640359,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79633176, 0.79633176, 0.79737556,\n",
       "        0.79632628, 0.79650301, 0.79633176,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79648644,\n",
       "        0.79633176, 0.79776216, 0.79633176, 0.79633176, 0.79641461,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79633176, 0.79660243, 0.79643118, 0.79633176,\n",
       "        0.79632628, 0.79630971,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79639256, 0.79633176,\n",
       "        0.79634833, 0.79633176, 0.79633176, 0.79661345,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79633176, 0.79666317, 0.79637045, 0.79633731, 0.79633176,\n",
       "        0.79633176,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79633176, 0.7964533 , 0.79644775,\n",
       "        0.79633176, 0.79633176, 0.79634285,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'split2_test_score': array([0.79966199, 0.79943556, 0.80049044, 0.79904896, 0.79913735,\n",
       "        0.79970068,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79987186, 0.79983872, 0.80078864,\n",
       "        0.79904896, 0.79949635, 0.79924226,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79916495,\n",
       "        0.80030817, 0.80066162, 0.79945213, 0.80031371, 0.79994917,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79960126, 0.80004859, 0.80120838, 0.79919809,\n",
       "        0.79891646, 0.80070579,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79860163, 0.79862922,\n",
       "        0.79918152, 0.7986182 , 0.79862922, 0.7986182 ,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79862374, 0.79863477, 0.79879493, 0.7986182 , 0.79862374,\n",
       "        0.79925334,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79866791, 0.79864031, 0.79942453,\n",
       "        0.7986182 , 0.7986182 , 0.7986182 ,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.7985906 ,\n",
       "        0.79962885, 0.79986084, 0.7992754 , 0.7986182 , 0.79923677,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.7986182 , 0.79863477, 0.79865688, 0.7986182 ,\n",
       "        0.7986182 , 0.7986182 ,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.7986182 , 0.79861265,\n",
       "        0.79880047, 0.7986182 , 0.7986182 , 0.79863477,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.7986182 , 0.79861265, 0.79860163, 0.7986182 , 0.7986182 ,\n",
       "        0.79860717,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79861265, 0.79859614, 0.79863477,\n",
       "        0.7986182 , 0.7986182 , 0.79862374,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'split3_test_score': array([0.79832   , 0.79973382, 0.79926437, 0.79774559, 0.79986084,\n",
       "        0.79886675,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79846358, 0.79996574, 0.79933614,\n",
       "        0.79758543, 0.79820955, 0.79915941,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79942453,\n",
       "        0.79986638, 0.80083281, 0.79840833, 0.79799414, 0.79864031,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79840833, 0.7998001 , 0.80002654, 0.79769593,\n",
       "        0.79873973, 0.79977798,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79734796, 0.79732037,\n",
       "        0.79778427, 0.79733694, 0.79753572, 0.79838073,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79733694, 0.79733139, 0.79923677, 0.79733694, 0.79733139,\n",
       "        0.79733139,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79732591, 0.7982592 , 0.79760754,\n",
       "        0.79734248, 0.79746395, 0.79904348,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79751366,\n",
       "        0.79828131, 0.79735899, 0.79733694, 0.79734248, 0.79739213,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79733694, 0.79739767, 0.79737008, 0.79733694,\n",
       "        0.79733694, 0.79753023,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79735899, 0.79734248,\n",
       "        0.7974087 , 0.79733694, 0.79733694, 0.79781741,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79733694, 0.79733139, 0.79791129, 0.79733694, 0.79733694,\n",
       "        0.79733694,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79732037, 0.79739213, 0.79739767,\n",
       "        0.79733694, 0.79733694, 0.79733694,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'split4_test_score': array([0.79892749, 0.79995471, 0.80071688, 0.79876179, 0.79876733,\n",
       "        0.80038548,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79925334, 0.79999888, 0.80104822,\n",
       "        0.79849118, 0.79985529, 0.79903239,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79981112,\n",
       "        0.79954052, 0.80020875, 0.80004311, 0.7998001 , 0.79989398,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.80017561, 0.79978353, 0.80091017, 0.79890537,\n",
       "        0.79936379, 0.80043519,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79824817, 0.79824817,\n",
       "        0.79997683, 0.79824269, 0.79854643, 0.79823714,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79838628, 0.79874521, 0.79834759, 0.79824817, 0.79824817,\n",
       "        0.79914838,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79824817, 0.79833102, 0.79910421,\n",
       "        0.79824817, 0.79863477, 0.79831445,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79825372,\n",
       "        0.79828131, 0.79877287, 0.79840833, 0.7982592 , 0.79824817,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79824817, 0.79827029, 0.79824269, 0.79826474,\n",
       "        0.79825372, 0.79824817,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79824817, 0.79824817,\n",
       "        0.79827029, 0.79824817, 0.79824817, 0.79824817,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79824817, 0.79824817, 0.7982316 , 0.79824817, 0.79824817,\n",
       "        0.79824817,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79824817, 0.79825372, 0.79851329,\n",
       "        0.79824817, 0.79824817, 0.79824817,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'mean_test_score': array([0.79898492, 0.79950848, 0.80000664, 0.79850774, 0.79911416,\n",
       "        0.7990766 ,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79936711, 0.79966863, 0.80028055,\n",
       "        0.79853977, 0.79905561, 0.7992986 ,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79939359,\n",
       "        0.79981997, 0.80039763, 0.79896504, 0.79887005, 0.7992964 ,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79953609, 0.79987189, 0.80039324, 0.79848014,\n",
       "        0.79891865, 0.79988401,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79798418, 0.79804052,\n",
       "        0.79886011, 0.79792345, 0.79810127, 0.7981311 ,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79811895, 0.79826252, 0.79852874, 0.79793117, 0.79792565,\n",
       "        0.7984238 ,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79793228, 0.79814434, 0.79853536,\n",
       "        0.79792455, 0.79806814, 0.7983752 ,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.79798641,\n",
       "        0.7984481 , 0.79863036, 0.79816202, 0.79792343, 0.79807587,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.79793228, 0.79799856, 0.79817971, 0.79792786,\n",
       "        0.79792566, 0.79795879,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.79794111, 0.79792234,\n",
       "        0.79801512, 0.79792454, 0.79792454, 0.79809355,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.79792454, 0.79798861, 0.79808029, 0.79792565, 0.79792454,\n",
       "        0.79792234,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.79792012, 0.79795327, 0.79817859,\n",
       "        0.79792454, 0.79792454, 0.79792787,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'std_test_score': array([0.00097291, 0.0010449 , 0.0011066 , 0.00099365, 0.00092696,\n",
       "        0.00119539,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.00103029, 0.00119558, 0.00083126,\n",
       "        0.00085998, 0.00118545, 0.00053481,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.00089991,\n",
       "        0.00090087, 0.0007089 , 0.00115028, 0.00132935, 0.00078892,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.00157205, 0.00134632, 0.00109656, 0.00083636,\n",
       "        0.00103027, 0.00074419,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.00105512, 0.00108474,\n",
       "        0.00123841, 0.00098157, 0.00086171, 0.00094482,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00121337, 0.00132178, 0.00080782, 0.00097121, 0.00098469,\n",
       "        0.00133542,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.00099047, 0.00096019, 0.00086216,\n",
       "        0.00098305, 0.00095356, 0.00110526,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan, 0.00090803,\n",
       "        0.00122796, 0.00094786, 0.00118403, 0.00097681, 0.00106098,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "               nan, 0.00099117, 0.00089162, 0.00126624, 0.00098304,\n",
       "        0.00098539, 0.00096912,        nan,        nan,        nan,\n",
       "               nan,        nan,        nan, 0.0009596 , 0.00097788,\n",
       "        0.00103415, 0.00098193, 0.00098193, 0.00086118,        nan,\n",
       "               nan,        nan,        nan,        nan,        nan,\n",
       "        0.00098193, 0.00087789, 0.00096971, 0.00098013, 0.00098193,\n",
       "        0.00098038,        nan,        nan,        nan,        nan,\n",
       "               nan,        nan, 0.00098315, 0.00092928, 0.00117364,\n",
       "        0.00098193, 0.00098193, 0.00097912,        nan,        nan,\n",
       "               nan,        nan,        nan,        nan]),\n",
       " 'rank_test_score': array([18, 10,  4, 27, 15, 16, 73, 73, 73, 73, 73, 73, 12,  8,  3, 24, 17,\n",
       "        13, 73, 73, 73, 73, 73, 73, 11,  7,  1, 19, 21, 14, 73, 73, 73, 73,\n",
       "        73, 73,  9,  6,  2, 28, 20,  5, 73, 73, 73, 73, 73, 73, 49, 44, 22,\n",
       "        68, 39, 37, 73, 73, 73, 73, 73, 73, 38, 32, 26, 55, 59, 30, 73, 73,\n",
       "        73, 73, 73, 73, 53, 36, 25, 61, 43, 31, 73, 73, 73, 73, 73, 73, 48,\n",
       "        29, 23, 35, 69, 42, 73, 73, 73, 73, 73, 73, 53, 46, 33, 57, 58, 50,\n",
       "        73, 73, 73, 73, 73, 73, 52, 70, 45, 62, 62, 40, 73, 73, 73, 73, 73,\n",
       "        73, 62, 47, 41, 59, 62, 70, 73, 73, 73, 73, 73, 73, 72, 51, 34, 62,\n",
       "        62, 56, 73, 73, 73, 73, 73, 73])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid_search를 수행했을 경우의 각각의 결과\n",
    "result = grid_search.cv_results_\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 위의 모델을 생성한 결과를 바탕으로 hyperparameter를 설정한 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 모델(without EarlyStopping)\n",
    "- avg accuracy: 0.8006682634353638\n",
    "- avg F1-score: 0.800676167011261\n",
    "- avg recall: 0.8006762623786926\n",
    "- avg precision: 0.8006762623786926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 1000,\n",
       " 'epochs': 100,\n",
       " 'loss': 'categorical_crossentropy',\n",
       " 'num_layers': 2,\n",
       " 'num_nodes': 40}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4657 - accuracy: 0.7983 - f1_score: 0.7983 - recall: 0.7983 - precision: 0.7983\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4562 - accuracy: 0.8002 - f1_score: 0.8002 - recall: 0.8002 - precision: 0.8002\n",
      "Epoch 3/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4546 - accuracy: 0.8005 - f1_score: 0.8005 - recall: 0.8005 - precision: 0.8005\n",
      "Epoch 4/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4535 - accuracy: 0.8009 - f1_score: 0.8009 - recall: 0.8009 - precision: 0.8009\n",
      "Epoch 5/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4527 - accuracy: 0.8014 - f1_score: 0.8014 - recall: 0.8014 - precision: 0.8014\n",
      "Epoch 6/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4521 - accuracy: 0.8017 - f1_score: 0.8016 - recall: 0.8016 - precision: 0.8016\n",
      "Epoch 7/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4515 - accuracy: 0.8016 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "Epoch 8/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4512 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "Epoch 9/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4510 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "Epoch 10/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4505 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "Epoch 11/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4502 - accuracy: 0.8020 - f1_score: 0.8020 - recall: 0.8020 - precision: 0.8020\n",
      "Epoch 12/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4499 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "Epoch 13/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4498 - accuracy: 0.8023 - f1_score: 0.8022 - recall: 0.8022 - precision: 0.8022\n",
      "Epoch 14/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4494 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "Epoch 15/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4492 - accuracy: 0.8024 - f1_score: 0.8023 - recall: 0.8023 - precision: 0.8023\n",
      "Epoch 16/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4491 - accuracy: 0.8025 - f1_score: 0.8025 - recall: 0.8025 - precision: 0.8025\n",
      "Epoch 17/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4489 - accuracy: 0.8025 - f1_score: 0.8025 - recall: 0.8025 - precision: 0.8025\n",
      "Epoch 18/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4489 - accuracy: 0.8025 - f1_score: 0.8025 - recall: 0.8025 - precision: 0.8025\n",
      "Epoch 19/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4487 - accuracy: 0.8024 - f1_score: 0.8024 - recall: 0.8024 - precision: 0.8024\n",
      "Epoch 20/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4486 - accuracy: 0.8022 - f1_score: 0.8022 - recall: 0.8022 - precision: 0.8022\n",
      "Epoch 21/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4484 - accuracy: 0.8023 - f1_score: 0.8023 - recall: 0.8023 - precision: 0.8023\n",
      "Epoch 22/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4483 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "Epoch 23/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4482 - accuracy: 0.8025 - f1_score: 0.8025 - recall: 0.8025 - precision: 0.8025\n",
      "Epoch 24/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4480 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "Epoch 25/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4479 - accuracy: 0.8027 - f1_score: 0.8027 - recall: 0.8027 - precision: 0.8027\n",
      "Epoch 26/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4479 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 27/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4477 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 28/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4478 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 29/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4475 - accuracy: 0.8026 - f1_score: 0.8027 - recall: 0.8027 - precision: 0.8027\n",
      "Epoch 30/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4476 - accuracy: 0.8025 - f1_score: 0.8025 - recall: 0.8025 - precision: 0.8025\n",
      "Epoch 31/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4475 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "Epoch 32/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4474 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 33/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4473 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 34/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4472 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "Epoch 35/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4472 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 36/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4472 - accuracy: 0.8027 - f1_score: 0.8027 - recall: 0.8027 - precision: 0.8027\n",
      "Epoch 37/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4470 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 38/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4470 - accuracy: 0.8028 - f1_score: 0.8027 - recall: 0.8027 - precision: 0.8027\n",
      "Epoch 39/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4470 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 40/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4469 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 41/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4469 - accuracy: 0.8029 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 42/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4467 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 43/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4467 - accuracy: 0.8030 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 44/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4466 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 45/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4466 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 46/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4467 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 47/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4466 - accuracy: 0.8027 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 48/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4466 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 49/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4465 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 50/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4465 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 51/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4463 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 52/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4463 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 53/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4464 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 54/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4463 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 55/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4462 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "Epoch 56/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4463 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 57/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4463 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 58/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4460 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 59/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4461 - accuracy: 0.8028 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 60/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4460 - accuracy: 0.8029 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 61/100\n",
      "725/725 [==============================] - 7s 10ms/step - loss: 0.4460 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 62/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4459 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 63/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4460 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 64/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4459 - accuracy: 0.8031 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 65/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4460 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 66/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4459 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 67/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4459 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 68/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4458 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 69/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4457 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 70/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4457 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 71/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4457 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 72/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4458 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 73/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4456 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 74/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4457 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 75/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4456 - accuracy: 0.8034 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 76/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4456 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 77/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4455 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 78/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4456 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 79/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4456 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "Epoch 80/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4454 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 81/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4455 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 82/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 83/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 84/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 85/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 86/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8032 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 87/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 88/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 89/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 90/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 91/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 92/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 93/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 94/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4453 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 95/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 96/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4453 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 97/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 98/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 99/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 100/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4525 - accuracy: 0.8030 - f1_score: 0.8030 - recall: 0.8030 - precision: 0.8030\n",
      "===================================\n",
      "Validation accuracy: 0.8029922246932983\n",
      "Validation F1-score: 0.8029966950416565\n",
      "Validation recall: 0.8029967546463013\n",
      "Validation precision: 0.8029967546463013\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4473 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4470 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 3/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4470 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 4/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4469 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 5/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4467 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 6/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4468 - accuracy: 0.8034 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 7/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4467 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 8/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4467 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 9/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4467 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 10/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4466 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 11/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4466 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 12/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4465 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 13/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4465 - accuracy: 0.8034 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 14/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4464 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 15/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4463 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 16/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4464 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 17/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4464 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 18/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4462 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 19/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4463 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 20/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4462 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 21/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4461 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 22/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4462 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 23/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4461 - accuracy: 0.8037 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 24/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4462 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 25/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4461 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 26/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4460 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 27/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4459 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 28/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4459 - accuracy: 0.8037 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 29/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4457 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 30/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4460 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 31/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4460 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 32/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4459 - accuracy: 0.8036 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 33/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4458 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 34/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4459 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 35/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 36/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 37/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 38/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4456 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 39/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 40/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 41/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 42/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 43/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 44/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4456 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 45/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 46/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4454 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 47/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 48/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4456 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 49/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 50/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 51/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4455 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 52/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 53/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 54/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 55/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4452 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 56/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 57/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 58/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 59/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4452 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 60/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 61/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 62/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 63/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 64/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 65/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 66/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4450 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 67/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 68/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8037 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 69/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 70/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 71/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 72/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 73/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 74/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 75/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 76/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 77/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 78/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 79/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 80/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 81/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 82/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8038 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 83/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 84/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 85/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 86/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 87/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 88/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 89/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 90/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 91/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 92/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 93/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 94/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 95/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 96/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8039 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 97/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 98/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 99/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 100/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8041 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4491 - accuracy: 0.8023 - f1_score: 0.8023 - recall: 0.8023 - precision: 0.8023\n",
      "===================================\n",
      "Validation accuracy: 0.8023184537887573\n",
      "Validation F1-score: 0.8023148775100708\n",
      "Validation recall: 0.8023149371147156\n",
      "Validation precision: 0.8023149371147156\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4466 - accuracy: 0.8029 - f1_score: 0.8029 - recall: 0.8029 - precision: 0.8029\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4461 - accuracy: 0.8032 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 3/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4460 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 4/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4460 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 5/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4458 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 6/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 7/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 8/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4458 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 9/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 10/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 11/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 12/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 13/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 14/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8031 - f1_score: 0.8031 - recall: 0.8031 - precision: 0.8031\n",
      "Epoch 15/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 16/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4455 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 17/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 18/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 19/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 20/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 21/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 22/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 23/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 24/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 25/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 26/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 27/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 28/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 29/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 30/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 31/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4451 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 32/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 33/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 34/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 35/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 36/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 37/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 38/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 39/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 40/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 41/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 42/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 43/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 44/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 45/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 46/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 47/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 48/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 49/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4449 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 50/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 51/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 52/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 53/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 54/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 55/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 56/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 57/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4447 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 58/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 59/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4447 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 60/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 61/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 62/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 63/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 64/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 65/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 66/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 67/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 68/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 69/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4444 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 70/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 71/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4446 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 72/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 73/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 74/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 75/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4444 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 76/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4446 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 77/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 78/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 79/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4446 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 80/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4444 - accuracy: 0.8039 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 81/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 82/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4444 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 83/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 84/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 85/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 86/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 87/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 88/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 89/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 90/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 91/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 92/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4442 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 93/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 94/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 95/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 96/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 97/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4444 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 98/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4440 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 99/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 100/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "5659/5659 [==============================] - 9s 2ms/step - loss: 0.4489 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "===================================\n",
      "Validation accuracy: 0.8032131195068359\n",
      "Validation F1-score: 0.8032175898551941\n",
      "Validation recall: 0.8032176494598389\n",
      "Validation precision: 0.8032176494598389\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4461 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4458 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 3/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4457 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 4/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4458 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 5/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4455 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 6/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4455 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 7/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4455 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 8/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4454 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 9/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4455 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 10/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4453 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 11/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 12/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4452 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 13/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4452 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 14/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 15/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4453 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 16/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 17/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 18/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 19/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 20/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 21/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 22/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 23/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4451 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 24/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 25/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4449 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 26/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4449 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 27/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4450 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 28/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 29/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4448 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 30/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4450 - accuracy: 0.8032 - f1_score: 0.8032 - recall: 0.8032 - precision: 0.8032\n",
      "Epoch 31/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 32/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4448 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 33/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 34/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 35/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 36/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 37/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 38/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 39/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4446 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 40/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 41/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 42/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 43/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 44/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 45/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 46/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 47/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4446 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 48/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4446 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 49/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 50/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4446 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 51/100\n",
      "725/725 [==============================] - 7s 9ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 52/100\n",
      "725/725 [==============================] - 8s 12ms/step - loss: 0.4445 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 53/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4445 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 54/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4446 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 55/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 56/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 57/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 58/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 59/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4444 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 60/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4444 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 61/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 62/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 63/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 64/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 65/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 66/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 67/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 68/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8035 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 69/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 70/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 71/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 72/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 73/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 74/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 75/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8034 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 76/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 77/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 78/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 79/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 80/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 81/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 82/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 83/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 84/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 85/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 86/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 87/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 88/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 89/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 90/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 91/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 92/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 93/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 94/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 95/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 96/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 97/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 98/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 99/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8036 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 100/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8035 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "5659/5659 [==============================] - 6s 972us/step - loss: 0.4474 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "===================================\n",
      "Validation accuracy: 0.8036052584648132\n",
      "Validation F1-score: 0.8036176562309265\n",
      "Validation recall: 0.8036177754402161\n",
      "Validation precision: 0.8036177754402161\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4457 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4456 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 3/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4454 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 4/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4454 - accuracy: 0.8033 - f1_score: 0.8033 - recall: 0.8033 - precision: 0.8033\n",
      "Epoch 5/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 6/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4453 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 7/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4452 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 8/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4450 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 9/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 10/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4452 - accuracy: 0.8034 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 11/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4451 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 12/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4450 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 13/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4450 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 14/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 15/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 16/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4450 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 17/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 18/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 19/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 20/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 21/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 22/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 23/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4449 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 24/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4447 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 25/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 26/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4447 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 27/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4448 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 28/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 29/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4446 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 30/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4447 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 31/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 32/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 33/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4446 - accuracy: 0.8033 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "Epoch 34/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4446 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 35/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4446 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 36/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4446 - accuracy: 0.8038 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 37/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 38/100\n",
      "725/725 [==============================] - 7s 10ms/step - loss: 0.4446 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 39/100\n",
      "725/725 [==============================] - 9s 12ms/step - loss: 0.4445 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 40/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 41/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 42/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 43/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 44/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4445 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 45/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4444 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 46/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4442 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 47/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 48/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 49/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4445 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 50/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4443 - accuracy: 0.8039 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 51/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4442 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 52/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 53/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 54/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 55/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 56/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 57/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 58/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 59/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 60/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 61/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 62/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 63/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 64/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 65/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 66/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 67/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 68/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 69/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 70/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8036 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 71/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 72/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 73/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 74/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4442 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 75/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 76/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 77/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 78/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 79/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4443 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 80/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 81/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8037 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 82/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 83/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 84/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 85/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 86/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8036 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 87/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 88/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4440 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 89/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 90/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 91/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 92/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8039 - f1_score: 0.8039 - recall: 0.8039 - precision: 0.8039\n",
      "Epoch 93/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4441 - accuracy: 0.8035 - f1_score: 0.8035 - recall: 0.8035 - precision: 0.8035\n",
      "Epoch 94/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8037 - f1_score: 0.8036 - recall: 0.8036 - precision: 0.8036\n",
      "Epoch 95/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 96/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4440 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 97/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4439 - accuracy: 0.8037 - f1_score: 0.8037 - recall: 0.8037 - precision: 0.8037\n",
      "Epoch 98/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "Epoch 99/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4440 - accuracy: 0.8040 - f1_score: 0.8040 - recall: 0.8040 - precision: 0.8040\n",
      "Epoch 100/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4438 - accuracy: 0.8038 - f1_score: 0.8038 - recall: 0.8038 - precision: 0.8038\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4472 - accuracy: 0.8034 - f1_score: 0.8034 - recall: 0.8034 - precision: 0.8034\n",
      "===================================\n",
      "Validation accuracy: 0.803362250328064\n",
      "Validation F1-score: 0.8033585548400879\n",
      "Validation recall: 0.8033586144447327\n",
      "Validation precision: 0.8033586144447327\n",
      "###################################\n",
      "avg Validation accuracy: 0.8030982613563538\n",
      "avg Validation F1-score: 0.8031010746955871\n",
      "avg Validation recall: 0.8031011462211609\n",
      "avg Validation precision: 0.8031011462211609\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(best_params['num_nodes'], input_dim=86, activation='relu'))\n",
    "for _ in range(best_params['num_layers']):\n",
    "    model.add(Dense(best_params['num_nodes'], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss=best_params['loss'], optimizer='Adam', metrics=[\"accuracy\", f1_score, recall, precision])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, Y_train_fold, \n",
    "              batch_size=best_params['batch_size'], \n",
    "              epochs=best_params['epochs'], \n",
    "              verbose=1)\n",
    "\n",
    "    # 모델 validation\n",
    "    valid_loss, valid_acc, valid_f1, valid_recall, valid_precision = model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_f1s.append(valid_f1)\n",
    "    valid_recalls.append(valid_recall)\n",
    "    valid_precisions.append(valid_precision)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "    print(\"Validation F1-score:\", valid_f1)\n",
    "    print(\"Validation recall:\", valid_recall)\n",
    "    print(\"Validation precision:\", valid_precision)\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dnn_models/pca1_feature86/without_earlystopping_0421.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적 model(with EarlyStopping)\n",
    "### earlystopping: min_delta was 0.001\n",
    "- avg accuracy: 0.8006682634353638\n",
    "- avg F1-score: 0.800676167011261\n",
    "- avg recall: 0.8006762623786926\n",
    "- avg precision: 0.8006762623786926\n",
    "\n",
    "### earlystopping: min_delta was 0.0001\n",
    "- avg accuracy: 0.80015869140625\n",
    "- avg F1-score: 0.8001592040061951\n",
    "- avg recall: 0.8001593112945556\n",
    "- avg precision: 0.8001593112945556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4717 - accuracy: 0.7955 - f1_score: 0.7955 - recall: 0.7955 - precision: 0.7955\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4569 - accuracy: 0.8000 - f1_score: 0.8000 - recall: 0.8000 - precision: 0.8000\n",
      "5659/5659 [==============================] - 6s 1ms/step - loss: 0.4542 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "===================================\n",
      "Validation accuracy: 0.8017992973327637\n",
      "Validation F1-score: 0.8018038868904114\n",
      "Validation recall: 0.8018039464950562\n",
      "Validation precision: 0.8018039464950562\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4552 - accuracy: 0.8010 - f1_score: 0.8010 - recall: 0.8010 - precision: 0.8010\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4540 - accuracy: 0.8014 - f1_score: 0.8014 - recall: 0.8014 - precision: 0.8014\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4534 - accuracy: 0.7994 - f1_score: 0.7994 - recall: 0.7994 - precision: 0.7994\n",
      "===================================\n",
      "Validation accuracy: 0.7993582487106323\n",
      "Validation F1-score: 0.7993549704551697\n",
      "Validation recall: 0.7993550896644592\n",
      "Validation precision: 0.7993550896644592\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 8ms/step - loss: 0.4532 - accuracy: 0.8009 - f1_score: 0.8009 - recall: 0.8009 - precision: 0.8009\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4525 - accuracy: 0.8013 - f1_score: 0.8013 - recall: 0.8013 - precision: 0.8013\n",
      "5659/5659 [==============================] - 8s 1ms/step - loss: 0.4523 - accuracy: 0.8009 - f1_score: 0.8009 - recall: 0.8009 - precision: 0.8009\n",
      "===================================\n",
      "Validation accuracy: 0.8009267449378967\n",
      "Validation F1-score: 0.8009232878684998\n",
      "Validation recall: 0.8009233474731445\n",
      "Validation precision: 0.8009233474731445\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4521 - accuracy: 0.8015 - f1_score: 0.8015 - recall: 0.8015 - precision: 0.8015\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 6s 9ms/step - loss: 0.4515 - accuracy: 0.8019 - f1_score: 0.8019 - recall: 0.8019 - precision: 0.8019\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4505 - accuracy: 0.8010 - f1_score: 0.8010 - recall: 0.8010 - precision: 0.8010\n",
      "===================================\n",
      "Validation accuracy: 0.8010261058807373\n",
      "Validation F1-score: 0.8010307550430298\n",
      "Validation recall: 0.8010308742523193\n",
      "Validation precision: 0.8010308742523193\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4513 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 6s 8ms/step - loss: 0.4507 - accuracy: 0.8022 - f1_score: 0.8023 - recall: 0.8023 - precision: 0.8023\n",
      "5659/5659 [==============================] - 7s 1ms/step - loss: 0.4501 - accuracy: 0.8020 - f1_score: 0.8020 - recall: 0.8020 - precision: 0.8020\n",
      "===================================\n",
      "Validation accuracy: 0.8019760251045227\n",
      "Validation F1-score: 0.801972508430481\n",
      "Validation recall: 0.8019725680351257\n",
      "Validation precision: 0.8019725680351257\n",
      "###################################\n",
      "avg Validation accuracy: 0.8010172843933105\n",
      "avg Validation F1-score: 0.8010170817375183\n",
      "avg Validation recall: 0.801017165184021\n",
      "avg Validation precision: 0.801017165184021\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(best_params['num_nodes'], input_dim=86, activation='relu'))\n",
    "for _ in range(best_params['num_layers']):\n",
    "    model.add(Dense(best_params['num_nodes'], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss=best_params['loss'], optimizer='Adam', metrics=[\"accuracy\", f1_score, recall, precision])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.001)\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, Y_train_fold, \n",
    "              batch_size=best_params['batch_size'], \n",
    "              epochs=best_params['epochs'], \n",
    "              verbose=1, \n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 validation\n",
    "    valid_loss, valid_acc, valid_f1, valid_recall, valid_precision = model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_f1s.append(valid_f1)\n",
    "    valid_recalls.append(valid_recall)\n",
    "    valid_precisions.append(valid_precision)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "    print(\"Validation F1-score:\", valid_f1)\n",
    "    print(\"Validation recall:\", valid_recall)\n",
    "    print(\"Validation precision:\", valid_precision)\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))\n",
    "model.save('dnn_models/pca1_feature86/with_earlystopping_001_0421.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4692 - accuracy: 0.7972 - f1_score: 0.7972 - recall: 0.7972 - precision: 0.7972\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4567 - accuracy: 0.7999 - f1_score: 0.7999 - recall: 0.7999 - precision: 0.7999\n",
      "5659/5659 [==============================] - 5s 929us/step - loss: 0.4546 - accuracy: 0.8022 - f1_score: 0.8022 - recall: 0.8022 - precision: 0.8022\n",
      "===================================\n",
      "Validation accuracy: 0.8022080063819885\n",
      "Validation F1-score: 0.8022125363349915\n",
      "Validation recall: 0.8022125959396362\n",
      "Validation precision: 0.8022125959396362\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4549 - accuracy: 0.8011 - f1_score: 0.8011 - recall: 0.8011 - precision: 0.8011\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4538 - accuracy: 0.8014 - f1_score: 0.8014 - recall: 0.8014 - precision: 0.8014\n",
      "5659/5659 [==============================] - 5s 945us/step - loss: 0.4526 - accuracy: 0.8007 - f1_score: 0.8007 - recall: 0.8007 - precision: 0.8007\n",
      "===================================\n",
      "Validation accuracy: 0.800727903842926\n",
      "Validation F1-score: 0.800724446773529\n",
      "Validation recall: 0.8007245659828186\n",
      "Validation precision: 0.8007245659828186\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4531 - accuracy: 0.8013 - f1_score: 0.8013 - recall: 0.8013 - precision: 0.8013\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4526 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "5659/5659 [==============================] - 6s 982us/step - loss: 0.4517 - accuracy: 0.8028 - f1_score: 0.8028 - recall: 0.8028 - precision: 0.8028\n",
      "===================================\n",
      "Validation accuracy: 0.8028044700622559\n",
      "Validation F1-score: 0.8028008937835693\n",
      "Validation recall: 0.8028008937835693\n",
      "Validation precision: 0.8028008937835693\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4524 - accuracy: 0.8017 - f1_score: 0.8017 - recall: 0.8017 - precision: 0.8017\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 7ms/step - loss: 0.4518 - accuracy: 0.8018 - f1_score: 0.8018 - recall: 0.8018 - precision: 0.8018\n",
      "5659/5659 [==============================] - 5s 868us/step - loss: 0.4510 - accuracy: 0.8021 - f1_score: 0.8021 - recall: 0.8021 - precision: 0.8021\n",
      "===================================\n",
      "Validation accuracy: 0.8020754456520081\n",
      "Validation F1-score: 0.802079975605011\n",
      "Validation recall: 0.8020800948143005\n",
      "Validation precision: 0.8020800948143005\n",
      "Epoch 1/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4514 - accuracy: 0.8021 - f1_score: 0.8021 - recall: 0.8021 - precision: 0.8021\n",
      "Epoch 2/100\n",
      "725/725 [==============================] - 5s 6ms/step - loss: 0.4509 - accuracy: 0.8022 - f1_score: 0.8022 - recall: 0.8022 - precision: 0.8022\n",
      "5659/5659 [==============================] - 5s 873us/step - loss: 0.4498 - accuracy: 0.8026 - f1_score: 0.8026 - recall: 0.8026 - precision: 0.8026\n",
      "===================================\n",
      "Validation accuracy: 0.8026443123817444\n",
      "Validation F1-score: 0.8026406764984131\n",
      "Validation recall: 0.8026407361030579\n",
      "Validation precision: 0.8026407361030579\n",
      "###################################\n",
      "avg Validation accuracy: 0.8020920276641845\n",
      "avg Validation F1-score: 0.8020917057991028\n",
      "avg Validation recall: 0.8020917773246765\n",
      "avg Validation precision: 0.8020917773246765\n"
     ]
    }
   ],
   "source": [
    "# # 최고의 파라미터 : {'batch_size': 1000, 'epochs': 100, 'loss': 'categorical_crossentropy', 'num_layers': 2, 'num_nodes': 40}\n",
    "valid_accs, valid_f1s, valid_recalls, valid_precisions = [], [], [], []\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# 신경망층 설계\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(best_params['num_nodes'], input_dim=86, activation='relu'))\n",
    "for _ in range(best_params['num_layers']):\n",
    "    model.add(Dense(best_params['num_nodes'], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss=best_params['loss'], optimizer='Adam', metrics=[\"accuracy\", f1_score, recall, precision])\n",
    "# KFoldvalidation 사용함(k=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=30)\n",
    "early_stopping = EarlyStopping(monitor='loss',min_delta=0.0001)\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    X_train_fold, X_val_fold = x_train[train_index], x_train[val_index]\n",
    "    Y_train_fold, Y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # 모델 학습\n",
    "    model.fit(X_train_fold, Y_train_fold, \n",
    "              batch_size=best_params['batch_size'], \n",
    "              epochs=best_params['epochs'], \n",
    "              verbose=1, \n",
    "              callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 valdation\n",
    "    valid_loss, valid_acc, valid_f1, valid_recall, valid_precision = model.evaluate(X_val_fold, Y_val_fold)\n",
    "    valid_accs.append(valid_acc)\n",
    "    valid_f1s.append(valid_f1)\n",
    "    valid_recalls.append(valid_recall)\n",
    "    valid_precisions.append(valid_precision)\n",
    "    print(\"===================================\")\n",
    "    print(\"Validation accuracy:\", valid_acc)\n",
    "    print(\"Validation F1-score:\", valid_f1)\n",
    "    print(\"Validation recall:\", valid_recall)\n",
    "    print(\"Validation precision:\", valid_precision)\n",
    "print(\"###################################\")\n",
    "print(\"avg Validation accuracy:\", np.mean(valid_accs))\n",
    "print(\"avg Validation F1-score:\", np.mean(valid_f1s))\n",
    "print(\"avg Validation recall:\", np.mean(valid_recalls))\n",
    "print(\"avg Validation precision:\", np.mean(valid_precisions))\n",
    "model.save('dnn_models/pca1_feature86/with_earlystopping_0001_0421.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
